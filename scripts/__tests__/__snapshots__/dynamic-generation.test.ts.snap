// Vitest Snapshot v1, https://vitest.dev/guide/snapshot.html

exports[`dynamic generation snapshots > with beads flag > should match snapshot for add-command.md 1`] = `
"---
description: Guide for creating new slash commands
argument-hint: <command-name> <description>
_hint: Create command
_category: Utilities
_order: 3
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




# Slash Command Creator Guide

## How This Command Works
The \`/add-command\` command shows this guide for creating new slash commands. It includes:
- Command structure and syntax
- Common patterns and examples
- Security restrictions and limitations
- Frontmatter options

**Note for AI**: When creating commands, you CAN use bash tools like \`Bash(mkdir:*)\`, \`Bash(ls:*)\`, \`Bash(git status:*)\` in the \`allowed-tools\` frontmatter of NEW commands - but ONLY for operations within the current project directory. This command itself doesn't need bash tools since it's just documentation.

## Command Locations
- **Personal**: \`~/.claude/commands/\` (available across all projects)
- **Project**: \`.claude/commands/\` (shared with team, shows "(project)")

## Basic Structure

\`\`\`markdown
---
allowed-tools: Read, Glob, Grep, Bash(git status:*), Task
description: Brief description of what this command does
argument-hint: [required-arg] [optional-arg]
---

# Command Title

Your command instructions here.

**User arguments:**

Add-command: $ARGUMENTS

**End of user arguments**

File reference: @path/to/file.js

Bash command output: (exclamation)git status(backticks)
\`\`\`

## âš ï¸ Security Restrictions

**Bash Commands (exclamation prefix)**: Limited to current working directory only.
- âœ… Works: \`! + backtick + git status + backtick\` (in project dir)
- âŒ Blocked: \`! + backtick + ls /outside/project + backtick\` (outside project)  
- âŒ Blocked: \`! + backtick + pwd + backtick\` (if referencing dirs outside project)

**File References (\`@\` prefix)**: No directory restrictions.
- âœ… Works: \`@/path/to/system/file.md\`
- âœ… Works: \`@../other-project/file.js\`

## Common Patterns

### Simple Command
\`\`\`bash
echo "Review this code for bugs and suggest fixes" > ~/.claude/commands/review.md
\`\`\`

### Command with Arguments
**Note for AI**: The example below uses a fullwidth dollar sign (ï¼„, U+FF04) to prevent interpolation in this documentation. When creating actual commands, use the regular \`$\` character.

\`\`\`markdown
Fix issue ï¼„ARGUMENTS following our coding standards
\`\`\`

### Command with File References
\`\`\`markdown
Compare @src/old.js with @src/new.js and explain differences
\`\`\`

### Command with Bash Output (Project Directory Only)
\`\`\`markdown
---
allowed-tools: Bash(git status:*), Bash(git branch:*), Bash(git log:*)
---
Current status: (!)git status(\`)
Current branch: (!)git branch --show-current(\`)
Recent commits: (!)git log --oneline -5(\`)

Create commit for these changes.
\`\`\`

**Note**: Only works with commands in the current project directory.

### Namespaced Command
**Note for AI**: The example below uses a fullwidth dollar sign (ï¼„, U+FF04) to prevent interpolation in this documentation. When creating actual commands, use the regular \`$\` character.

\`\`\`bash
mkdir -p ~/.claude/commands/ai
echo "Ask GPT-5 about: ï¼„ARGUMENTS" > ~/.claude/commands/ai/gpt5.md
# Creates: /ai:gpt5
\`\`\`

## Frontmatter Options
- \`allowed-tools\`: Tools this command can use
  - **Important**: Intrusive tools like \`Write\`, \`Edit\`, \`NotebookEdit\` should NEVER be allowed in commands unless the user explicitly requests them. These tools modify files and should only be used when the command's purpose is to make changes.
  - âœ… Safe for most commands: \`Read\`, \`Glob\`, \`Grep\`, \`Bash(git status:*)\`, \`Task\`, \`AskUserQuestion\`
- \`description\`: Brief description (shows in /help)
- \`argument-hint\`: Help text for arguments
- \`model\`: Specific model to use

## Best Practices

### Safe Commands (No Security Issues)
\`\`\`markdown
# System prompt editor (file reference only)  
(@)path/to/system/prompt.md

Edit your system prompt above.
\`\`\`

### Project-Specific Commands (Bash OK)
\`\`\`markdown
---
allowed-tools: Bash(git status:*), Bash(npm list:*)
---
Current git status: (!)git status(\`)
Package info: (!)npm list --depth=0(\`)

Review project state and suggest next steps.
\`\`\`

### Cross-Directory File Access (Use @ not !)
\`\`\`markdown
# Compare config files
Compare (@)path/to/system.md with (@)project/config.md

Show differences and suggest improvements.
\`\`\`

## Usage
After creating: \`/<command-name> [arguments]\`

Example: \`/review\` or \`/ai:gpt5 "explain this code"\`
"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for ask.md 1`] = `
"---
description: Request team review and approval - for complex changes needing discussion
argument-hint: [optional-pr-title-and-description]
_hint: Request review
_category: Ship / Show / Ask
_order: 3
_selectedByDefault: false
---

# Ask - Request Review and Approval

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




**Ship/Show/Ask Pattern - ASK**

Ask is for complex changes that need team discussion and approval. Examples:

- Breaking API changes
- New architecture decisions
- Significant feature additions
- Performance trade-offs
- Security-sensitive changes

## When to Ask

Use **Ask** when:

- Changes affect multiple systems
- Breaking changes are needed
- You need input on approach
- Security implications exist
- Performance trade-offs need discussion
- Uncertain about the best solution

## Workflow

Current branch status:
!\`git status\`

Recent commits:
!\`git log --oneline -5\`

**User arguments:**

Ask: $ARGUMENTS

**End of user arguments**

**This is the traditional Pull Request workflow**, but with explicit intent that review and approval are required.

**Process:**

1. **Ensure Branch is Ready**:
   !\`git status\`
   - Commit all changes
   - Push to remote: \`git push origin [branch-name]\`

2. **Create Ask PR**: Create a PR that clearly needs review

   Title: conventional commits format, prefixed with \`[ASK]\`

   Description template:

   \`\`\`markdown
   ## ðŸ¤” Ask - Review and Approval Needed

   **This is an ASK PR**: These changes need team review and discussion.

   <!--
     References: [link to relevant issues]
   -->

   ### What changed

   [Detailed description of changes]

   ### Why

   [Rationale and context]

   ### Questions for reviewers

   - [ ] Question 1
   - [ ] Question 2

   ### Concerns

   - Potential concern 1
   - Potential concern 2

   ### Test Plan

   - [ ] Unit tests
   - [ ] Integration tests
   - [ ] Manual testing steps

   ### Alternatives considered

   - Alternative 1: [why not chosen]
   - Alternative 2: [why not chosen]
   \`\`\`

3. **Request Reviewers**: Assign specific reviewers who should weigh in

4. **Add Labels**:
   - "needs-review"
   - "breaking-change" (if applicable)
   - "security" (if applicable)

5. **Link Issues**: Reference related issues in the description

6. **Monitor Discussion**: Be responsive to reviewer feedback and questions

## Use GitHub MCP Tools

1. Check current branch and ensure it's pushed
2. Create a well-formatted pull request with [ASK] prefix
3. Set reviewers
4. Add appropriate labels
5. Link related issues from commit messages

## Decision Guide

Use **Ask** when:

- âœ… Change is complex or risky
- âœ… Breaking changes involved
- âœ… Need team input on approach
- âœ… Multiple solutions possible
- âœ… Security implications

Use **/show** instead if: confident in approach, just want visibility

Use **/ship** instead if: change is tiny, obvious, and safe

### Beads Integration

Use Beads MCP to:
- Track work with \`bd ready\` to find next task
- Create issues with \`bd create "description"\`
- Track dependencies with \`bd dep add\`

See https://github.com/steveyegge/beads for more information.

"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for beepboop.md 1`] = `
"---
description: Communicate AI-generated content with transparent attribution
argument-hint: <task-description>
_hint: AI attribution
_category: Utilities
_order: 2
---

# AI-Attributed Communication Command

Execute the user's requested task (e.g., posting PR comments, GitHub issue comments, or other communications through various MCPs), but frame the output with clear AI attribution.

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




## Instructions

**User arguments:**

Beepboop: $ARGUMENTS

**End of user arguments**

**IMPORTANT Communication Format:**

1. **Opening**: Begin with "*Beep boop, I am Claude Code ðŸ¤–, my user has reviewed and approved the following written by me:*"
   - Use italics for this line
   - Clearly establishes AI authorship

2. **Middle**: Perform the requested task (post comment, create review, etc.)
   - Execute whatever communication task the user requested
   - Write the actual content that accomplishes the user's goal

3. **Closing**: End with "*Beep boop, Claude Code ðŸ¤– out!*"
   - Use italics for this line
   - Provides clear closure

## Purpose

This command ensures transparency about AI usage while maintaining that the user has reviewed and approved the content. It prevents offloading review responsibility to other users while being open about AI assistance.

## Examples

- Posting a GitHub PR review comment
- Adding a comment to a GitHub issue
- Responding to feedback with AI-generated explanations
- Any communication where AI attribution is valuable
"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for busycommit.md 1`] = `
"---
description: Create multiple atomic git commits, one logical change at a time
argument-hint: [optional-commit-description]
_hint: Atomic commits
_category: Workflow
_order: 2
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




Create multiple atomic git commits, committing the smallest possible logical unit at a time

**User arguments:**

Busycommit: $ARGUMENTS

**End of user arguments**

## Commit Message Rules

Follows [Conventional Commits](https://www.conventionalcommits.org/) standard.

1. **Format**: \`type(#issue): description\`
   - Use \`#123\` for local repo issues
   - Use \`owner/repo#123\` for cross-repo issues
   - Common types: \`feat\`, \`fix\`, \`docs\`, \`refactor\`, \`test\`, \`chore\`

2. **AI Credits**: **NEVER include AI credits in commit messages**
   - No "Generated with Claude Code"
   - No "Co-Authored-By: Claude" or "Co-Authored-By: Happy"
   - Focus on the actual changes made, not conversation history

3. **Content**: Write clear, concise commit messages describing what changed and why

## Process

1. Run \`git status\` and \`git diff\` to review changes
2. Run \`git log --oneline -5\` to see recent commit style
3. Stage relevant files with \`git add\`
4. Create commit with descriptive message
5. Verify with \`git status\`

## Example

\`\`\`bash
git add <files>
git commit -m "feat(#123): add validation to user input form"
\`\`\`


## Atomic Commit Approach

Each commit should represent ONE logical change. Do NOT bundle multiple unrelated changes into one commit.

- Identify the smallest atomic units of change
- For EACH atomic unit: stage only those files/hunks, commit, verify
- Use \`git add -p\` to stage partial file changes when a file contains multiple logical changes
- Repeat until all changes are committed
- It is OK to create multiple commits without stopping - keep going until \`git status\` shows clean

## Multi-Commit Example

If a single file contains multiple unrelated changes, use \`git add -p\` to stage hunks interactively:

\`\`\`bash
# Stage only the validation-related hunks from the file
git add -p src/user-service.ts
# Select 'y' for validation hunks, 'n' for others
git commit -m "feat(#123): add email format validation"

# Stage the error handling hunks
git add -p src/user-service.ts
git commit -m "fix(#124): handle null user gracefully"

# Stage remaining changes
git add src/user-service.ts
git commit -m "refactor: extract user lookup to helper"
\`\`\`
"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for code-review.md 1`] = `
"---
description: Code review using dynamic category detection and domain-specific analysis
argument-hint: (optional) [branch, PR#, or PR URL] - defaults to current branch
_hint: Review code
_category: Workflow
_order: 35
_requested-tools:
  - Bash(git diff:*)
  - Bash(git status:*)
  - Bash(git log:*)
  - Bash(git rev-parse:*)
  - Bash(git merge-base:*)
  - Bash(git branch:*)
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




# Code Review

Perform a code review using dynamic category detection.

## Phase 0: Setup & Categorization

### Determine What to Review

Parse the argument to determine the review target:

| Input | Action |
|-------|--------|
| No argument | Detect divergence point, confirm scope with user |
| Branch name | Use specified branch as base |
| PR number (e.g., \`123\`) | Fetch PR diff from GitHub |
| PR URL (e.g., \`https://github.com/owner/repo/pull/123\`) | Extract PR number and fetch diff |



**For GitHub PRs:**

1. Try GitHub MCP first: \`mcp__github__pull_request_read\` with \`method: "get_diff"\`
2. Fall back to \`gh\` CLI: \`gh pr diff <number>\`
3. If neither works, report error and stop


**For local branches (no argument or branch name provided):**

1. **Get current branch**: \`git rev-parse --abbrev-ref HEAD\`

2. **Check for uncommitted changes**: \`git status --porcelain\`
   - If output is non-empty, note that uncommitted changes exist

3. **Detect divergence point** (skip if branch name was provided as argument):
   - Get all local branches except current: \`git branch --format='%(refname:short)'\`
   - For each branch, find merge-base: \`git merge-base HEAD <branch>\`
   - Count commits from merge-base to HEAD: \`git rev-list --count <merge-base>..HEAD\`
   - The branch with the **fewest commits back** (closest merge-base) is the likely parent
   - If no other branches exist, fall back to \`main\`, \`master\`, or \`develop\` if they exist as remote tracking branches

4. **Confirm scope with user** using \`AskUserQuestion\`:

   **Question 1 - "Review scope"** (header: "Base branch"):
   - Option A: \`From <detected-branch>\` â€” "Review N commits since diverging from <branch>"
   - Option B: \`Different branch\` â€” "Specify another branch to compare against"
   - Option C: \`Uncommitted only\` â€” "Review only staged/unstaged changes, skip committed work"

   **Question 2 - "Include uncommitted?"** (header: "Uncommitted", only ask if uncommitted changes exist AND user didn't pick option C):
   - Option A: \`Yes\` â€” "Include N staged/unstaged files in review"
   - Option B: \`No\` â€” "Review only committed changes"

5. **Collect changed files** based on user selection:
   - From branch: \`git diff --name-only <base>...HEAD\`
   - Uncommitted unstaged: \`git diff --name-only\`
   - Uncommitted staged: \`git diff --name-only --cached\`
   - Combine and deduplicate the file list

6. **If no changes**: Report "Nothing to review" and stop

### Categorize Files

Check for CLAUDE.md - if it exists, note any project-specific review patterns.

Categorize each changed file into ONE primary category based on these patterns:

| Category | File Patterns |
|----------|---------------|
| Frontend/UI | \`*.tsx\`, \`*.jsx\`, \`components/\`, \`pages/\`, \`views/\`, \`*.vue\` |
| Frontend/Styling | \`*.css\`, \`*.scss\`, \`*.less\`, \`styles/\`, \`*.tailwind*\`, \`*.styled.*\` |
| Backend/API | \`routes/\`, \`api/\`, \`controllers/\`, \`services/\`, \`*.controller.*\`, \`*.service.*\`, \`*.resolver.*\` |
| Backend/Data | \`migrations/\`, \`models/\`, \`prisma/\`, \`schema.*\`, \`*.model.*\`, \`*.entity.*\` |
| Tooling/Config | \`scripts/\`, \`*.config.*\`, \`package.json\`, \`tsconfig.*\`, \`vite.*\`, \`webpack.*\`, \`eslint.*\` |
| CI/CD | \`.github/\`, \`.gitlab-ci.*\`, \`Dockerfile\`, \`docker-compose.*\`, \`*.yml\` in CI paths |
| Tests | \`*.test.*\`, \`*.spec.*\`, \`__tests__/\`, \`__mocks__/\`, \`*.stories.*\` |
| Docs | \`*.md\`, \`docs/\`, \`README*\`, \`CHANGELOG*\` |


Output the categorization:

\`\`\`
## Categorization

Base branch: <branch>
Total files changed: <n>

| Category | Files |
|----------|-------|
| <category> | <count> |
...
\`\`\`

## Phase 1: Branch Brief

From the diff and recent commit messages (\`git log <base>...HEAD --oneline\`), infer:

- **Goal**: What this branch accomplishes (1-3 sentences)
- **Constraints**: Any implied requirements (security, performance, backwards compatibility)
- **Success checklist**: What must work after this change, what must not break

\`\`\`
## Branch Brief

**Goal**: ...
**Constraints**: ...
**Checklist**:
- [ ] ...
\`\`\`

## Phase 2: Category Reviews

For each detected category with changes, run a targeted review. Skip categories with no changes.

### Frontend/UI Review Criteria

- Accessibility: ARIA attributes, keyboard navigation, screen reader support
- Component patterns: Composition, prop drilling, context usage
- State management: Unnecessary re-renders, stale closures
- Performance: memo/useMemo/useCallback usage, lazy loading, bundle impact

### Frontend/Styling Review Criteria

- Responsive design: Breakpoints, mobile-first
- Design system: Token usage, consistent spacing/colors
- CSS specificity: Overly specific selectors, !important usage
- Theme support: Dark mode, CSS variables

### Backend/API Review Criteria

- Input validation: Sanitization, type checking, bounds
- Security: Authentication checks, authorization, injection risks
- Error handling: Proper status codes, meaningful messages, logging
- Performance: N+1 queries, missing indexes, pagination

### Backend/Data Review Criteria

- Migration safety: Reversibility, data preservation
- Data integrity: Constraints, foreign keys, nullability
- Index usage: Queries have appropriate indexes
- Backwards compatibility: Existing data still works

### Tooling/Config Review Criteria

- Breaking changes: Does this affect developer workflow?
- Dependency compatibility: Version conflicts, peer deps
- Build performance: Added build time, bundle size

### CI/CD Review Criteria

- Secrets exposure: Credentials in logs, env vars
- Pipeline efficiency: Caching, parallelization
- Failure handling: Notifications, rollback strategy

### Tests Review Criteria

#### FIRST Principles

| Principle | What to Check |
|-----------|---------------|
| **Fast** | Tests complete quickly, no I/O, no network calls, no sleep()/setTimeout delays |
| **Independent** | No shared mutable state, no execution order dependencies between tests |
| **Repeatable** | No Date.now(), no Math.random() without seeding, no external service dependencies |
| **Self-validating** | Meaningful assertions that verify behavior, no manual verification needed |

#### TDD Anti-patterns

| Anti-pattern | Detection Signals |
|--------------|-------------------|
| **The Liar** | \`expect(true).toBe(true)\`, empty test bodies, tests with no assertions |
| **Excessive Setup** | >20 lines of arrange code, >5 mocks, deep nested object construction |
| **The One** | >5 assertions testing unrelated behaviors in a single test |
| **The Peeping Tom** | Testing private methods, asserting on internal state, tests that break on any refactor |
| **The Slow Poke** | Real database/network calls, file I/O, hard-coded timeouts |

#### Test Structure (AAA Pattern)

- **Arrange**: Clear setup with minimal fixtures
- **Act**: Single action being tested
- **Assert**: Specific, behavior-focused assertions


### Docs Review Criteria

- Technical accuracy: Code examples work, APIs documented correctly
- Completeness: All new features documented
- Clarity: Easy to follow, good examples

**Output format per category:**

\`\`\`
## <Category> Review (<n> files)

### file:line - [blocker|risky|nit] Title
Description of the issue and why it matters.
Suggested fix or question to investigate.

...
\`\`\`

## Phase 3: Cross-Cutting Analysis

After reviewing all categories, check for cross-cutting issues:

- API changed but tests didn't update?
- New feature but no documentation?
- Migration added but no rollback tested?
- Config changed but README not updated?
- Security-sensitive code without corresponding test?

\`\`\`
## Cross-Cutting Issues

- [ ] <issue description>
...
\`\`\`

## Phase 4: Summary

### PR Description (draft)

Provide a ready-to-paste PR description:

\`\`\`
## What changed
- <by category, 1-2 bullets each>

## Why
- <motivation>

## Testing
- <how to verify>

## Notes
- <migration steps, breaking changes, etc.>
\`\`\`

### Review Checklist

\`\`\`
## Before Merge

### Blockers (must fix)
- [ ] ...

### Risky (highlight to reviewers)
- [ ] ...

### Follow-ups (can defer)
- [ ] ...
\`\`\`

---

**User arguments:**

Code-review: $ARGUMENTS

**End of user arguments**
"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for commit.md 1`] = `
"---
description: Create a git commit following project standards
argument-hint: [optional-commit-description]
_hint: Git commit
_category: Workflow
_order: 1
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




Create a git commit following project standards

**User arguments:**

Commit: $ARGUMENTS

**End of user arguments**

## Commit Message Rules

Follows [Conventional Commits](https://www.conventionalcommits.org/) standard.

1. **Format**: \`type(#issue): description\`
   - Use \`#123\` for local repo issues
   - Use \`owner/repo#123\` for cross-repo issues
   - Common types: \`feat\`, \`fix\`, \`docs\`, \`refactor\`, \`test\`, \`chore\`

2. **AI Credits**: **NEVER include AI credits in commit messages**
   - No "Generated with Claude Code"
   - No "Co-Authored-By: Claude" or "Co-Authored-By: Happy"
   - Focus on the actual changes made, not conversation history

3. **Content**: Write clear, concise commit messages describing what changed and why

## Process

1. Run \`git status\` and \`git diff\` to review changes
2. Run \`git log --oneline -5\` to see recent commit style
3. Stage relevant files with \`git add\`
4. Create commit with descriptive message
5. Verify with \`git status\`

## Example

\`\`\`bash
git add <files>
git commit -m "feat(#123): add validation to user input form"
\`\`\`

"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for commitlint-checklist-nodejs.md 1`] = `
"---
description: Audit commit hook automation for Node.js projects
argument-hint: (no arguments - interactive)
_hint: Audit commit hooks (Node.js)
_category: Utilities
_order: 25
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).


# Commit Hook Automation Checklist (Node.js)

Scan the Node.js repository and report what commit automation is in place.

**User arguments:**

Commitlint-checklist: $ARGUMENTS

**End of user arguments**

## Checks to Scan

Scan the repository for these configurations (do not display this list to user):

**ðŸ”§ Infrastructure**
- Husky: \`.husky/\`, \`package.json\` â†’ \`"prepare": "husky"\`
- Pre-commit hook: \`.husky/pre-commit\`
- lint-staged: \`lint-staged.config.*\`, \`.lintstagedrc.*\`, or \`package.json\` lint-staged key

**ðŸ§¹ Code Quality**
- Linting: \`eslint.config.*\`, \`biome.json\`, \`.eslintrc.*\`
- Formatting: \`.prettierrc*\`, \`biome.json\`
- Type checking: \`tsconfig.json\` + lint-staged runs \`tsc --noEmit\`
- Knip: \`knip.json\` or knip in scripts
- jscpd: \`.jscpd.json\` or jscpd in scripts

**ðŸ”’ Security**
- Secret scanning: \`.secretlintrc.json\`, secretlint in dependencies

**ðŸ“ Commits**
- Conventional commits: \`.husky/commit-msg\`, \`commitlint.config.*\`, \`@commitlint/*\` in deps

**ðŸ§ª Testing**
- Pre-commit tests: lint-staged or pre-commit hook runs test command
- Coverage thresholds: vitest/jest config with coverage thresholds

## Output Format

Display ONLY this summary format (no tables, no detailed breakdown):

\`\`\`
âœ… Passing:
  ðŸ”§ Infrastructure: Husky, pre-commit hook, lint-staged
  ðŸ§¹ Code Quality: Linting, formatting, type checking, Knip, jscpd
  ðŸ§ª Testing: Pre-commit tests, coverage thresholds

âŒ Missing (2):
  â€¢ Secret scanning (secretlint)
  â€¢ Conventional commits (commitlint)

ðŸ“Š 10/12 checks passing
\`\`\`

Omit categories with no passing checks from the "Passing" section.

## Follow-up

After the summary, tell the user:

> For working examples of most items above, see [hevolx/agent-instructions](https://github.com/hevolx/agent-instructions) on GitHub.
>
> Would you like help implementing any of the missing checks?
"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for create-adr.md 1`] = `
"---
description: Create a new Architecture Decision Record (ADR)
argument-hint: <title or topic of the architectural decision>
_hint: Create ADR
_category: Utilities
_order: 20
---

# Create ADR: Architecture Decision Record Creator

Create a new ADR to document an architectural decision. ADRs capture the "why" behind technical choices, helping future developers understand constraints and tradeoffs.

> ADRs were introduced by Michael Nygard in 2011. The core structure (Context, Decision, Consequences) remains the standard.

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




**User arguments:**

Create-adr: $ARGUMENTS

**End of user arguments**

(If no input provided, ask user for the architectural decision topic)

## Process

### Step 1: Detect Existing ADR Setup

Check for existing ADR directory and structure:

\`\`\`bash
# Check common ADR directories (in order of preference)
for dir in doc/adr docs/adr decisions doc/architecture/decisions; do
  if [ -d "$dir" ]; then
    echo "Found: $dir"
    ls "$dir"/*.md 2>/dev/null
    break
  fi
done
\`\`\`

**If ADRs exist:** Read the first few ADRs (especially 0001 if present) to understand:

- The template/structure this project uses
- Any project-specific sections or frontmatter
- Naming conventions and style

Adapt the new ADR to match the existing pattern.

**If no ADR directory exists:** Run the initialization flow (Step 1b).

### Step 1b: Initialize ADR Practice (First-Time Setup)

When no existing ADRs are found, help the user set up their ADR practice.

Ask the user (use AskUserQuestion):

**Directory location:**

- doc/adr (Recommended - conventional location)
- docs/adr
- decisions
- doc/architecture/decisions

**Template style:**

- Minimal (Nygard's original: Context, Decision, Consequences)
- Standard (Minimal + Status, Date, References)
- With scope (Standard + applies_to patterns, code examples)

**Create a foundational ADR-0001?**

- Yes - document "We will use ADRs to record architectural decisions"
- No - proceed directly to creating the requested ADR

If creating ADR-0001, generate it with:

- Context: Why the team needs to document decisions
- Decision: Adopt ADRs following [chosen template style]
- Consequences: Better knowledge transfer, slight overhead per decision

### Step 2: Determine ADR Number

Calculate next number from existing ADRs:

1. Extract highest existing number
2. Increment by 1
3. Format as 4-digit zero-padded (e.g., \`0001\`, \`0012\`)

### Step 3: Discovery Questions

Gather context through conversation (use AskUserQuestion for structured choices):

**Context & Problem**

- What forces are at play? (technological, social, project constraints)
- What problem, pattern, or situation prompted this decision?
- What triggered the need to decide now? (bug, confusion, inconsistency, new requirement)
- Are there related PRs, issues, or prior discussions to reference?

**The Decision**

- What are we deciding to do (or not do)?
- What alternatives were considered?
- Why was this approach chosen over alternatives?

**Consequences**

- What becomes easier or more consistent with this decision?
- What becomes harder, more constrained, or riskier?
- What tradeoffs are we explicitly accepting?

**Scope**

- Which parts of the codebase does this apply to?
- Are there exceptions or areas where this doesn't apply?

### Step 4: Generate ADR File

Create \`{adr_directory}/NNNN-title-slug.md\`:

- Convert title to kebab-case slug (lowercase, hyphens, no special chars)
- Use today's date for the \`date\` field
- Default status to \`accepted\` (most ADRs are written after the decision is made)

**ADR Template:**

\`\`\`markdown
---
status: accepted
date: YYYY-MM-DD
applies_to:
  - "**/*.ts"
  - "**/*.tsx"
---

# N. Title

## Context

[Forces at play - technological, social, project constraints.
What problem prompted this? Value-neutral description of the situation.]

## Decision

We will [decision statement in active voice].

[If the decision involves code patterns, include concrete examples:]

**Forbidden pattern:**
\\\`\\\`\\\`typescript
// âŒ BAD - [explanation]
[example of what NOT to do]
\\\`\\\`\\\`

**Required pattern:**
\\\`\\\`\\\`typescript
// âœ… GOOD - [explanation]
[example of what TO do]
\\\`\\\`\\\`

## Consequences

**Positive:**
- [What becomes easier]
- [What becomes more consistent]

**Negative:**
- [What becomes harder]
- [What constraints we accept]

**Neutral:**
- [Other impacts worth noting]

## References

- [Related PRs, issues, or documentation]
\`\`\`

### Step 5: Refine applies_to Scope

Help user define which files this decision applies to using glob patterns:

- All TypeScript files: **/*.ts
- All React component files: **/*.tsx
- Only files in components directory: src/components/**
- Exclude test files (prefix with !): !**/*.test.ts
- Exclude type definition files: !**/*.d.ts
- Specific package only: packages/api/**

If the decision applies broadly, use **/* (all files).

**Note**: \`applies_to\` is recommended for the "With scope" template. Linters and AI assistants use these patterns to determine which files to check against this ADR.

### Step 6: Confirm and Write

Show the complete ADR content and ask user to confirm before writing.

After creation, suggest:

- Review the ADR for completeness
- Commit with \`/commit\`

## Tips for Good ADRs

1. **Focus on the "why"** - The decision itself may be obvious; the reasoning often isn't
2. **Keep it concise** - 1-2 pages maximum; should be readable in 5 minutes
3. **Use active voice** - "We will use X" not "X will be used"
4. **Include concrete examples** - Code examples make abstract decisions tangible
5. **Document tradeoffs honestly** - Every decision has costs; be explicit about them
6. **Link to context** - Reference PRs, issues, or discussions where the decision was made
7. **Be specific about scope** - Use \`applies_to\` patterns to clarify affected code

## Status Values

| Status | When to Use |
|--------|-------------|
| \`proposed\` | Under discussion, not yet agreed |
| \`accepted\` | Agreed upon and should be followed |
| \`deprecated\` | No longer relevant (context changed) |
| \`superseded\` | Replaced by another ADR (link to it) |

To supersede an existing ADR:

1. Create new ADR with the updated decision
2. Update old ADR's status to \`superseded by ADR-NNNN\`

## Integration with Other Commands

- After creating: Commit with \`/commit\`
- If decision needs discussion: Create issue with \`/create-issues\`
"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for create-issues.md 1`] = `
"---
description: Create implementation plan from feature/requirement with PRD-style discovery and TDD acceptance criteria
argument-hint: <feature/requirement description or GitHub issue URL/number>
_hint: Create issues
_category: Planning
_order: 2
---

# Create Issues: PRD-Informed Task Planning for TDD

Create structured implementation plan that bridges product thinking (PRD) with test-driven development.

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




**User arguments:**

Create-issues: $ARGUMENTS

**End of user arguments**

(If no input provided, check conversation context or run \`bd ready\` to see existing work)

## Input Processing

The input can be one of:
1. **GitHub Issue URL** (e.g., \`https://github.com/owner/repo/issues/123\`)
2. **GitHub Issue Number** (e.g., \`#123\` or \`123\`)
3. **Feature Description** (e.g., "Add user authentication")
4. **Empty** - use conversation context

### GitHub Issue Integration

If input looks like a GitHub issue:

**Step 1: Extract Issue Number**
- From URL: extract owner/repo/number
- From number: try to infer repo from git remote
- From branch name: check patterns like \`issue-123\`, \`123-feature\`, \`feature/123\`

**Step 2: Fetch Issue**



Try to fetch the issue using GitHub MCP (mcp__github__issue_read tool).

If GitHub MCP is not configured, show:
\`\`\`
GitHub MCP not configured!
See: https://github.com/modelcontextprotocol/servers/tree/main/src/github
Trying GitHub CLI fallback...
\`\`\`

Then try using \`gh issue view [ISSUE_NUMBER] --json\` as fallback.


**Step 3: Use Issue as Discovery Input**
- Title â†’ Feature name
- Description â†’ Problem statement and context
- Labels â†’ Type/priority hints
- Comments â†’ Additional requirements and discussion
- Linked issues â†’ Dependencies

Extract from GitHub issue:
- Problem statement and context
- Acceptance criteria (if present)
- Technical notes (if present)
- Related issues/dependencies

## Process

## Discovery Phase

Understand the requirement by asking (use AskUserQuestion if needed):

**Problem Statement**
- What problem does this solve?
- Who experiences this problem?
- What's the current pain point?

**Desired Outcome**
- What should happen after this is built?
- How will users interact with it?
- What does success look like?

**Scope & Constraints**
- What's in scope vs. out of scope?
- Any technical constraints?
- Dependencies on other systems/features?

**Context Check**
- Search codebase for related features/modules
- Check for existing test files that might be relevant


### Create Beads Issues

For each task, create a bd issue with:

\`\`\`bash
bd create "Task title" \\
  --type [feature|bug|task|chore] \\
  --priority [1-3] \\
  --description "Context and what needs to be built" \\
  --design "Technical approach, architecture notes" \\
  --acceptance "Given-When-Then acceptance criteria"
\`\`\`

**Issue Structure Best Practices:**

**Title**: Action-oriented, specific
- âœ… "Add JWT token validation middleware"
- âŒ "Authentication stuff"

**Description**: Provide context
- Why this task exists
- How it fits into the larger feature
- Links to related issues/docs

**Design**: Technical approach
- Key interfaces/types needed
- Algorithm or approach
- Libraries or patterns to use
- Known gotchas or considerations

**Acceptance Criteria**: Test-ready scenarios
- Given-When-Then format
- Concrete, verifiable conditions
- Cover main case + edge cases
- Map 1:1 to future tests

**Dependencies**: Link related issues
\`\`\`bash
bd dep add ISSUE-123 ISSUE-456 --type blocks
\`\`\`

### Validation

After creating issues, verify:
- âœ… Each issue has clear acceptance criteria
- âœ… Dependencies are mapped (use \`bd dep add\`)
- âœ… Issues are ordered by implementation sequence
- âœ… First few issues are ready to start (\`bd ready\` shows them)
- âœ… Each issue is small enough for TDD (if too big, break down more)


## Key Principles

**From PRD World:**
- Start with user problems, not solutions
- Define success criteria upfront
- Understand constraints and scope

**From TDD World:**
- Make acceptance criteria test-ready
- Break work into small, testable pieces
- Each task should map to test(s)

### Beads Integration

Use Beads MCP to:
- Track work with \`bd ready\` to find next task
- Create issues with \`bd create "description"\`
- Track dependencies with \`bd dep add\`

See https://github.com/steveyegge/beads for more information.


## Integration with Other Commands

- **Before /create-issues**: Use \`/spike\` if you need technical exploration first
- **After /create-issues**: Use \`/red\` to start TDD on first task
- **During work**: Use \`bd update\` to add notes/findings back to issues
- **When stuck**: Check \`bd show ISSUE-ID\` to review acceptance criteria

"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for cycle.md 1`] = `
"---
description: Execute complete TDD cycle - Red, Green, and Refactor phases in sequence
argument-hint: <feature or requirement description>
_hint: Full TDD cycle
_category: Test-Driven Development
_order: 5
---

**User arguments:**

Cycle: $ARGUMENTS

**End of user arguments**

RED+GREEN+REFACTOR (one cycle) PHASE! Apply the below to the user input above.

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




(If there was no info above, fallback to:
1. Context of the conversation, if there's an immediate thing
2. \`bd ready\` to see what to work on next and start from there)


## TDD Fundamentals

### The TDD Cycle

The foundation of TDD is the Red-Green-Refactor cycle:

1. **Red Phase**: Write ONE failing test that describes desired behavior

   - The test must fail for the RIGHT reason (not syntax/import errors)
   - Only one test at a time - this is critical for TDD discipline
     - Exception: For browser-level tests or expensive setup (e.g., Storybook \`*.stories.tsx\`), group multiple assertions within a single test block to avoid redundant setup - but only when adding assertions to an existing interaction flow. If new user interactions are required, still create a new test. Split files by category if they exceed ~1000 lines.
   - **Adding a single test to a test file is ALWAYS allowed** - no prior test output needed
   - Starting TDD for a new feature is always valid, even if test output shows unrelated work
   - For DOM-based tests, use \`data-testid\` attributes to select elements rather than CSS classes, tag names, or text content
   - Avoid hard-coded timeouts both in form of sleep() or timeout: 5000 etc; use proper async patterns (\`waitFor\`, \`findBy*\`, event-based sync) instead and rely on global test configs for timeout settings

2. **Green Phase**: Write MINIMAL code to make the test pass

   - Implement only what's needed for the current failing test
   - No anticipatory coding or extra features
   - Address the specific failure message

3. **Refactor Phase**: Improve code structure while keeping tests green
   - Only allowed when relevant tests are passing
   - Requires proof that tests have been run and are green
   - Applies to BOTH implementation and test code
   - No refactoring with failing tests - fix them first

### Core Violations

1. **Multiple Test Addition**

   - Adding more than one new test at once
   - Exception: Initial test file setup or extracting shared test utilities

2. **Over-Implementation**

   - Code that exceeds what's needed to pass the current failing test
   - Adding untested features, methods, or error handling
   - Implementing multiple methods when test only requires one

3. **Premature Implementation**
   - Adding implementation before a test exists and fails properly
   - Adding implementation without running the test first
   - Refactoring when tests haven't been run or are failing

### Critical Principle: Incremental Development

Each step in TDD should address ONE specific issue:

- Test fails "not defined" â†’ Create empty stub/class only
- Test fails "not a function" â†’ Add method stub only
- Test fails with assertion â†’ Implement minimal logic only

### Optional Pre-Phase: Spike Phase

In rare cases where the problem space, interface, or expected behavior is unclear, a **Spike Phase** may be used **before the Red Phase**.
This phase is **not part of the regular TDD workflow** and must only be applied under exceptional circumstances.

- The goal of a Spike is **exploration and learning**, not implementation.
- The code written during a Spike is **disposable** and **must not** be merged or reused directly.
- Once sufficient understanding is achieved, all spike code is discarded, and normal TDD resumes starting from the **Red Phase**.
- A Spike is justified only when it is impossible to define a meaningful failing test due to technical uncertainty or unknown system behavior.

### General Information

- Sometimes the test output shows as no tests have been run when a new test is failing due to a missing import or constructor. In such cases, allow the agent to create simple stubs. Ask them if they forgot to create a stub if they are stuck.
- It is never allowed to introduce new logic without evidence of relevant failing tests. However, stubs and simple implementation to make imports and test infrastructure work is fine.
- In the refactor phase, it is perfectly fine to refactor both test and implementation code. That said, completely new functionality is not allowed. Types, clean up, abstractions, and helpers are allowed as long as they do not introduce new behavior.
- Adding types, interfaces, or a constant in order to replace magic values is perfectly fine during refactoring.
- Provide the agent with helpful directions so that they do not get stuck when blocking them.

"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for forever.md 1`] = `
"---
description: Run autonomously until stopped or stuck
argument-hint: [optional: initial task or focus area]
_hint: Autonomous mode
_category: Workflow
_order: 20
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).


**User arguments:**

Forever: $ARGUMENTS

**End of user arguments**

Run autonomously, finding and completing work until interrupted or truly stuck.

## Operating Loop

Execute this cycle continuously:

### 1. Find Work

Check in order until something is found:

1. **Arguments above** - If provided, start there
2. **Conversation context** - Any unfinished discussion
3. **Gaps** - Incomplete implementations or missing tests
4. **Git status** - Uncommitted changes to address
5. **Think** - If nothing else, consider: What would improve this codebase?

### 2. Execute Work

- Make atomic, committable progress
- Leave clear trail via commits

### 3. Continue or Pivot

After completing a unit of work:

- If more related work exists - continue
- If blocked - note blocker, find different work
- If genuinely nothing to do - report status and wait

**Do not stop unless:**

- User interrupts (Escape)
- Genuinely no work can be identified
- A decision requires human judgment (ambiguous requirements, architectural choices)

## Anti-Stuck Tactics

When progress stalls:

| Situation | Action |
|-----------|--------|
| Test failures | Make the failing tests pass |
| Unclear requirements | Make reasonable assumption, document it, proceed |
| Build errors | Fix incrementally, commit fixes |
| Context confusion | Re-read recent commits and task tracker to reorient |
| Repeated failures | Try different approach, or move to different task |

## Work Discovery Heuristics

When thinking about what to do:

- Tests that could be added (coverage gaps)
- Code that could be simplified
- Documentation that's missing or stale
- TODOs or FIXMEs in code
- Dependencies that could be updated
- Performance improvements
- Refactoring opportunities

## Session Continuity

Every few completed tasks:

- Update task tracker with progress notes
- Commit work in progress
- Brief self-summary of what's been done

This ensures work survives context limits.

## Communication Style

- Work silently - don't narrate every step
- Report meaningful completions (commits, closed issues)
- Surface decisions that need human input
- Keep responses concise to preserve context
"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for gap.md 1`] = `
"---
description: Analyze conversation context for unaddressed items and gaps
argument-hint: [optional additional info]
_hint: Find gaps
_category: Workflow
_order: 11
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




Analyze the current conversation context and identify things that have not yet been addressed. Look for:

1. **Incomplete implementations** - Code that was started but not finished
2. **Unused variables/results** - Values that were captured but never used
3. **Missing tests** - Functionality without test coverage
4. **Open issues** - Beads issues that are still open or in progress

4. **User requests** - Things the user asked for that weren't fully completed
5. **TODO comments** - Any TODOs mentioned in conversation
6. **Error handling gaps** - Missing error cases or edge cases
7. **Documentation gaps** - Undocumented APIs or features
8. **Consistency check** - Look for inconsistent patterns, naming conventions, or structure across the codebase


Present findings as a prioritized list with:

- What the gap is
- Why it matters
- Suggested next action

If there are no gaps, confirm that everything discussed has been addressed.

**User arguments:**

Gap: $ARGUMENTS

**End of user arguments**
"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for green.md 1`] = `
"---
description: Execute TDD Green Phase - write minimal implementation to pass the failing test
argument-hint: <implementation description>
_hint: Make it pass
_category: Test-Driven Development
_order: 3
---

**User arguments:**

Green: $ARGUMENTS

**End of user arguments**

GREEN PHASE! Apply the below to the user input above.

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




(If there was no info above, fallback to:
1. Context of the conversation, if there's an immediate thing
2. \`bd ready\` to see what to work on next and start from there)


## TDD Fundamentals

### The TDD Cycle

The foundation of TDD is the Red-Green-Refactor cycle:

1. **Red Phase**: Write ONE failing test that describes desired behavior

   - The test must fail for the RIGHT reason (not syntax/import errors)
   - Only one test at a time - this is critical for TDD discipline
     - Exception: For browser-level tests or expensive setup (e.g., Storybook \`*.stories.tsx\`), group multiple assertions within a single test block to avoid redundant setup - but only when adding assertions to an existing interaction flow. If new user interactions are required, still create a new test. Split files by category if they exceed ~1000 lines.
   - **Adding a single test to a test file is ALWAYS allowed** - no prior test output needed
   - Starting TDD for a new feature is always valid, even if test output shows unrelated work
   - For DOM-based tests, use \`data-testid\` attributes to select elements rather than CSS classes, tag names, or text content
   - Avoid hard-coded timeouts both in form of sleep() or timeout: 5000 etc; use proper async patterns (\`waitFor\`, \`findBy*\`, event-based sync) instead and rely on global test configs for timeout settings

2. **Green Phase**: Write MINIMAL code to make the test pass

   - Implement only what's needed for the current failing test
   - No anticipatory coding or extra features
   - Address the specific failure message

3. **Refactor Phase**: Improve code structure while keeping tests green
   - Only allowed when relevant tests are passing
   - Requires proof that tests have been run and are green
   - Applies to BOTH implementation and test code
   - No refactoring with failing tests - fix them first

### Core Violations

1. **Multiple Test Addition**

   - Adding more than one new test at once
   - Exception: Initial test file setup or extracting shared test utilities

2. **Over-Implementation**

   - Code that exceeds what's needed to pass the current failing test
   - Adding untested features, methods, or error handling
   - Implementing multiple methods when test only requires one

3. **Premature Implementation**
   - Adding implementation before a test exists and fails properly
   - Adding implementation without running the test first
   - Refactoring when tests haven't been run or are failing

### Critical Principle: Incremental Development

Each step in TDD should address ONE specific issue:

- Test fails "not defined" â†’ Create empty stub/class only
- Test fails "not a function" â†’ Add method stub only
- Test fails with assertion â†’ Implement minimal logic only

### Optional Pre-Phase: Spike Phase

In rare cases where the problem space, interface, or expected behavior is unclear, a **Spike Phase** may be used **before the Red Phase**.
This phase is **not part of the regular TDD workflow** and must only be applied under exceptional circumstances.

- The goal of a Spike is **exploration and learning**, not implementation.
- The code written during a Spike is **disposable** and **must not** be merged or reused directly.
- Once sufficient understanding is achieved, all spike code is discarded, and normal TDD resumes starting from the **Red Phase**.
- A Spike is justified only when it is impossible to define a meaningful failing test due to technical uncertainty or unknown system behavior.

### General Information

- Sometimes the test output shows as no tests have been run when a new test is failing due to a missing import or constructor. In such cases, allow the agent to create simple stubs. Ask them if they forgot to create a stub if they are stuck.
- It is never allowed to introduce new logic without evidence of relevant failing tests. However, stubs and simple implementation to make imports and test infrastructure work is fine.
- In the refactor phase, it is perfectly fine to refactor both test and implementation code. That said, completely new functionality is not allowed. Types, clean up, abstractions, and helpers are allowed as long as they do not introduce new behavior.
- Adding types, interfaces, or a constant in order to replace magic values is perfectly fine during refactoring.
- Provide the agent with helpful directions so that they do not get stuck when blocking them.

"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for issue.md 1`] = `
"---
description: Analyze GitHub issue and create TDD implementation plan
argument-hint: [optional-issue-number]
_hint: Analyze issue
_category: Planning
_order: 1
---

Analyze GitHub issue and create TDD implementation plan.

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




Process:

1. Get Issue Number

**User arguments:**

Issue: $ARGUMENTS

**End of user arguments**

- Check if argument is an issue number
- Otherwise try branch name patterns: issue-123, 123-feature, feature/123, fix/123
- If not found: ask user

2. Fetch Issue



Try to fetch the issue using GitHub MCP (mcp__github__issue_read tool).

If GitHub MCP is not configured, show:
\`\`\`
GitHub MCP not configured!
See: https://github.com/modelcontextprotocol/servers/tree/main/src/github
Trying GitHub CLI fallback...
\`\`\`

Then try using \`gh issue view [ISSUE_NUMBER] --json\` as fallback.


3. Analyze and Plan

Summarize the issue and requirements, then:

## Discovery Phase

Understand the requirement by asking (use AskUserQuestion if needed):

**Problem Statement**
- What problem does this solve?
- Who experiences this problem?
- What's the current pain point?

**Desired Outcome**
- What should happen after this is built?
- How will users interact with it?
- What does success look like?

**Scope & Constraints**
- What's in scope vs. out of scope?
- Any technical constraints?
- Dependencies on other systems/features?

**Context Check**
- Search codebase for related features/modules
- Check for existing test files that might be relevant


### Beads Integration

Use Beads MCP to:
- Track work with \`bd ready\` to find next task
- Create issues with \`bd create "description"\`
- Track dependencies with \`bd dep add\`

See https://github.com/steveyegge/beads for more information.


## TDD Fundamentals

### The TDD Cycle

The foundation of TDD is the Red-Green-Refactor cycle:

1. **Red Phase**: Write ONE failing test that describes desired behavior

   - The test must fail for the RIGHT reason (not syntax/import errors)
   - Only one test at a time - this is critical for TDD discipline
     - Exception: For browser-level tests or expensive setup (e.g., Storybook \`*.stories.tsx\`), group multiple assertions within a single test block to avoid redundant setup - but only when adding assertions to an existing interaction flow. If new user interactions are required, still create a new test. Split files by category if they exceed ~1000 lines.
   - **Adding a single test to a test file is ALWAYS allowed** - no prior test output needed
   - Starting TDD for a new feature is always valid, even if test output shows unrelated work
   - For DOM-based tests, use \`data-testid\` attributes to select elements rather than CSS classes, tag names, or text content
   - Avoid hard-coded timeouts both in form of sleep() or timeout: 5000 etc; use proper async patterns (\`waitFor\`, \`findBy*\`, event-based sync) instead and rely on global test configs for timeout settings

2. **Green Phase**: Write MINIMAL code to make the test pass

   - Implement only what's needed for the current failing test
   - No anticipatory coding or extra features
   - Address the specific failure message

3. **Refactor Phase**: Improve code structure while keeping tests green
   - Only allowed when relevant tests are passing
   - Requires proof that tests have been run and are green
   - Applies to BOTH implementation and test code
   - No refactoring with failing tests - fix them first

### Core Violations

1. **Multiple Test Addition**

   - Adding more than one new test at once
   - Exception: Initial test file setup or extracting shared test utilities

2. **Over-Implementation**

   - Code that exceeds what's needed to pass the current failing test
   - Adding untested features, methods, or error handling
   - Implementing multiple methods when test only requires one

3. **Premature Implementation**
   - Adding implementation before a test exists and fails properly
   - Adding implementation without running the test first
   - Refactoring when tests haven't been run or are failing

### Critical Principle: Incremental Development

Each step in TDD should address ONE specific issue:

- Test fails "not defined" â†’ Create empty stub/class only
- Test fails "not a function" â†’ Add method stub only
- Test fails with assertion â†’ Implement minimal logic only

### Optional Pre-Phase: Spike Phase

In rare cases where the problem space, interface, or expected behavior is unclear, a **Spike Phase** may be used **before the Red Phase**.
This phase is **not part of the regular TDD workflow** and must only be applied under exceptional circumstances.

- The goal of a Spike is **exploration and learning**, not implementation.
- The code written during a Spike is **disposable** and **must not** be merged or reused directly.
- Once sufficient understanding is achieved, all spike code is discarded, and normal TDD resumes starting from the **Red Phase**.
- A Spike is justified only when it is impossible to define a meaningful failing test due to technical uncertainty or unknown system behavior.

### General Information

- Sometimes the test output shows as no tests have been run when a new test is failing due to a missing import or constructor. In such cases, allow the agent to create simple stubs. Ask them if they forgot to create a stub if they are stuck.
- It is never allowed to introduce new logic without evidence of relevant failing tests. However, stubs and simple implementation to make imports and test infrastructure work is fine.
- In the refactor phase, it is perfectly fine to refactor both test and implementation code. That said, completely new functionality is not allowed. Types, clean up, abstractions, and helpers are allowed as long as they do not introduce new behavior.
- Adding types, interfaces, or a constant in order to replace magic values is perfectly fine during refactoring.
- Provide the agent with helpful directions so that they do not get stuck when blocking them.

"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for kata.md 1`] = `
"---
description: Generate a TDD practice challenge with boilerplate test setup
argument-hint: (no arguments - interactive)
_hint: Practice kata
_category: Utilities
_order: 10
_requested-tools:
  - WebFetch(domain:raw.githubusercontent.com)
  - WebFetch(domain:api.github.com)
---

# Kata: TDD Practice Challenge Generator

Generate a complete TDD practice setup:
- \`CHALLENGE.md\` - Problem description
- Test file with first test placeholder
- Implementation file with empty function

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




**User arguments:**

Kata: $ARGUMENTS

**End of user arguments**

(This command is interactive - arguments are ignored)

## Data Source

Exercises from [cyber-dojo](https://github.com/cyber-dojo/exercises-start-points) (60 exercises, stable 10+ years).

- **List:** \`https://api.github.com/repos/cyber-dojo/exercises-start-points/contents/start-points\`
- **Fetch:** \`https://raw.githubusercontent.com/cyber-dojo/exercises-start-points/master/start-points/{NAME}/readme.txt\`

## Kata Categories

<kata_categories>
<by_difficulty>
  <beginner description="Simple logic, single function">
    Fizz_Buzz, Leap_Years, Prime_Factors, Word_Wrap, Closest_To_Zero,
    Remove_Duplicates, Array_Shuffle, Friday_13th, Five_Weekends,
    Number_Names, Print_Diamond, LCD_Digits, Fisher_Yates_Shuffle
  </beginner>
  <intermediate description="Multiple rules, edge cases">
    Roman_Numerals, Reverse_Roman, Bowling_Game, Tennis, Anagrams,
    ISBN, Balanced_Parentheses, Calc_Stats, Recently_Used_List,
    Phone_Numbers, Combined_Number, Group_Neighbours, Longest_Common_Prefix,
    Yatzy, Yatzy_Cutdown, Harry_Potter, Vending_Machine, Count_Coins,
    100_doors, Haiku_Review, ABC_Problem, Align_Columns, Best_Shuffle,
    Filename_Range, Unsplice, 12_Days_of_Xmas
  </intermediate>
  <advanced description="Complex algorithms, state management">
    Game_of_Life, Mars_Rover, Mine_Sweeper, Mine_Field, Eight_Queens,
    Knights_Tour, Reversi, Poker_Hands, Levenshtein_Distance,
    Magic_Square, Saddle_Points, Tiny_Maze, Gray_Code, Monty_Hall,
    Number_Chains, Wonderland_Number, Zeckendorf_Number,
    Diff_Selector, Diversion, Reordering, Fizz_Buzz_Plus
  </advanced>
</by_difficulty>

<by_type>
  <string_text description="String/Text manipulation">
    Anagrams, Word_Wrap, Align_Columns, Best_Shuffle, Haiku_Review,
    Phone_Numbers, 12_Days_of_Xmas, Longest_Common_Prefix, Unsplice
  </string_text>
  <math_numbers description="Math & Numbers">
    Fizz_Buzz, Fizz_Buzz_Plus, Prime_Factors, Roman_Numerals, Reverse_Roman,
    Zeckendorf_Number, Number_Names, Combined_Number, Closest_To_Zero,
    Count_Coins, ISBN, LCD_Digits, Leap_Years, Five_Weekends, Friday_13th
  </math_numbers>
  <data_structures description="Data Structures & Algorithms">
    Recently_Used_List, Balanced_Parentheses, Calc_Stats, Remove_Duplicates,
    Array_Shuffle, Fisher_Yates_Shuffle, Levenshtein_Distance, Gray_Code,
    Number_Chains, Wonderland_Number, 100_doors, Magic_Square, Saddle_Points,
    Diff_Selector, Diversion, Reordering
  </data_structures>
  <game_logic description="Game Logic & Simulation">
    Game_of_Life, Bowling_Game, Tennis, Yatzy, Yatzy_Cutdown,
    Mine_Sweeper, Mine_Field, Mars_Rover, Reversi, Poker_Hands,
    Harry_Potter, Eight_Queens, Knights_Tour, Tiny_Maze, Monty_Hall,
    Vending_Machine, Print_Diamond, ABC_Problem, Group_Neighbours, Filename_Range
  </game_logic>
</by_type>
</kata_categories>

## Process

Use conversational AskUserQuestion flow. User can skip to suggestions at any point.

<execution_steps>

<step_1>
  <description>Ask about difficulty preference</description>
  <prompt>
    <message>What difficulty level interests you?</message>
    <options>
      <option value="beginner">
        <label>Beginner</label>
        <description>Simple logic, single function (Fizz_Buzz, Leap_Years, Prime_Factors...)</description>
      </option>
      <option value="intermediate">
        <label>Intermediate</label>
        <description>Multiple rules, edge cases (Roman_Numerals, Bowling_Game, Tennis...)</description>
      </option>
      <option value="advanced">
        <label>Advanced</label>
        <description>Complex algorithms, state (Game_of_Life, Mars_Rover, Mine_Sweeper...)</description>
      </option>
      <option value="skip">
        <label>Just show me suggestions!</label>
        <description>Skip questions and see kata recommendations</description>
      </option>
    </options>
  </prompt>
  <if value="skip">Jump to step_suggest</if>
  <set_variable>$DIFFICULTY = selected value</set_variable>
</step_1>

<step_2>
  <description>Ask about challenge type</description>
  <prompt>
    <message>What kind of challenge interests you?</message>
    <options>
      <option value="string_text">
        <label>String/Text</label>
        <description>Text manipulation, parsing, formatting</description>
      </option>
      <option value="math_numbers">
        <label>Math & Numbers</label>
        <description>Calculations, conversions, number theory</description>
      </option>
      <option value="data_structures">
        <label>Data Structures</label>
        <description>Lists, algorithms, transformations</description>
      </option>
      <option value="game_logic">
        <label>Game Logic</label>
        <description>Games, simulations, state machines</description>
      </option>
      <option value="skip">
        <label>Show me suggestions now!</label>
        <description>Skip remaining questions</description>
      </option>
    </options>
  </prompt>
  <if value="skip">Jump to step_suggest</if>
  <set_variable>$TYPE = selected value</set_variable>
</step_2>

<step_suggest>
  <description>Show kata suggestions based on filters</description>
  <logic>
    - Filter kata_categories by $DIFFICULTY (if set)
    - Filter by $TYPE (if set)
    - If no filters, pick 3 varied suggestions
    - Select 3 katas that match criteria
    - For each kata, fetch a preview or use the descriptions below
  </logic>
  <kata_descriptions>
    <!-- Beginner -->
    Fizz_Buzz: Print 1-100 replacing multiples of 3/5 with Fizz/Buzz
    Leap_Years: Determine if a year is a leap year (divisibility rules)
    Prime_Factors: Find prime factors of a number
    Word_Wrap: Wrap text to fit within a column width
    Closest_To_Zero: Find the number closest to zero from a list
    Remove_Duplicates: Remove duplicate elements from a list
    Array_Shuffle: Randomly shuffle array elements
    Friday_13th: Count Friday the 13ths in a given year
    Five_Weekends: Find months with 5 Fridays, Saturdays, and Sundays
    Number_Names: Convert numbers to English words (one, two, three...)
    Print_Diamond: Print a diamond shape of letters
    LCD_Digits: Display numbers as LCD-style digits
    Fisher_Yates_Shuffle: Implement the Fisher-Yates shuffle algorithm

    <!-- Intermediate -->
    Roman_Numerals: Convert Arabic numbers to Roman numerals
    Reverse_Roman: Convert Roman numerals back to Arabic numbers
    Bowling_Game: Score a 10-pin bowling game with spares/strikes
    Tennis: Track tennis game score with deuce/advantage
    Anagrams: Find all anagrams of a word from a dictionary
    ISBN: Validate ISBN-10 check digits
    Balanced_Parentheses: Check if brackets are properly balanced
    Calc_Stats: Calculate min/max/count/average from numbers
    Recently_Used_List: Implement a most-recently-used list
    Phone_Numbers: Convert phone numbers to words using keypad letters
    Combined_Number: Arrange numbers to form the largest combined number
    Group_Neighbours: Group adjacent equal elements in a list
    Longest_Common_Prefix: Find longest common prefix among strings
    Yatzy: Score a Yatzy dice game (full version)
    Yatzy_Cutdown: Simplified Yatzy scoring
    Harry_Potter: Calculate discounts for Harry Potter book sets
    Vending_Machine: Calculate change for vending machine purchases
    Count_Coins: Count ways to make change with given coins
    100_doors: Toggle 100 doors puzzle (open/close pattern)
    Haiku_Review: Validate haiku syllable structure (5-7-5)
    ABC_Problem: Spell words using lettered blocks
    Align_Columns: Align text into formatted columns
    Best_Shuffle: Shuffle string so no character stays in place
    Filename_Range: Generate filename sequences (file001, file002...)
    Unsplice: Reverse a string splice operation
    12_Days_of_Xmas: Generate "12 Days of Christmas" lyrics

    <!-- Advanced -->
    Game_of_Life: Conway's cellular automaton simulation
    Mars_Rover: Navigate a rover on a grid with commands (L/R/M)
    Mine_Sweeper: Generate numbers for a minesweeper grid
    Mine_Field: Place mines randomly on a grid
    Eight_Queens: Place 8 queens on a chessboard without conflicts
    Knights_Tour: Find a path visiting all squares on a chessboard
    Reversi: Implement Reversi/Othello game logic
    Poker_Hands: Compare and rank poker hands
    Levenshtein_Distance: Calculate edit distance between strings
    Magic_Square: Generate magic squares (rows/cols/diags sum equal)
    Saddle_Points: Find saddle points in a matrix
    Tiny_Maze: Solve a small maze pathfinding problem
    Gray_Code: Generate Gray code sequences
    Monty_Hall: Simulate the Monty Hall probability problem
    Number_Chains: Find chains where numbers link by digits
    Wonderland_Number: Find 6-digit number with multiplication property
    Zeckendorf_Number: Represent as sum of non-consecutive Fibonacci numbers
    Diff_Selector: Select differences between data sets
    Diversion: Route around obstacles
    Reordering: Reorder elements based on rules
    Fizz_Buzz_Plus: Extended FizzBuzz with additional rules
  </kata_descriptions>
  <prompt>
    <message>Here are 3 katas that match your criteria:</message>
    <options>
      <option value="kata_1">
        <label>{Kata_Name_1}</label>
        <description>{Description from kata_descriptions}</description>
      </option>
      <option value="kata_2">
        <label>{Kata_Name_2}</label>
        <description>{Description from kata_descriptions}</description>
      </option>
      <option value="kata_3">
        <label>{Kata_Name_3}</label>
        <description>{Description from kata_descriptions}</description>
      </option>
      <option value="more_suggestions">
        <label>Show me 3 different ones</label>
        <description>Same criteria, different suggestions</description>
      </option>
      <option value="more_questions">
        <label>Ask me more questions...</label>
        <description>Refine my preferences further</description>
      </option>
    </options>
  </prompt>
  <if value="more_suggestions">Pick 3 different katas matching same criteria, repeat step_suggest</if>
  <if value="more_questions">
    - If user skipped from step_1: Jump to step_1 (ask difficulty)
    - If user answered step_1 but skipped step_2: Jump to step_2 (ask type)
    - If user answered both: Jump to step_1 to reconsider from start
  </if>
  <set_variable>$SELECTED_KATA = selected kata name</set_variable>
</step_suggest>

<step_fetch>
  <description>Fetch kata content from cyber-dojo</description>
  <action>
    Use WebFetch to get:
    https://raw.githubusercontent.com/cyber-dojo/exercises-start-points/master/start-points/{$SELECTED_KATA}/readme.txt
  </action>
  <store>$KATA_CONTENT = fetched readme content</store>
</step_fetch>

<step_confirm>
  <description>Show full kata details and ask for language</description>
  <display>
    Present the full kata content to the user:

    ## {$SELECTED_KATA}

    **Difficulty:** {$DIFFICULTY_STARS based on category - â­ Beginner, â­â­ Intermediate, â­â­â­ Advanced}

    ### Problem Description
    {$KATA_CONTENT from readme.txt}
  </display>
  <prompt>
    <message>What language will you use for this kata?</message>
    <options>
      <option value="typescript">
        <label>TypeScript + Vitest</label>
        <description>Modern test runner with great DX</description>
      </option>
      <option value="javascript">
        <label>JavaScript + Vitest</label>
        <description>Same great runner, no types</description>
      </option>
      <option value="python">
        <label>Python + pytest</label>
        <description>Simple and powerful testing</description>
      </option>
      <option value="back">
        <label>Show me other katas</label>
        <description>Go back to suggestions</description>
      </option>
    </options>
  </prompt>
  <note>User can also select "Other" to type a custom language/framework</note>
  <if value="back">Jump back to step_suggest</if>
  <set_variable>$LANGUAGE = selected value (or custom input)</set_variable>
</step_confirm>

<step_generate>
  <description>Generate kata files based on language</description>
  <action>Create the following files based on $LANGUAGE:</action>

  <file name="CHALLENGE.md">
# Kata: {$SELECTED_KATA}

## Difficulty
{$DIFFICULTY_STARS based on category}

## Problem
{$KATA_CONTENT from readme.txt}

## TDD Approach

Work through this kata using the red-green-refactor cycle:

1. **Red**: Write a failing test for the simplest case
2. **Green**: Write minimal code to pass
3. **Refactor**: Clean up while keeping tests green
4. **Repeat**: Add the next test case

## Source

[cyber-dojo: {$SELECTED_KATA}](https://cyber-dojo.org)
  </file>

  <file name="kata.ts" condition="$LANGUAGE == typescript">
/**
 * {$SELECTED_KATA}
 * See CHALLENGE.md for requirements
 */
export function solve(input: unknown): unknown {
  throw new Error("Not implemented - start with a failing test!");
}
  </file>

  <file name="kata.test.ts" condition="$LANGUAGE == typescript">
import { describe, it, expect } from "vitest";
import { solve } from "./kata";

describe("{$SELECTED_KATA}", () => {
  it.todo("should handle the simplest case");

  // Add your first real test here using the red-green-refactor cycle
});
  </file>

  <file name="kata.js" condition="$LANGUAGE == javascript">
/**
 * {$SELECTED_KATA}
 * See CHALLENGE.md for requirements
 */
export function solve(input) {
  throw new Error("Not implemented - start with a failing test!");
}
  </file>

  <file name="kata.test.js" condition="$LANGUAGE == javascript">
import { describe, it, expect } from "vitest";
import { solve } from "./kata.js";

describe("{$SELECTED_KATA}", () => {
  it.todo("should handle the simplest case");

  // Add your first real test here using the red-green-refactor cycle
});
  </file>

  <file name="kata.py" condition="$LANGUAGE == python">
"""
{$SELECTED_KATA}
See CHALLENGE.md for requirements
"""

def solve(input):
    raise NotImplementedError("Start with a failing test!")
  </file>

  <file name="test_kata.py" condition="$LANGUAGE == python">
import pytest
from kata import solve

class TestKata:
    def test_placeholder(self):
        """Remove this and add your first real test"""
        pytest.skip("Start with a failing test!")

    # Add your first real test here using the red-green-refactor cycle
  </file>

  <custom_language condition="$LANGUAGE is custom input (user typed via 'Other')">
    Generate appropriate boilerplate based on user's specified language:
    - CHALLENGE.md (always)
    - Implementation file with idiomatic naming and empty function
    - Test file using common test framework for that language

    Examples:
    - "Go" â†’ kata.go + kata_test.go (testing package)
    - "Rust" â†’ src/lib.rs + tests/kata_test.rs (cargo test)
    - "Java" â†’ Kata.java + KataTest.java (JUnit)
    - "C#" â†’ Kata.cs + KataTests.cs (xUnit/NUnit)
    - "Ruby" â†’ kata.rb + kata_spec.rb (RSpec)
    - "Elixir" â†’ kata.ex + kata_test.exs (ExUnit)
    - "Haskell" â†’ Kata.hs + KataSpec.hs (Hspec)
    - "Clojure" â†’ kata.clj + kata_test.clj (clojure.test)

    Use your knowledge of the language's conventions and popular test frameworks.
    Follow the same pattern: empty function that throws/raises "not implemented".
  </custom_language>

  <message>
    Kata setup complete!

    Created files:
    - CHALLENGE.md (problem description)
    {if $LANGUAGE != other}
    - Implementation file with empty solve() function
    - Test file with placeholder test
    {endif}

    Start practicing with \`/red\` to write your first failing test!
  </message>
</step_generate>

</execution_steps>

## Notes

- Kata content is fetched at runtime from cyber-dojo's GitHub repository
- The cyber-dojo project has been maintained for 10+ years with stable URLs
- All exercises are designed for TDD practice
- User can always go back to refine their preferences
- Boilerplate uses \`solve()\` as the main function - rename as needed for clarity
"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for polish.md 1`] = `
"---
description: Review and address issues in existing code - fix problems or justify skipping
argument-hint: [branch, PR#, file, or area to polish]
_hint: Fix or skip issues
_category: Workflow
_order: 36
_requested-tools:
  - Bash(git diff:*)
  - Bash(git status:*)
  - Bash(git log:*)
  - Bash(git rev-parse:*)
  - Bash(git merge-base:*)
  - Bash(git branch:*)
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




# Polish

Take another pass at existing work to address issues. Unlike \`/code-review\` which only identifies problems, \`/polish\` resolves each finding by either:

1. **Fixing** - Implement the improvement
2. **Skipping with justification** - Document why the issue can be deferred or ignored

## Phase 0: Determine Scope

Parse the argument to determine what to polish:

| Input | Action |
|-------|--------|
| No argument | Detect divergence point, review uncommitted + committed changes |
| Branch name | Changes from that branch to HEAD |
| PR number (e.g., \`123\`) | Fetch PR diff from GitHub |
| PR URL (e.g., \`github.com/.../pull/123\`) | Extract PR number and fetch diff |
| File/path | Focus on specific file(s) |



**For GitHub PRs:**

1. Try GitHub MCP first: \`mcp__github__pull_request_read\` with \`method: "get_diff"\`
2. Fall back to \`gh\` CLI: \`gh pr diff <number>\`
3. If neither works, report error and stop


**For local branches:**

1. Get current branch: \`git rev-parse --abbrev-ref HEAD\`
2. Detect divergence point (same logic as \`/code-review\`)
3. Collect changed files from diff and uncommitted changes

## Phase 1: Identify Issues

Categorize files based on these patterns:

| Category | File Patterns |
|----------|---------------|
| Frontend/UI | \`*.tsx\`, \`*.jsx\`, \`components/\`, \`pages/\`, \`views/\`, \`*.vue\` |
| Frontend/Styling | \`*.css\`, \`*.scss\`, \`*.less\`, \`styles/\`, \`*.tailwind*\`, \`*.styled.*\` |
| Backend/API | \`routes/\`, \`api/\`, \`controllers/\`, \`services/\`, \`*.controller.*\`, \`*.service.*\`, \`*.resolver.*\` |
| Backend/Data | \`migrations/\`, \`models/\`, \`prisma/\`, \`schema.*\`, \`*.model.*\`, \`*.entity.*\` |
| Tooling/Config | \`scripts/\`, \`*.config.*\`, \`package.json\`, \`tsconfig.*\`, \`vite.*\`, \`webpack.*\`, \`eslint.*\` |
| CI/CD | \`.github/\`, \`.gitlab-ci.*\`, \`Dockerfile\`, \`docker-compose.*\`, \`*.yml\` in CI paths |
| Tests | \`*.test.*\`, \`*.spec.*\`, \`__tests__/\`, \`__mocks__/\`, \`*.stories.*\` |
| Docs | \`*.md\`, \`docs/\`, \`README*\`, \`CHANGELOG*\` |


For each category, identify issues at these severity levels:

- **blocker** - Must fix before merge
- **risky** - Should fix or have strong justification
- **nit** - Nice to have, easily skippable

## Phase 2: Address Each Issue

For each identified issue, present it and then take action:

### Format

\`\`\`
### [file:line] [severity] Title

**Issue:** Description of the problem

**Action taken:**
- [ ] Fixed: [what was done]
- [ ] Skipped: [justification]
\`\`\`

### Decision Guidelines

**Fix when:**

- Security vulnerability
- Correctness bug
- Missing error handling that could crash
- Breaking API changes without migration
- Tests that don't actually test anything

**Skip with justification when:**

- Stylistic preference with no functional impact
- Optimization for unlikely hot paths
- Refactoring that would expand scope significantly
- Issue exists in code outside the change scope
- Technical debt documented for future sprint

### Fixing Issues

When fixing:

1. Make the minimal change to address the issue
2. Ensure tests still pass (run them if needed)
3. Don't expand scope beyond the identified issue

### Watch for Brittle Tests

When refactoring implementation, watch for **Peeping Tom** tests that:

- Test private methods or internal state directly
- Assert on implementation details rather than behavior
- Break on any refactoring even when behavior is preserved

If tests fail after a pure refactoring (no behavior change), consider whether the tests are testing implementation rather than behavior.


### Skipping Issues

Valid skip justifications:

- "Out of scope - exists in unchanged code"
- "Performance optimization unnecessary - called N times per request"
- "Tracked for future work - see issue #X"
- "Intentional design decision - [reason]"
- "Would require significant refactoring - defer to dedicated PR"

Invalid skip justifications:

- "Too hard to fix"
- "It works fine"
- "No time"

## Phase 3: Cross-Cutting Check

After addressing individual issues:

8. **Consistency check** - Look for inconsistent patterns, naming conventions, or structure across the codebase


Additional cross-cutting checks:

- Did fixes introduce new inconsistencies?
- Are skip justifications consistent with each other?
- Any patterns in what was skipped that suggest a bigger issue?

## Phase 4: Summary

\`\`\`
## Polish Summary

### Fixed
- [list of fixes applied]

### Skipped (with justification)
- [issue]: [justification]

### Tests
- [ ] All tests passing
- [ ] No new warnings introduced

### Remaining Work
- [any follow-up items identified]
\`\`\`

---

**User arguments:**

Polish: $ARGUMENTS

**End of user arguments**
"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for pr.md 1`] = `
"---
description: Creates a pull request using GitHub MCP
argument-hint: [optional-pr-title-and-description]
_hint: Create PR
_category: Workflow
_order: 5
---

# Create Pull Request

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




Create a pull request for the current branch using GitHub MCP tools.

## Workflow

Current branch status:
!\`git status\`

Recent commits:
!\`git log --oneline -5\`

**User arguments:**

PR: $ARGUMENTS

**End of user arguments**

**Process:**

1. **Ensure Branch is Ready**:
   !\`git status\`
   - Commit all changes
   - Push to remote: \`git push origin [branch-name]\`

2. **Create PR**: Create a well-formatted pull request

   Title: conventional commits format, like \`feat(#123): add user authentication\`

   Description template:

   \`\`\`markdown
   <!--
     Are there any relevant issues / PRs / mailing lists discussions?
     Please reference them here.
   -->

   ## References

   - [links to github issues referenced in commit messages]

   ## Summary

   [Brief description of changes]

   ## Test Plan

   - [ ] Tests pass
   - [ ] Manual testing completed
   \`\`\`

3. **Set Base Branch**: Default to main unless specified otherwise

4. **Link Issues**: Reference related issues found in commit messages

## Use GitHub MCP Tools

1. Check current branch and ensure it's pushed
2. Create a well-formatted pull request with proper title and description
3. Set the base branch (default: main)
4. Include relevant issue references if found in commit messages

### Beads Integration

Use Beads MCP to:
- Track work with \`bd ready\` to find next task
- Create issues with \`bd create "description"\`
- Track dependencies with \`bd dep add\`

See https://github.com/steveyegge/beads for more information.

"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for red.md 1`] = `
"---
description: Execute TDD Red Phase - write ONE failing test
argument-hint: [optional additional info]
_hint: Failing test
_category: Test-Driven Development
_order: 2
---

**User arguments:**

Red: $ARGUMENTS

**End of user arguments**

RED PHASE! Apply the below to the user input above.

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




(If there was no info above, fallback to:
1. Context of the conversation, if there's an immediate thing
2. \`bd ready\` to see what to work on next and start from there)


## TDD Fundamentals

### The TDD Cycle

The foundation of TDD is the Red-Green-Refactor cycle:

1. **Red Phase**: Write ONE failing test that describes desired behavior

   - The test must fail for the RIGHT reason (not syntax/import errors)
   - Only one test at a time - this is critical for TDD discipline
     - Exception: For browser-level tests or expensive setup (e.g., Storybook \`*.stories.tsx\`), group multiple assertions within a single test block to avoid redundant setup - but only when adding assertions to an existing interaction flow. If new user interactions are required, still create a new test. Split files by category if they exceed ~1000 lines.
   - **Adding a single test to a test file is ALWAYS allowed** - no prior test output needed
   - Starting TDD for a new feature is always valid, even if test output shows unrelated work
   - For DOM-based tests, use \`data-testid\` attributes to select elements rather than CSS classes, tag names, or text content
   - Avoid hard-coded timeouts both in form of sleep() or timeout: 5000 etc; use proper async patterns (\`waitFor\`, \`findBy*\`, event-based sync) instead and rely on global test configs for timeout settings

2. **Green Phase**: Write MINIMAL code to make the test pass

   - Implement only what's needed for the current failing test
   - No anticipatory coding or extra features
   - Address the specific failure message

3. **Refactor Phase**: Improve code structure while keeping tests green
   - Only allowed when relevant tests are passing
   - Requires proof that tests have been run and are green
   - Applies to BOTH implementation and test code
   - No refactoring with failing tests - fix them first

### Core Violations

1. **Multiple Test Addition**

   - Adding more than one new test at once
   - Exception: Initial test file setup or extracting shared test utilities

2. **Over-Implementation**

   - Code that exceeds what's needed to pass the current failing test
   - Adding untested features, methods, or error handling
   - Implementing multiple methods when test only requires one

3. **Premature Implementation**
   - Adding implementation before a test exists and fails properly
   - Adding implementation without running the test first
   - Refactoring when tests haven't been run or are failing

### Critical Principle: Incremental Development

Each step in TDD should address ONE specific issue:

- Test fails "not defined" â†’ Create empty stub/class only
- Test fails "not a function" â†’ Add method stub only
- Test fails with assertion â†’ Implement minimal logic only

### Optional Pre-Phase: Spike Phase

In rare cases where the problem space, interface, or expected behavior is unclear, a **Spike Phase** may be used **before the Red Phase**.
This phase is **not part of the regular TDD workflow** and must only be applied under exceptional circumstances.

- The goal of a Spike is **exploration and learning**, not implementation.
- The code written during a Spike is **disposable** and **must not** be merged or reused directly.
- Once sufficient understanding is achieved, all spike code is discarded, and normal TDD resumes starting from the **Red Phase**.
- A Spike is justified only when it is impossible to define a meaningful failing test due to technical uncertainty or unknown system behavior.

### General Information

- Sometimes the test output shows as no tests have been run when a new test is failing due to a missing import or constructor. In such cases, allow the agent to create simple stubs. Ask them if they forgot to create a stub if they are stuck.
- It is never allowed to introduce new logic without evidence of relevant failing tests. However, stubs and simple implementation to make imports and test infrastructure work is fine.
- In the refactor phase, it is perfectly fine to refactor both test and implementation code. That said, completely new functionality is not allowed. Types, clean up, abstractions, and helpers are allowed as long as they do not introduce new behavior.
- Adding types, interfaces, or a constant in order to replace magic values is perfectly fine during refactoring.
- Provide the agent with helpful directions so that they do not get stuck when blocking them.


### Test Structure (AAA Pattern)

Structure each test with clear phases:

- **Arrange**: Set up test data and preconditions (keep minimal)
- **Act**: Execute the single action being tested
- **Assert**: Verify the expected outcome with specific assertions

"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for refactor.md 1`] = `
"---
description: Execute TDD Refactor Phase - improve code structure while keeping tests green
argument-hint: <refactoring description>
_hint: Clean up code
_category: Test-Driven Development
_order: 4
---

**User arguments:**

Refactor: $ARGUMENTS

**End of user arguments**

Apply this document (specifically the Refactor phase) to the user input above.

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




(If there was no info above, fallback to:
1. Context of the conversation, if there's an immediate thing
2. \`bd ready\` to see what to work on next and start from there)


## TDD Fundamentals

### The TDD Cycle

The foundation of TDD is the Red-Green-Refactor cycle:

1. **Red Phase**: Write ONE failing test that describes desired behavior

   - The test must fail for the RIGHT reason (not syntax/import errors)
   - Only one test at a time - this is critical for TDD discipline
     - Exception: For browser-level tests or expensive setup (e.g., Storybook \`*.stories.tsx\`), group multiple assertions within a single test block to avoid redundant setup - but only when adding assertions to an existing interaction flow. If new user interactions are required, still create a new test. Split files by category if they exceed ~1000 lines.
   - **Adding a single test to a test file is ALWAYS allowed** - no prior test output needed
   - Starting TDD for a new feature is always valid, even if test output shows unrelated work
   - For DOM-based tests, use \`data-testid\` attributes to select elements rather than CSS classes, tag names, or text content
   - Avoid hard-coded timeouts both in form of sleep() or timeout: 5000 etc; use proper async patterns (\`waitFor\`, \`findBy*\`, event-based sync) instead and rely on global test configs for timeout settings

2. **Green Phase**: Write MINIMAL code to make the test pass

   - Implement only what's needed for the current failing test
   - No anticipatory coding or extra features
   - Address the specific failure message

3. **Refactor Phase**: Improve code structure while keeping tests green
   - Only allowed when relevant tests are passing
   - Requires proof that tests have been run and are green
   - Applies to BOTH implementation and test code
   - No refactoring with failing tests - fix them first

### Core Violations

1. **Multiple Test Addition**

   - Adding more than one new test at once
   - Exception: Initial test file setup or extracting shared test utilities

2. **Over-Implementation**

   - Code that exceeds what's needed to pass the current failing test
   - Adding untested features, methods, or error handling
   - Implementing multiple methods when test only requires one

3. **Premature Implementation**
   - Adding implementation before a test exists and fails properly
   - Adding implementation without running the test first
   - Refactoring when tests haven't been run or are failing

### Critical Principle: Incremental Development

Each step in TDD should address ONE specific issue:

- Test fails "not defined" â†’ Create empty stub/class only
- Test fails "not a function" â†’ Add method stub only
- Test fails with assertion â†’ Implement minimal logic only

### Optional Pre-Phase: Spike Phase

In rare cases where the problem space, interface, or expected behavior is unclear, a **Spike Phase** may be used **before the Red Phase**.
This phase is **not part of the regular TDD workflow** and must only be applied under exceptional circumstances.

- The goal of a Spike is **exploration and learning**, not implementation.
- The code written during a Spike is **disposable** and **must not** be merged or reused directly.
- Once sufficient understanding is achieved, all spike code is discarded, and normal TDD resumes starting from the **Red Phase**.
- A Spike is justified only when it is impossible to define a meaningful failing test due to technical uncertainty or unknown system behavior.

### General Information

- Sometimes the test output shows as no tests have been run when a new test is failing due to a missing import or constructor. In such cases, allow the agent to create simple stubs. Ask them if they forgot to create a stub if they are stuck.
- It is never allowed to introduce new logic without evidence of relevant failing tests. However, stubs and simple implementation to make imports and test infrastructure work is fine.
- In the refactor phase, it is perfectly fine to refactor both test and implementation code. That said, completely new functionality is not allowed. Types, clean up, abstractions, and helpers are allowed as long as they do not introduce new behavior.
- Adding types, interfaces, or a constant in order to replace magic values is perfectly fine during refactoring.
- Provide the agent with helpful directions so that they do not get stuck when blocking them.


## Code Complexity Signals

Look for these refactoring opportunities:

- [ ] Nesting > 3 levels deep
- [ ] Functions > 20 lines
- [ ] Duplicate code blocks
- [ ] Abstractions with single implementation
- [ ] "Just in case" parameters or config
- [ ] Magic values without names
- [ ] Dead/unused code


### Watch for Brittle Tests

When refactoring implementation, watch for **Peeping Tom** tests that:

- Test private methods or internal state directly
- Assert on implementation details rather than behavior
- Break on any refactoring even when behavior is preserved

If tests fail after a pure refactoring (no behavior change), consider whether the tests are testing implementation rather than behavior.


8. **Consistency check** - Look for inconsistent patterns, naming conventions, or structure across the codebase

"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for research.md 1`] = `
"---
description: Research a problem in parallel via web docs, web search, codebase exploration, and deep ultrathink
argument-hint: <research topic or question>
_hint: Deep research
_category: Utilities
_order: 20
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).


**User arguments:**

Research: $ARGUMENTS

**End of user arguments**

Research the following problem or question thoroughly, like a senior developer would.

## Step 1: Launch Parallel Research Agents

Use the Task tool to spawn these subagents **in parallel** (all in a single message):

1. **Web Documentation Agent** (subagent_type: general-purpose)
   - Search official documentation for the topic
   - Find best practices and recommended patterns
   - Locate relevant GitHub issues or discussions

2. **Web Search Agent** (subagent_type: general-purpose)
   - Perform broad web searches for solutions and discussions
   - Find Stack Overflow answers, blog posts, and tutorials
   - Note common pitfalls and gotchas

3. **Codebase Explorer Agent** (subagent_type: Explore)
   - Search the codebase for related patterns
   - Find existing solutions to similar problems
   - Identify relevant files, functions, or components

## Step 2: Library Documentation (Optional)

If the research involves specific frameworks or libraries:
- Use Context7 MCP tools (mcp__context7__resolve-library-id, then get-library-docs)
- Get up-to-date API references and code examples
- If Context7 is unavailable, note this in findings so user knows library docs were harder to obtain

## Step 3: Deep Analysis

With all gathered context, perform extended reasoning (ultrathink) to:
- Analyze the problem from first principles
- Consider edge cases and trade-offs
- Synthesize insights across all sources
- Identify conflicts between sources

## Step 4: Present Findings

Present a structured summary to the user:

### Problem Statement
Describe the problem and why it matters.

### Key Findings
Summarize the most relevant solutions and approaches.

### Codebase Patterns
Document how the current codebase handles similar cases.

### Recommended Approach
Provide your recommendation based on all research.

### Conflicts
Highlight where sources disagree and provide assessment of which is more reliable.

### Sources
List all source links with brief descriptions. This section is required.

## Research Guidelines

- Prioritize official documentation over blog posts
- Prefer solutions that match existing codebase patterns
- Note major.minor versions for libraries/frameworks (patch versions only if critical)
- Flag conflicting information across sources
- Write concise, actionable content
- Use active voice throughout
- **Do not create output files** - present findings directly in conversation unless user explicitly requests a file
"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for ship.md 1`] = `
"---
description: Ship code directly to main - for small, obvious changes that don't need review
argument-hint: [optional-commit-message]
_hint: Direct to main
_category: Ship / Show / Ask
_order: 1
_selectedByDefault: false
---

# Ship - Direct Merge to Main

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




**Ship/Show/Ask Pattern - SHIP**

Ship is for small, obvious changes that don't need code review. Examples:

- Typo fixes
- Formatting changes
- Documentation updates
- Obvious bug fixes
- Dependency updates with passing tests

## Prerequisites

Before shipping directly to main:

1. All tests must pass
2. Linter must pass
3. Changes must be small and low-risk
4. CI must be green (if configured)

## Workflow

Current branch status:
!\`git status\`

Recent commits:
!\`git log --oneline -5\`

**User arguments:**

Ship: $ARGUMENTS

**End of user arguments**

**Process:**

1. **Verify Change Size**: Check git diff to ensure changes are small and focused
   !\`git diff --stat main\`

2. **Run Tests**: Ensure all tests pass
   !\`npm test\` or !\`yarn test\` or appropriate test command for the project

3. **Run Linter**: Ensure code quality checks pass
   !\`npm run lint\` or !\`yarn lint\` or appropriate lint command (if available)

4. **Safety Check**: Confirm with user that this is truly a ship-worthy change:
   - Is this a small, obvious change?
   - Do all tests pass?
   - Is CI green?

   If ANY of these are "no", suggest using \`/show\` or \`/ask\` instead.

5. **Merge to Main**: If all checks pass and user confirms:

   \`\`\`bash
   git checkout main
   git pull origin main
   git merge --ff-only [feature-branch] || git merge [feature-branch]
   git push origin main
   \`\`\`

6. **Cleanup**: Delete the feature branch

   \`\`\`bash
   git branch -d [feature-branch]
   git push origin --delete [feature-branch]
   \`\`\`

7. **Notify**: Show summary of what was shipped

## Safety Rails

If tests fail, linter fails, or changes are large/complex, STOP and suggest:

- Use \`/show\` for changes that should be seen but don't need approval
- Use \`/ask\` (traditional PR) for complex changes needing discussion

### Beads Integration

Use Beads MCP to:
- Track work with \`bd ready\` to find next task
- Create issues with \`bd create "description"\`
- Track dependencies with \`bd dep add\`

See https://github.com/steveyegge/beads for more information.

"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for show.md 1`] = `
"---
description: Show code to team with auto-merge - for changes that should be visible but don't need approval
argument-hint: [optional-pr-title-and-description]
_hint: Auto-merge PR
_category: Ship / Show / Ask
_order: 2
_selectedByDefault: false
---

# Show - Visible Merge with Optional Feedback

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




**Ship/Show/Ask Pattern - SHOW**

Show is for changes that teammates should see, but don't require approval. Examples:

- Refactoring with test coverage
- New features with comprehensive tests
- Performance improvements
- Non-breaking API changes

## Prerequisites

Before using show:

1. All tests must pass
2. Changes should have good test coverage
3. Changes should be non-breaking or backward compatible
4. CI must be green

## Workflow

Current branch status:
!\`git status\`

Recent commits:
!\`git log --oneline -5\`

**User arguments:**

Show: $ARGUMENTS

**End of user arguments**

**Process:**

1. **Verify Quality**: Check that changes meet show criteria
   - Run tests: !\`npm test\` or !\`yarn test\` or appropriate test command
   - Check coverage is maintained or improved
   - Verify no breaking changes

2. **Create Show PR**: Create a PR that will auto-merge after a short window
   - Title: conventional commits format, prefixed with \`[SHOW]\`
   - Description: Clear explanation of what changed and why
   - Add label: "show" or "auto-merge"
   - Set auto-merge if GitHub setting allows

3. **Configure Auto-Merge**:
   - If GitHub Actions is configured, set to auto-merge after CI passes
   - If not, provide instructions to merge after 1-2 hours of visibility
   - Add notice that feedback is welcome but not required

4. **PR Description Template**:

   \`\`\`markdown
   ## ðŸš€ Show - Auto-merging after CI

   **This is a SHOW PR**: Changes are ready to merge but shared for visibility.
   Feedback welcome but not required. Will auto-merge when CI passes.

   <!--
     References: [link to relevant issues]
   -->

   ### What changed

   [Brief description]

   ### Why

   [Rationale for change]

   ### Test coverage

   - [ ] All tests pass
   - [ ] Coverage maintained/improved
   - [ ] No breaking changes
   \`\`\`

5. **Monitoring**: Check PR status and auto-merge when ready

## Decision Guide

Use **Show** when:

- âœ… Tests are comprehensive
- âœ… Changes are non-breaking
- âœ… You're confident in the approach
- âœ… Team should be aware of the change

Use **/ship** instead if: change is tiny and obvious (typo, formatting)

Use **/ask** instead if: change needs discussion, breaks APIs, or you're uncertain

### Beads Integration

Use Beads MCP to:
- Track work with \`bd ready\` to find next task
- Create issues with \`bd create "description"\`
- Track dependencies with \`bd dep add\`

See https://github.com/steveyegge/beads for more information.

"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for simplify.md 1`] = `
"---
description: Reduce code complexity while keeping tests green
argument-hint: [file, function, or area to simplify]
_hint: Reduce complexity
_category: Test-Driven Development
_order: 16
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).


**User arguments:**

Simplify: $ARGUMENTS

**End of user arguments**

(If there was no info above, fallback to the context of the conversation)


Reduce complexity while keeping tests green.

## Core Principles

**YAGNI** - Don't build until actually needed. Delete "just in case" code.

**KISS** - Simplest solution that works. Clever is the enemy of clear.

**Rule of Three** - Don't abstract until 3rd occurrence. "Prefer duplication over wrong abstraction" (Sandi Metz).

## When NOT to Simplify

- Essential domain complexity (regulations, business rules)
- Performance-critical optimized code
- Concurrency/thread-safety requirements
- Security-sensitive explicit checks

## Prerequisites

Tests must be green. If failing, use \`/green\` first.

## Code Complexity Signals

Look for these refactoring opportunities:

- [ ] Nesting > 3 levels deep
- [ ] Functions > 20 lines
- [ ] Duplicate code blocks
- [ ] Abstractions with single implementation
- [ ] "Just in case" parameters or config
- [ ] Magic values without names
- [ ] Dead/unused code


## Techniques

| Pattern | Before | After |
|---------|--------|-------|
| Guard clause | Nested \`if/else\` | Early \`return\` |
| Named condition | Complex boolean | \`const isValid = ...\` |
| Extract constant | \`if (x > 3)\` | \`if (x > MAX_RETRIES)\` |
| Flatten callback | \`.then().then()\` | \`async/await\` |

**Also apply:** Consolidate duplicates, inline unnecessary abstractions, delete dead code.

## Validate

1. Tests still green
2. Code reads more clearly
3. No behavioral changes

**Simplify** removes complexity locally. **Refactor** improves architecture broadly. Use \`/refactor\` if changes require structural reorganization.

### Watch for Brittle Tests

When refactoring implementation, watch for **Peeping Tom** tests that:

- Test private methods or internal state directly
- Assert on implementation details rather than behavior
- Break on any refactoring even when behavior is preserved

If tests fail after a pure refactoring (no behavior change), consider whether the tests are testing implementation rather than behavior.

"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for spike.md 1`] = `
"---
description: Execute TDD Spike Phase - exploratory coding to understand problem space before TDD
argument-hint: <exploration description>
_hint: Explore first
_category: Test-Driven Development
_order: 1
---

**User arguments:**

Spike: $ARGUMENTS

**End of user arguments**

SPIKE PHASE! Apply the below to the user input above.

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




(If there was no info above, fallback to:
1. Context of the conversation, if there's an immediate thing
2. \`bd ready\` to see what to work on next and start from there)


## TDD Fundamentals

### The TDD Cycle

The foundation of TDD is the Red-Green-Refactor cycle:

1. **Red Phase**: Write ONE failing test that describes desired behavior

   - The test must fail for the RIGHT reason (not syntax/import errors)
   - Only one test at a time - this is critical for TDD discipline
     - Exception: For browser-level tests or expensive setup (e.g., Storybook \`*.stories.tsx\`), group multiple assertions within a single test block to avoid redundant setup - but only when adding assertions to an existing interaction flow. If new user interactions are required, still create a new test. Split files by category if they exceed ~1000 lines.
   - **Adding a single test to a test file is ALWAYS allowed** - no prior test output needed
   - Starting TDD for a new feature is always valid, even if test output shows unrelated work
   - For DOM-based tests, use \`data-testid\` attributes to select elements rather than CSS classes, tag names, or text content
   - Avoid hard-coded timeouts both in form of sleep() or timeout: 5000 etc; use proper async patterns (\`waitFor\`, \`findBy*\`, event-based sync) instead and rely on global test configs for timeout settings

2. **Green Phase**: Write MINIMAL code to make the test pass

   - Implement only what's needed for the current failing test
   - No anticipatory coding or extra features
   - Address the specific failure message

3. **Refactor Phase**: Improve code structure while keeping tests green
   - Only allowed when relevant tests are passing
   - Requires proof that tests have been run and are green
   - Applies to BOTH implementation and test code
   - No refactoring with failing tests - fix them first

### Core Violations

1. **Multiple Test Addition**

   - Adding more than one new test at once
   - Exception: Initial test file setup or extracting shared test utilities

2. **Over-Implementation**

   - Code that exceeds what's needed to pass the current failing test
   - Adding untested features, methods, or error handling
   - Implementing multiple methods when test only requires one

3. **Premature Implementation**
   - Adding implementation before a test exists and fails properly
   - Adding implementation without running the test first
   - Refactoring when tests haven't been run or are failing

### Critical Principle: Incremental Development

Each step in TDD should address ONE specific issue:

- Test fails "not defined" â†’ Create empty stub/class only
- Test fails "not a function" â†’ Add method stub only
- Test fails with assertion â†’ Implement minimal logic only

### Optional Pre-Phase: Spike Phase

In rare cases where the problem space, interface, or expected behavior is unclear, a **Spike Phase** may be used **before the Red Phase**.
This phase is **not part of the regular TDD workflow** and must only be applied under exceptional circumstances.

- The goal of a Spike is **exploration and learning**, not implementation.
- The code written during a Spike is **disposable** and **must not** be merged or reused directly.
- Once sufficient understanding is achieved, all spike code is discarded, and normal TDD resumes starting from the **Red Phase**.
- A Spike is justified only when it is impossible to define a meaningful failing test due to technical uncertainty or unknown system behavior.

### General Information

- Sometimes the test output shows as no tests have been run when a new test is failing due to a missing import or constructor. In such cases, allow the agent to create simple stubs. Ask them if they forgot to create a stub if they are stuck.
- It is never allowed to introduce new logic without evidence of relevant failing tests. However, stubs and simple implementation to make imports and test infrastructure work is fine.
- In the refactor phase, it is perfectly fine to refactor both test and implementation code. That said, completely new functionality is not allowed. Types, clean up, abstractions, and helpers are allowed as long as they do not introduce new behavior.
- Adding types, interfaces, or a constant in order to replace magic values is perfectly fine during refactoring.
- Provide the agent with helpful directions so that they do not get stuck when blocking them.

"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for summarize.md 1`] = `
"---
description: Summarize conversation progress and next steps
argument-hint: [optional additional info]
_hint: Summarize chat
_category: Workflow
_order: 10
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




Create a concise summary of the current conversation suitable for transferring context to a new conversation.

**User arguments:**

Summarize: $ARGUMENTS

**End of user arguments**

## Summary Structure

Provide a summary with these sections:

### What We Did
- Key accomplishments and changes made
- Important decisions or discoveries
- Files created, modified, or analyzed

### What We're Doing Next
- Immediate next steps
- Pending tasks or work in progress
- Goals or objectives to continue

### Blockers & User Input Needed
- Any issues requiring user intervention
- Decisions that need to be made
- Missing information or clarifications needed

## Output Format

Keep the summary concise and actionable - suitable for pasting into a new conversation to quickly restore context without needing the full conversation history.


## Beads Integration

If Beads MCP is available, check for task tracking status and ask if the user wants to:
1. Review current task status
2. Update task states based on conversation progress
3. Include Beads context in the summary

Use AskUserQuestion to confirm Beads integration preferences.
"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for tdd.md 1`] = `
"---
description: Remind agent about TDD approach and continue conversation
argument-hint: [optional-response-to-last-message]
_hint: TDD reminder
_category: Test-Driven Development
_order: 1
---

# TDD Reminder

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




## TDD Fundamentals

### The TDD Cycle

The foundation of TDD is the Red-Green-Refactor cycle:

1. **Red Phase**: Write ONE failing test that describes desired behavior

   - The test must fail for the RIGHT reason (not syntax/import errors)
   - Only one test at a time - this is critical for TDD discipline
     - Exception: For browser-level tests or expensive setup (e.g., Storybook \`*.stories.tsx\`), group multiple assertions within a single test block to avoid redundant setup - but only when adding assertions to an existing interaction flow. If new user interactions are required, still create a new test. Split files by category if they exceed ~1000 lines.
   - **Adding a single test to a test file is ALWAYS allowed** - no prior test output needed
   - Starting TDD for a new feature is always valid, even if test output shows unrelated work
   - For DOM-based tests, use \`data-testid\` attributes to select elements rather than CSS classes, tag names, or text content
   - Avoid hard-coded timeouts both in form of sleep() or timeout: 5000 etc; use proper async patterns (\`waitFor\`, \`findBy*\`, event-based sync) instead and rely on global test configs for timeout settings

2. **Green Phase**: Write MINIMAL code to make the test pass

   - Implement only what's needed for the current failing test
   - No anticipatory coding or extra features
   - Address the specific failure message

3. **Refactor Phase**: Improve code structure while keeping tests green
   - Only allowed when relevant tests are passing
   - Requires proof that tests have been run and are green
   - Applies to BOTH implementation and test code
   - No refactoring with failing tests - fix them first

### Core Violations

1. **Multiple Test Addition**

   - Adding more than one new test at once
   - Exception: Initial test file setup or extracting shared test utilities

2. **Over-Implementation**

   - Code that exceeds what's needed to pass the current failing test
   - Adding untested features, methods, or error handling
   - Implementing multiple methods when test only requires one

3. **Premature Implementation**
   - Adding implementation before a test exists and fails properly
   - Adding implementation without running the test first
   - Refactoring when tests haven't been run or are failing

### Critical Principle: Incremental Development

Each step in TDD should address ONE specific issue:

- Test fails "not defined" â†’ Create empty stub/class only
- Test fails "not a function" â†’ Add method stub only
- Test fails with assertion â†’ Implement minimal logic only

### Optional Pre-Phase: Spike Phase

In rare cases where the problem space, interface, or expected behavior is unclear, a **Spike Phase** may be used **before the Red Phase**.
This phase is **not part of the regular TDD workflow** and must only be applied under exceptional circumstances.

- The goal of a Spike is **exploration and learning**, not implementation.
- The code written during a Spike is **disposable** and **must not** be merged or reused directly.
- Once sufficient understanding is achieved, all spike code is discarded, and normal TDD resumes starting from the **Red Phase**.
- A Spike is justified only when it is impossible to define a meaningful failing test due to technical uncertainty or unknown system behavior.

### General Information

- Sometimes the test output shows as no tests have been run when a new test is failing due to a missing import or constructor. In such cases, allow the agent to create simple stubs. Ask them if they forgot to create a stub if they are stuck.
- It is never allowed to introduce new logic without evidence of relevant failing tests. However, stubs and simple implementation to make imports and test infrastructure work is fine.
- In the refactor phase, it is perfectly fine to refactor both test and implementation code. That said, completely new functionality is not allowed. Types, clean up, abstractions, and helpers are allowed as long as they do not introduce new behavior.
- Adding types, interfaces, or a constant in order to replace magic values is perfectly fine during refactoring.
- Provide the agent with helpful directions so that they do not get stuck when blocking them.


## Continue Conversation

**User arguments:**

TDD: $ARGUMENTS

**End of user arguments**

Please continue with the user input above, applying TDD approach.
"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for tdd-review.md 1`] = `
"---
description: Review test suite quality against FIRST principles and TDD anti-patterns
argument-hint: [optional test file or directory path]
_hint: Review tests
_category: Test-Driven Development
_order: 45
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




(If there was no info above, fallback to:
1. Context of the conversation, if there's an immediate thing
2. \`bd ready\` to see what to work on next and start from there)


# Test Quality Review

Analyze test files against FIRST principles and TDD best practices.

## Phase 1: Scope

| Input | Action |
|-------|--------|
| No argument | Find all test files in project |
| File path | Analyze specific test file |
| Directory | Analyze tests in directory |

Detect test files using common patterns: \`*.test.*\`, \`*.spec.*\`, \`*.stories.*\`, \`__tests__/**\`

Also check for framework-specific patterns based on the project's languages and tools (e.g., \`*_test.go\`, \`*_test.py\`, \`Test*.java\`, \`*.feature\` for BDD).

## Phase 2: Analysis

For each test file, check against these criteria:

### Quality Criteria

#### FIRST Principles

| Principle | What to Check |
|-----------|---------------|
| **Fast** | Tests complete quickly, no I/O, no network calls, no sleep()/setTimeout delays |
| **Independent** | No shared mutable state, no execution order dependencies between tests |
| **Repeatable** | No Date.now(), no Math.random() without seeding, no external service dependencies |
| **Self-validating** | Meaningful assertions that verify behavior, no manual verification needed |

#### TDD Anti-patterns

| Anti-pattern | Detection Signals |
|--------------|-------------------|
| **The Liar** | \`expect(true).toBe(true)\`, empty test bodies, tests with no assertions |
| **Excessive Setup** | >20 lines of arrange code, >5 mocks, deep nested object construction |
| **The One** | >5 assertions testing unrelated behaviors in a single test |
| **The Peeping Tom** | Testing private methods, asserting on internal state, tests that break on any refactor |
| **The Slow Poke** | Real database/network calls, file I/O, hard-coded timeouts |

#### Test Structure (AAA Pattern)

- **Arrange**: Clear setup with minimal fixtures
- **Act**: Single action being tested
- **Assert**: Specific, behavior-focused assertions


## Phase 3: Report

Output a structured report:

\`\`\`
## Test Quality Report

### Summary
- Files analyzed: N
- Tests found: N
- Issues found: N (X blockers, Y warnings)

### By File

#### path/to/file.test.ts

| Line | Issue | Severity | Description |
|------|-------|----------|-------------|
| 15 | The Liar | blocker | Test has no assertions |
| 42 | Slow Poke | warning | Uses setTimeout(500) |

### Recommendations
- [ ] Fix blockers before merge
- [ ] Consider refactoring tests with excessive setup
\`\`\`

### Severity Levels

- **blocker**: Must fix - test provides false confidence (The Liar, no assertions)
- **warning**: Should fix - test quality issue (Slow Poke, Excessive Setup)
- **info**: Consider - style or structure suggestion (AAA pattern)

---

**User arguments:**

TDD-review: $ARGUMENTS

**End of user arguments**
"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for upgrade-deps.md 1`] = `
"---
description: Check for dependency upgrades and assess safety before updating
argument-hint: (no arguments - interactive)
_hint: Upgrade packages
_category: Utilities
_order: 50
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




# Upgrade Dependencies

Analyze available dependency upgrades, assess codebase safety signals, and guide the user through upgrade decisions.

## Phase 1: Detect Environment

Scan for package manager and project setup:

| Check | How |
|-------|-----|
| Package manager | Look for \`pnpm-lock.yaml\`, \`yarn.lock\`, \`package-lock.json\`, \`bun.lockb\` |
| Lockfile present | Exists and tracked in git |
| Test script | \`package.json\` has \`test\` script |
| Test files | \`*.test.*\`, \`*.spec.*\`, \`__tests__/\` exist |
| CI config | \`.github/workflows/\`, \`.gitlab-ci.yml\`, etc. |
| Type checking | \`tsconfig.json\` or \`jsconfig.json\` present |

Output:

\`\`\`
## Environment

Package manager: pnpm
Lockfile: âœ“ pnpm-lock.yaml (tracked)
Test script: âœ“ "vitest run"
Test files: âœ“ 16 test files found
CI config: âœ“ .github/workflows/release.yml
Types: âœ“ tsconfig.json
\`\`\`

## Phase 2: List Available Upgrades

Run the appropriate outdated command:

| Manager | Command |
|---------|---------|
| pnpm | \`pnpm outdated --format json\` |
| npm | \`npm outdated --json\` |
| yarn | \`yarn outdated --json\` |
| bun | \`bun outdated\` |

Group results by semver level:

\`\`\`
## Available Upgrades

### Patch (low risk)
| Package | Current | Latest | Age |
|---------|---------|--------|-----|
| lodash | 4.17.20 | 4.17.21 | 6 months |

### Minor (medium risk)
| Package | Current | Latest | Age |
|---------|---------|--------|-----|
| vitest | 1.5.0 | 1.6.0 | 3 weeks |

### Major (review changelog)
| Package | Current | Latest | Age |
|---------|---------|--------|-----|
| typescript | 4.9.5 | 5.4.2 | 8 months |
\`\`\`

Note: "Age" is time since the latest version was published. Packages < 14 days old may pose supply chain risk.

## Phase 3: Safety Assessment

Present observable signals (not guarantees):

\`\`\`
## Safety Signals

âœ“ Test files present (16 files)
âœ“ Test script configured
âœ“ CI pipeline exists
âœ“ Lockfile tracked in git
âœ“ TypeScript for type safety
âš  No pre-commit hooks detected
âœ— Coverage threshold not configured

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âš  DISCLAIMER: This is a surface-level check based
on file presence, not actual test quality or
coverage. You know your codebase best.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
\`\`\`

## Phase 4: Ask User

Use \`AskUserQuestion\` to determine next steps:

**Question: "How would you like to proceed?"** (header: "Upgrade")

Options:
1. **Patches only** - "Upgrade patch versions (X.Y.Z â†’ X.Y.Z+1), lowest risk"
2. **Patches + minors** - "Include minor versions (X.Y â†’ X.Y+1), some API additions"
3. **Interactive selection** - "Choose specific packages to upgrade"
4. **Doctor mode** - "Test each upgrade individually, keep only working ones (slower but safest)"
5. **Just show outdated** - "Don't upgrade, just list what's available"

## Phase 5: Execute Upgrade

Based on user selection:

### Patches Only
\`\`\`bash
# pnpm
pnpm update --no-save  # updates lockfile only for patches within range

# or with ncu
npx npm-check-updates --target patch -u && pnpm install
\`\`\`

### Patches + Minors
\`\`\`bash
npx npm-check-updates --target minor -u && pnpm install
\`\`\`

### Interactive Selection
\`\`\`bash
npx npm-check-updates -i
\`\`\`

### Doctor Mode
\`\`\`bash
npx npm-check-updates --doctor -u
\`\`\`

This will:
1. Verify tests pass before starting
2. Try upgrading all dependencies
3. If tests fail, test each dependency individually
4. Keep only upgrades that pass tests

### After Any Upgrade

1. Run tests: \`pnpm test\`
2. Run type check if available: \`pnpm typecheck\`
3. Run build if available: \`pnpm build\`
4. Report results to user

## Phase 6: Summary

\`\`\`
## Upgrade Summary

### Upgraded
- lodash: 4.17.20 â†’ 4.17.21 (patch)
- vitest: 1.5.0 â†’ 1.6.0 (minor)

### Skipped
- typescript: 4.9.5 â†’ 5.4.2 (major - user opted out)

### Verification
- [x] Tests passing
- [x] Types checking
- [x] Build successful

### Next Steps
- Review changelog for skipped major upgrades
- Consider running full test suite in CI before merging
\`\`\`

---

**User arguments:**

Upgrade: $ARGUMENTS

**End of user arguments**
"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for worktree-add.md 1`] = `
"---
description: Add a new git worktree from branch name or issue URL, copy settings, install deps, and open in current IDE
argument-hint: <branch-name-or-issue-url> [optional-base-branch]
_hint: Add worktree
_category: Worktree Management
_order: 1
---

# Git Worktree Setup

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




**User arguments:**

Worktree-add: $ARGUMENTS

**End of user arguments**

<current_state>
Current branch: \`git branch --show-current\`
Current worktrees: \`git worktree list\`
Remote branches: \`git branch -r\`
Uncommitted changes: \`git status --short\`
</current_state>

<execution_steps>
<step_0>
  <description>Ask user for setup mode</description>
  <prompt>
    <message>How would you like to set up the worktree?</message>
    <options>
      <option value="quick">
        <label>Quick</label>
        <description>Just create the worktree (skip deps, settings, IDE)</description>
      </option>
      <option value="full">
        <label>Full setup</label>
        <description>Install dependencies, copy settings, open in IDE</description>
      </option>
    </options>
  </prompt>
  <set_variable>$SETUP_MODE = user selection ("quick" or "full")</set_variable>
  <purpose>Allow quick worktree creation when user just needs the branch</purpose>
</step_0>

<step_0b>
  <description>Detect git hosting provider and available tools (only needed if argument is an issue URL)</description>
  <condition>Only run this step if first argument looks like a git hosting URL</condition>


<detect_provider>
  <check_remote_url>git remote get-url origin</check_remote_url>
  <identify_host>
    - github.com â†’ GitHub
    - gitlab.com â†’ GitLab
    - bitbucket.org â†’ Bitbucket
    - Other â†’ Ask user
  </identify_host>
</detect_provider>
<check_available_tools>
  <list_mcp_servers>Check which git-hosting MCP servers are available (github, gitlab, etc.)</list_mcp_servers>
  <check_cli>Check if gh/glab CLI is available as fallback</check_cli>
</check_available_tools>
<select_tool>
  <if_single_mcp>If only one relevant MCP available, confirm with user</if_single_mcp>
  <if_multiple>Let user choose which tool to use</if_multiple>
  <if_told_earlier>If user specified tool earlier in conversation, use that without asking again</if_told_earlier>
  <store_as>$GIT_HOST_TOOL (e.g., "github_mcp", "gitlab_mcp", "gh_cli")</store_as>
</select_tool>

  <purpose>Detect git hosting provider and select appropriate tool for issue lookup</purpose>
</step_0b>

  <step_1>
    <description>Detect current IDE environment</description>
    <condition>Only if $SETUP_MODE is "full"</condition>
    <detection_methods>
      <method_1>
        <tool>mcp__ide__getDiagnostics</tool>
        <vs_code_insiders>Check for paths containing "Code - Insiders"</vs_code_insiders>
        <vs_code>Check for paths containing "Code/" or "Code.app"</vs_code>
        <cursor>Check for paths containing "Cursor"</cursor>
        <zed>Check for paths containing "Zed"</zed>
      </method_1>
      <method_2>
        <fallback_detection>Use which command to find available IDEs</fallback_detection>
        <commands>which code-insiders code zed cursor</commands>
        <priority_order>code-insiders > cursor > zed > code</priority_order>
      </method_2>
    </detection_methods>
    <set_variables>
      <ide_command>Detected command (code-insiders|code|zed|cursor)</ide_command>
      <ide_name>Human-readable name</ide_name>
      <supports_tasks>true for VS Code variants, false for others</supports_tasks>
    </set_variables>
    <purpose>Automatically detect which IDE to use for opening the new worktree</purpose>
  </step_1>

  <step_2>
    <description>Determine default branch and parse arguments</description>
    <find_default_branch>
      <command>git symbolic-ref refs/remotes/origin/HEAD | sed 's@^refs/remotes/origin/@@'</command>
      <fallback>git remote show origin | grep 'HEAD branch' | cut -d: -f2 | tr -d ' '</fallback>
      <store_as>$DEFAULT_BRANCH (typically "main" or "master")</store_as>
    </find_default_branch>
    <input>The user-provided arguments</input>
    <expected_format>branch-name-or-issue-url [optional-base-branch]</expected_format>
    <example>fix/issue-123-main-content-area-visually-clipped main</example>
    <example_issue_url>https://github.com/owner/project/issues/123 main</example_issue_url>
    <base_branch>Use provided base branch, or $DEFAULT_BRANCH if not specified</base_branch>
  </step_2>

  <step_2_5>
    <description>Handle issue URLs from git hosting provider</description>
    <condition>If first argument matches issue URL pattern (detected in step_0)</condition>
    <url_detection>
      <github>Check if argument contains "github.com" and "/issues/"</github>
      <gitlab>Check if argument contains "gitlab.com" and "/-/issues/"</gitlab>
      <bitbucket>Check if argument contains "bitbucket.org" and "/issues/"</bitbucket>
    </url_detection>
    <url_parsing>
      <github_pattern>https://github.com/{owner}/{repo}/issues/{issue_number}</github_pattern>
      <gitlab_pattern>https://gitlab.com/{owner}/{repo}/-/issues/{issue_number}</gitlab_pattern>
      <bitbucket_pattern>https://bitbucket.org/{owner}/{repo}/issues/{issue_number}</bitbucket_pattern>
      <extract>owner, repo, issue_number from URL</extract>
    </url_parsing>
    <fetch_issue_details>
      <tool>Use $GIT_HOST_TOOL from step_0</tool>
      <method>get issue details</method>
      <parameters>owner, repo, issue_number</parameters>
    </fetch_issue_details>
    <generate_branch_name>
      <determine_type>Analyze issue title/labels to determine type (feat/fix/refactor/chore)</determine_type>
      <format>{type}/issue-{issue_number}-{kebab-case-title}</format>
      <kebab_case>Convert title to lowercase, replace spaces/special chars with hyphens</kebab_case>
      <sanitization>
        <rule>Always use lowercase for branch names</rule>
        <rule>Replace # with - (hash symbol not allowed in git branch names)</rule>
        <rule>Remove or replace other special characters: @, $, %, ^, &, *, (, ), [, ], {, }, \\, |, ;, :, ", ', <, >, ?, /, ~, \`</rule>
        <rule>Replace multiple consecutive hyphens with single hyphen</rule>
        <rule>Trim leading/trailing hyphens</rule>
      </sanitization>
      <truncate>Limit total branch name to reasonable length (~60 chars)</truncate>
    </generate_branch_name>
    <user_confirmation>
      <display>Show generated branch name and ask for confirmation</display>
      <options>"Yes, proceed" or "No, exit" or "Edit branch name"</options>
      <if_no>Exit command gracefully</if_no>
      <if_edit>Allow user to modify the branch name</if_edit>
      <if_yes>Continue with generated/modified branch name</if_yes>
    </user_confirmation>
    <examples>
      <input>https://github.com/owner/project/issues/456</input>
      <title>"Fix duplicate items in list view"</title>
      <generated>fix/issue-456-duplicate-items-in-list-view</generated>
    </examples>
  </step_2_5>

  <step_3>
    <description>Handle uncommitted changes if any exist</description>
    <condition>If git status --short output is not empty (has uncommitted changes)</condition>
    <prompt>
      <message>You have uncommitted changes. Move them to the new branch?</message>
      <options>
        <option value="yes">
          <label>Yes</label>
          <description>Stash changes and apply them in the new worktree</description>
        </option>
        <option value="no">
          <label>No</label>
          <description>Leave changes in current branch</description>
        </option>
      </options>
    </prompt>
    <if_yes>
      <command>git add -A && git stash push -m "Worktree switch: Moving changes to \${branch_name}"</command>
      <set_variable>$STASH_CREATED = true</set_variable>
    </if_yes>
    <if_no>
      <set_variable>$STASH_CREATED = false</set_variable>
    </if_no>
    <purpose>Let user decide whether to move work in progress to new branch</purpose>
  </step_3>

  <step_4>
    <description>Determine worktree parent directory</description>
    <check_if_in_worktree>git rev-parse --is-inside-work-tree && git worktree list --porcelain | grep "$(git rev-parse --show-toplevel)"</check_if_in_worktree>
    <set_parent_path>
      <if_main_worktree>Set parent_path=".."</if_main_worktree>
      <if_secondary_worktree>Set parent_path="../.." (need to go up two levels)</if_secondary_worktree>
    </set_parent_path>
    <purpose>Correctly determine where to create new worktree regardless of current location</purpose>
    <note>This handles both main worktree and secondary worktree scenarios</note>
  </step_4>

  <step_5>
    <description>Fetch latest changes from remote</description>
    <command>git fetch origin</command>
    <purpose>Ensure we have the latest remote branches and default branch state</purpose>
    <note>This ensures new worktrees are created from the most recent default branch</note>
  </step_5>

  <step_6>
    <description>Check if branch exists on remote</description>
    <command>git branch -r | grep "origin/\${branch_name}"</command>
    <decision>
      <if_exists>Branch exists on remote - will checkout existing branch</if_exists>
      <if_not_exists>Branch does not exist - will create new branch from base</if_not_exists>
    </decision>
  </step_6>

  <step_7>
    <description>Create the worktree</description>
    <option_a_new_branch>
      <condition>Remote branch does NOT exist</condition>
      <command>git worktree add \${parent_path}/\${branch_name} -b \${branch_name} --no-track \${base_branch}</command>
      <example>git worktree add ../fix/issue-123 -b fix/issue-123 --no-track origin/main</example>
      <note>--no-track prevents git from setting upstream to base_branch (which would make git push target main!)</note>
    </option_a_new_branch>
    <option_b_existing_branch>
      <condition>Remote branch EXISTS</condition>
      <command>git worktree add \${parent_path}/\${branch_name} \${branch_name}</command>
      <example>git worktree add ../fix/issue-123-main-content-area-visually-clipped fix/issue-123-main-content-area-visually-clipped</example>
    </option_b_existing_branch>
  </step_7>

  <step_7b>
    <description>Set up remote tracking for new branch</description>
    <condition>Only if new branch was created (option_a from step_7)</condition>
    <working_directory>\${parent_path}/\${branch_name}</working_directory>
    <command>cd \${parent_path}/\${branch_name} && git push -u origin \${branch_name}</command>
    <purpose>Establish remote tracking so git status shows ahead/behind and git push/pull work without specifying remote</purpose>
    <note>This creates the remote branch and sets upstream tracking in one step</note>
  </step_7b>

  <step_7c>
    <description>Quick mode completion</description>
    <condition>Only if $SETUP_MODE is "quick"</condition>
    <message>Worktree created at: \${parent_path}/\${branch_name}</message>
    <suggested_next_steps>
      <intro>You can now:</intro>
      <suggestion priority="1">Open in VS Code: \`code \${parent_path}/\${branch_name}\`</suggestion>
      <suggestion priority="2">Open in Cursor: \`cursor \${parent_path}/\${branch_name}\`</suggestion>
      <suggestion priority="3">Navigate to it: \`cd \${parent_path}/\${branch_name}\`</suggestion>
      <suggestion priority="4">Install dependencies: \`cd \${parent_path}/\${branch_name} && pnpm install\`</suggestion>
    </suggested_next_steps>
    <action>STOP here - do not continue to remaining steps</action>
  </step_7c>

  <step_8>
    <description>Copy Claude settings to new worktree</description>
    <condition>Only if $SETUP_MODE is "full"</condition>
    <source>.claude/settings.local.json</source>
    <destination>\${parent_path}/\${branch_name}/.claude/settings.local.json</destination>
    <command>cp -r .claude/settings.local.json \${parent_path}/\${branch_name}/.claude/settings.local.json</command>
    <purpose>Preserve all permission settings and configurations</purpose>
  </step_8>

  <step_9>
    <description>Copy .env.local files to new worktree</description>
    <condition>Only if $SETUP_MODE is "full"</condition>
    <search_command>find . -name ".env.local" -type f</search_command>
    <copy_logic>For each .env.local file found, copy to corresponding location in new worktree</copy_logic>
    <common_locations>
      - app/.env.local
      - packages/*/.env.local
      - (any other .env.local files found)
    </common_locations>
    <copy_command>find . -name ".env.local" -type f -exec sh -c 'mkdir -p "$(dirname "\${parent_path}/\${branch_name}/$1")" && cp "$1" "\${parent_path}/\${branch_name}/$1"' _ {} \\;</copy_command>
    <purpose>Preserve local environment configurations for development</purpose>
    <note>Only copies files that exist; ignores missing ones</note>
  </step_9>

  <step_10>
    <description>Create IDE-specific configuration (conditional)</description>
    <condition>Only if $SETUP_MODE is "full" AND supports_tasks is true (VS Code variants)</condition>
    <vs_code_tasks>
      <create_directory>mkdir -p \${parent_path}/\${branch_name}/.vscode</create_directory>
      <create_file_command>cat > \${parent_path}/\${branch_name}/.vscode/tasks.json << 'EOF'
{
  "version": "2.0.0",
  "tasks": [
    {
      "label": "Auto start Claude",
      "type": "shell",
      "command": "claude",
      "runOptions": {
        "runOn": "folderOpen"
      },
      "presentation": {
        "echo": false,
        "reveal": "always",
        "focus": true,
        "panel": "new"
      }
    }
  ]
}
EOF</create_file_command>
    </vs_code_tasks>
    <purpose>Create auto-start Claude task for VS Code variants on folder open</purpose>
    <note>Only creates for VS Code variants (code, code-insiders, cursor)</note>
    <skip_message>Skipping IDE-specific config for non-VS Code IDEs</skip_message>
  </step_10>

  <step_11>
    <description>Install dependencies in new worktree</description>
    <condition>Only if $SETUP_MODE is "full"</condition>
    <working_directory>\${parent_path}/\${branch_name}</working_directory>
    <command>cd \${parent_path}/\${branch_name} && pnpm install</command>
    <purpose>Ensure all node_modules are installed for the new worktree</purpose>
  </step_11>

  <step_12>
    <description>Apply stashed changes to new worktree (if stash was created)</description>
    <condition>Only if $SETUP_MODE is "full" AND $STASH_CREATED is true</condition>
    <working_directory>\${parent_path}/\${branch_name}</working_directory>
    <command>cd \${parent_path}/\${branch_name} && git stash pop</command>
    <purpose>Restore uncommitted work-in-progress to the new worktree branch</purpose>
    <note>This moves your uncommitted changes to the new branch where you'll continue working</note>
  </step_12>

  <step_13>
    <description>Open detected IDE in new worktree</description>
    <condition>Only if $SETUP_MODE is "full"</condition>
    <command>\${ide_command} \${parent_path}/\${branch_name}</command>
    <ide_specific_behavior>
      <vs_code_variants>Opens folder in VS Code/Insiders/Cursor with tasks.json auto-starting Claude</vs_code_variants>
      <zed>Opens folder in Zed editor</zed>
      <other>Uses detected IDE command to open folder</other>
    </ide_specific_behavior>
    <purpose>Launch development environment for the new worktree using detected IDE</purpose>
    <confirmation_message>Opening worktree in \${ide_name}</confirmation_message>
  </step_13>
</execution_steps>

<important_notes>

- Offers Quick or Full setup mode - Quick just creates the worktree, Full does everything
- Automatically detects and uses your current IDE (VS Code, VS Code Insiders, Cursor, Zed, etc.) in Full mode
- Creates VS Code-specific tasks.json only for VS Code variants (auto-starts Claude on folder open)
- Branch names with slashes (feat/, fix/, etc.) are fully supported
- The worktree directory path will match the full branch name including slashes
- Settings are copied to maintain the same permissions across worktrees
- Environment files (.env.local) are copied to preserve local configurations
- Each worktree has its own node_modules installation
- Uncommitted changes are automatically stashed and moved to the new worktree
- Your work-in-progress seamlessly transfers to the new branch
- IDE detection fallback: checks available editors and uses priority order
- New branches are automatically pushed with \`-u\` to set up remote tracking

Limitations:
- Assumes remote is named "origin" (most common convention)
- Supports macOS and Linux only (no Windows support)
- Requires MCP server or CLI for git hosting provider when using issue URLs (GitHub, GitLab, etc.)
- Dependency install command is pnpm (modify for npm/yarn if needed)
</important_notes>
"
`;

exports[`dynamic generation snapshots > with beads flag > should match snapshot for worktree-cleanup.md 1`] = `
"---
description: Clean up merged worktrees by verifying PR/issue status, consolidating settings, and removing stale worktrees
argument-hint: (no arguments)
_hint: Cleanup worktree
_category: Worktree Management
_order: 2
---

# Worktree Cleanup

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product


Beads is available for task tracking. Use \`mcp__beads__*\` tools to manage issues (the user interacts via \`bd\` commands).




Clean up merged worktrees by finding the oldest merged branch, consolidating settings, and removing stale worktrees.

**User arguments:**

Worktree-cleanup: $ARGUMENTS

**End of user arguments**

<current_state>
Current branch: \`git branch --show-current\`
Current worktrees: \`git worktree list\`
</current_state>

<execution_steps>
<step_0>
  <description>Detect git hosting provider and available tools</description>


<detect_provider>
  <check_remote_url>git remote get-url origin</check_remote_url>
  <identify_host>
    - github.com â†’ GitHub
    - gitlab.com â†’ GitLab
    - bitbucket.org â†’ Bitbucket
    - Other â†’ Ask user
  </identify_host>
</detect_provider>
<check_available_tools>
  <list_mcp_servers>Check which git-hosting MCP servers are available (github, gitlab, etc.)</list_mcp_servers>
  <check_cli>Check if gh/glab CLI is available as fallback</check_cli>
</check_available_tools>
<select_tool>
  <if_single_mcp>If only one relevant MCP available, confirm with user</if_single_mcp>
  <if_multiple>Let user choose which tool to use</if_multiple>
  <if_told_earlier>If user specified tool earlier in conversation, use that without asking again</if_told_earlier>
  <store_as>$GIT_HOST_TOOL (e.g., "github_mcp", "gitlab_mcp", "gh_cli")</store_as>
</select_tool>

  <purpose>Detect git hosting provider and select appropriate tool for PR verification</purpose>
</step_0>

  <step_1>
    <description>Determine default branch and verify we're on it</description>
    <find_default_branch>
      <command>git symbolic-ref refs/remotes/origin/HEAD | sed 's@^refs/remotes/origin/@@'</command>
      <fallback>git remote show origin | grep 'HEAD branch' | cut -d: -f2 | tr -d ' '</fallback>
      <store_as>$DEFAULT_BRANCH (typically "main" or "master")</store_as>
    </find_default_branch>
    <check_current_branch>git branch --show-current</check_current_branch>
    <verify>Current branch must equal $DEFAULT_BRANCH</verify>
    <error_if_not_default>Exit with error: "This command must be run from the default branch ($DEFAULT_BRANCH)"</error_if_not_default>
    <purpose>Ensure we're consolidating to the default branch worktree</purpose>
  </step_1>

  <step_2>
    <description>Get list of all worktrees</description>
    <command>git worktree list --porcelain</command>
    <parse_output>Extract worktree paths and branch names</parse_output>
    <exclude_default>Filter out the default branch worktree from cleanup candidates</exclude_default>
    <purpose>Identify all worktrees that could potentially be cleaned up</purpose>
  </step_2>

  <step_3>
    <description>Find oldest worktree by directory age</description>
    <get_worktree_ages>
      <detect_platform>uname -s (returns "Darwin" for macOS, "Linux" for Linux)</detect_platform>
      <command_macos>git worktree list | grep -v "\\[$DEFAULT_BRANCH\\]" | awk '{print $1}' > /tmp/worktrees-$$.txt && while IFS= read -r path; do echo "$(/usr/bin/stat -f '%Sm' -t '%Y-%m-%d %H:%M' "$path" 2>/dev/null)|$path"; done < /tmp/worktrees-$$.txt | sort; rm -f /tmp/worktrees-$$.txt</command_macos>
      <command_linux>git worktree list | grep -v "\\[$DEFAULT_BRANCH\\]" | awk '{print $1}' > /tmp/worktrees-$$.txt && while IFS= read -r path; do echo "$(stat -c '%y' "$path" 2>/dev/null | cut -d. -f1)|$path"; done < /tmp/worktrees-$$.txt | sort; rm -f /tmp/worktrees-$$.txt</command_linux>
      <purpose>List all worktrees sorted by directory modification time (oldest first)</purpose>
      <critical_notes>
        - Replace $DEFAULT_BRANCH with value from step_1 (e.g., "main" or "master")
        - grep "\\[branch\\]" matches branch name in brackets, not paths containing the word
        - Temp file approach avoids subshell parsing issues with piped while-loops
        - /usr/bin/stat on macOS avoids homebrew stat conflicts
      </critical_notes>
      <expected_output_format>YYYY-MM-DD HH:MM|/full/path/to/worktree (oldest first)</expected_output_format>
    </get_worktree_ages>
    <filter_recent>
      <exclude_new>For worktrees created within the last 24 hours, let user know that this worktree might not be worth cleaning</exclude_new>
      <get_current_time>date +"%Y-%m-%d %H:%M"</get_current_time>
    </filter_recent>
    <select_oldest>
      <extract_branch_name>Parse branch name from oldest worktree path</extract_branch_name>
      <important_note>DO NOT use "git branch --merged" to check merge status - it's unreliable</important_note>
      <proceed_to_pr_check>Move directly to step 4 to verify PR merge status instead</proceed_to_pr_check>
    </select_oldest>
    <purpose>Identify oldest worktree candidate - actual merge verification happens via PR/MR in next step</purpose>
  </step_3>

  <step_4>
    <description>Verify PR/MR merge status (primary merge verification)</description>
    <determine_repo>
      <check_remote>git remote get-url origin</check_remote>
      <parse_repo>Extract owner/repo from remote URL</parse_repo>
    </determine_repo>
    <search_pr>
      <tool>Use $GIT_HOST_TOOL from step_0 (e.g., mcp__github__search_pull_requests, mcp__gitlab__*, or gh CLI)</tool>
      <query>Find PRs/MRs where head={branch_name} and base=$DEFAULT_BRANCH</query>
      <purpose>Find PR/MR for this branch targeting default branch</purpose>
      <important>This is the PRIMARY way to verify if a branch was merged - NOT git commands</important>
    </search_pr>
    <verify_pr_merged>
      <if_pr_found>
        <get_pr_details>Use $GIT_HOST_TOOL to get full PR/MR info</get_pr_details>
        <confirm_merged>Verify PR/MR state is "closed"/"merged" AND merged_at is not null AND base is "$DEFAULT_BRANCH"</confirm_merged>
        <extract_issue_number>Look for issue references in PR title/body (e.g., #123, owner/repo#123)</extract_issue_number>
        <if_merged>Proceed with cleanup - this branch was definitively merged to default branch</if_merged>
        <if_not_merged>
          <skip_worktree>This worktree is NOT merged - continue to next oldest worktree</skip_worktree>
          <repeat_from_step_3>Go back and find the next oldest worktree to check</repeat_from_step_3>
        </if_not_merged>
      </if_pr_found>
      <if_no_pr>
        <skip_worktree>No PR found - this branch was likely never submitted for review</skip_worktree>
        <continue_to_next>Continue checking next oldest worktree</continue_to_next>
      </if_no_pr>
    </verify_pr_merged>
    <purpose>Use PR/MR status as the authoritative source for merge verification instead of unreliable git commands</purpose>
  </step_4>

  <step_4_5>
    <description>Check and close related issue</description>
    <if_issue_found>
      <get_issue_details>
        <tool>Use $GIT_HOST_TOOL to read issue details</tool>
        <extract_repo>From issue reference (main-repo vs cross-repo)</extract_repo>
      </get_issue_details>
      <check_issue_state>
        <if_open>
          <ask_close>Ask user: "Related issue #{number} is still open. Should I close it? (y/N)"</ask_close>
          <if_yes_close>
            <add_closing_comment>
              <tool>Use $GIT_HOST_TOOL to add comment</tool>
              <body_template>Closing this issue as branch {branch_name} was merged to {default_branch} on {merge_date} via PR/MR #{pr_number}.</body_template>
              <get_merge_date>Extract merge date from PR/MR details</get_merge_date>
              <get_pr_number>Use PR/MR number from search results</get_pr_number>
            </add_closing_comment>
            <close_issue>
              <tool>Use $GIT_HOST_TOOL to close issue</tool>
              <state>closed</state>
              <state_reason>completed</state_reason>
            </close_issue>
          </if_yes_close>
        </if_open>
        <if_closed>Inform user issue is already closed</if_closed>
      </check_issue_state>
    </if_issue_found>
    <if_no_issue>Continue without issue management</if_no_issue>
    <purpose>Ensure proper issue lifecycle management</purpose>
  </step_4_5>

  <step_5>
    <description>Check if worktree is locked</description>
    <check_command>git worktree list --porcelain | grep -A5 "worktree {path}" | grep "locked"</check_command>
    <if_locked>
      <unlock_command>git worktree unlock {path}</unlock_command>
      <notify_user>Inform user that worktree was unlocked</notify_user>
    </if_locked>
    <purpose>Unlock worktree if it was locked for tracking purposes</purpose>
  </step_5>

  <step_6>
    <description>Analyze Claude settings differences</description>
    <read_main_settings>.claude/settings.local.json</read_main_settings>
    <read_worktree_settings>{worktree_path}/.claude/settings.local.json</read_worktree_settings>
    <compare_allow_lists>
      <extract_main_allows>Extract "allow" array from main settings</extract_main_allows>
      <extract_worktree_allows>Extract "allow" array from worktree settings</extract_worktree_allows>
      <find_differences>Identify entries in worktree that are not in main</find_differences>
    </compare_allow_lists>
    <filter_suggestions>
      <include_filesystem>Read permissions for filesystem paths</include_filesystem>
      <exclude_intrusive>Exclude bash commands, write permissions, etc.</exclude_intrusive>
      <focus_user_specific>Include only user-specific, non-disruptive entries</focus_user_specific>
    </filter_suggestions>
    <purpose>Identify useful settings to consolidate before cleanup</purpose>
  </step_6>

  <step_7>
    <description>Suggest settings consolidation</description>
    <if_differences_found>
      <display_suggestions>Show filtered differences to user</display_suggestions>
      <ask_confirmation>Ask user which entries to add to main settings</ask_confirmation>
      <apply_changes>Update main .claude/settings.local.json with selected entries</apply_changes>
    </if_differences_found>
    <if_no_differences>Inform user no settings need consolidation</if_no_differences>
    <purpose>Preserve useful development settings before removing worktree</purpose>
  </step_7>

  <step_8>
    <description>Final cleanup confirmation</description>
    <summary>
      <display_worktree>Show worktree path and branch name</display_worktree>
      <show_pr_status>Show merged PR details if found</show_pr_status>
      <show_issue_status>Show related issue status if found</show_issue_status>
      <show_last_activity>Display directory creation/modification date</show_last_activity>
    </summary>
    <safety_checks>
      <check_uncommitted>git status --porcelain in worktree directory</check_uncommitted>
      <warn_if_dirty>Alert user if uncommitted changes exist</warn_if_dirty>
    </safety_checks>
    <ask_deletion>Ask user confirmation: "Delete this worktree? (y/N)"</ask_deletion>
    <purpose>Final safety check before irreversible deletion</purpose>
  </step_8>

  <step_9>
    <description>Delete worktree</description>
    <if_confirmed>
      <remove_worktree>git worktree remove {path} --force</remove_worktree>
      <cleanup_branch>git branch -d {branch_name}</cleanup_branch>
      <success_message>Inform user worktree was successfully removed</success_message>
      <next_steps>Suggest running command again to find next candidate</next_steps>
    </if_confirmed>
    <if_declined>Exit gracefully with no changes</if_declined>
    <purpose>Perform the actual cleanup and guide user for next iteration</purpose>
  </step_9>
</execution_steps>

<important_notes>

- Uses PR/MR merge status as the ONLY reliable way to verify if a branch was merged
- DOES NOT use "git branch --merged" command as it's unreliable for merge verification
- Only processes branches with PRs/MRs that were definitively merged to default branch
- Skips worktrees without merged PRs/MRs and continues to next oldest candidate
- Checks and optionally closes related issues
- Prioritizes oldest worktrees by directory age first for systematic cleanup
- Warns about very recent worktrees (created within 24 hours) to avoid cleaning active work
- Preserves useful development settings before deletion
- Requires explicit confirmation before any destructive actions
- Handles locked worktrees automatically
- Processes one worktree at a time to maintain control
- Must be run from default branch for safety

Limitations:
- Assumes remote is named "origin" (most common convention)
- Supports macOS and Linux only (no Windows support)
- Requires MCP server or CLI for git hosting provider (GitHub, GitLab, etc.)
</important_notes>
"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for add-command.md 1`] = `
"---
description: Guide for creating new slash commands
argument-hint: <command-name> <description>
_hint: Create command
_category: Utilities
_order: 3
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






# Slash Command Creator Guide

## How This Command Works
The \`/add-command\` command shows this guide for creating new slash commands. It includes:
- Command structure and syntax
- Common patterns and examples
- Security restrictions and limitations
- Frontmatter options

**Note for AI**: When creating commands, you CAN use bash tools like \`Bash(mkdir:*)\`, \`Bash(ls:*)\`, \`Bash(git status:*)\` in the \`allowed-tools\` frontmatter of NEW commands - but ONLY for operations within the current project directory. This command itself doesn't need bash tools since it's just documentation.

## Command Locations
- **Personal**: \`~/.claude/commands/\` (available across all projects)
- **Project**: \`.claude/commands/\` (shared with team, shows "(project)")

## Basic Structure

\`\`\`markdown
---
allowed-tools: Read, Glob, Grep, Bash(git status:*), Task
description: Brief description of what this command does
argument-hint: [required-arg] [optional-arg]
---

# Command Title

Your command instructions here.

**User arguments:**

Add-command: $ARGUMENTS

**End of user arguments**

File reference: @path/to/file.js

Bash command output: (exclamation)git status(backticks)
\`\`\`

## âš ï¸ Security Restrictions

**Bash Commands (exclamation prefix)**: Limited to current working directory only.
- âœ… Works: \`! + backtick + git status + backtick\` (in project dir)
- âŒ Blocked: \`! + backtick + ls /outside/project + backtick\` (outside project)  
- âŒ Blocked: \`! + backtick + pwd + backtick\` (if referencing dirs outside project)

**File References (\`@\` prefix)**: No directory restrictions.
- âœ… Works: \`@/path/to/system/file.md\`
- âœ… Works: \`@../other-project/file.js\`

## Common Patterns

### Simple Command
\`\`\`bash
echo "Review this code for bugs and suggest fixes" > ~/.claude/commands/review.md
\`\`\`

### Command with Arguments
**Note for AI**: The example below uses a fullwidth dollar sign (ï¼„, U+FF04) to prevent interpolation in this documentation. When creating actual commands, use the regular \`$\` character.

\`\`\`markdown
Fix issue ï¼„ARGUMENTS following our coding standards
\`\`\`

### Command with File References
\`\`\`markdown
Compare @src/old.js with @src/new.js and explain differences
\`\`\`

### Command with Bash Output (Project Directory Only)
\`\`\`markdown
---
allowed-tools: Bash(git status:*), Bash(git branch:*), Bash(git log:*)
---
Current status: (!)git status(\`)
Current branch: (!)git branch --show-current(\`)
Recent commits: (!)git log --oneline -5(\`)

Create commit for these changes.
\`\`\`

**Note**: Only works with commands in the current project directory.

### Namespaced Command
**Note for AI**: The example below uses a fullwidth dollar sign (ï¼„, U+FF04) to prevent interpolation in this documentation. When creating actual commands, use the regular \`$\` character.

\`\`\`bash
mkdir -p ~/.claude/commands/ai
echo "Ask GPT-5 about: ï¼„ARGUMENTS" > ~/.claude/commands/ai/gpt5.md
# Creates: /ai:gpt5
\`\`\`

## Frontmatter Options
- \`allowed-tools\`: Tools this command can use
  - **Important**: Intrusive tools like \`Write\`, \`Edit\`, \`NotebookEdit\` should NEVER be allowed in commands unless the user explicitly requests them. These tools modify files and should only be used when the command's purpose is to make changes.
  - âœ… Safe for most commands: \`Read\`, \`Glob\`, \`Grep\`, \`Bash(git status:*)\`, \`Task\`, \`AskUserQuestion\`
- \`description\`: Brief description (shows in /help)
- \`argument-hint\`: Help text for arguments
- \`model\`: Specific model to use

## Best Practices

### Safe Commands (No Security Issues)
\`\`\`markdown
# System prompt editor (file reference only)  
(@)path/to/system/prompt.md

Edit your system prompt above.
\`\`\`

### Project-Specific Commands (Bash OK)
\`\`\`markdown
---
allowed-tools: Bash(git status:*), Bash(npm list:*)
---
Current git status: (!)git status(\`)
Package info: (!)npm list --depth=0(\`)

Review project state and suggest next steps.
\`\`\`

### Cross-Directory File Access (Use @ not !)
\`\`\`markdown
# Compare config files
Compare (@)path/to/system.md with (@)project/config.md

Show differences and suggest improvements.
\`\`\`

## Usage
After creating: \`/<command-name> [arguments]\`

Example: \`/review\` or \`/ai:gpt5 "explain this code"\`
"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for ask.md 1`] = `
"---
description: Request team review and approval - for complex changes needing discussion
argument-hint: [optional-pr-title-and-description]
_hint: Request review
_category: Ship / Show / Ask
_order: 3
_selectedByDefault: false
---

# Ask - Request Review and Approval

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






**Ship/Show/Ask Pattern - ASK**

Ask is for complex changes that need team discussion and approval. Examples:

- Breaking API changes
- New architecture decisions
- Significant feature additions
- Performance trade-offs
- Security-sensitive changes

## When to Ask

Use **Ask** when:

- Changes affect multiple systems
- Breaking changes are needed
- You need input on approach
- Security implications exist
- Performance trade-offs need discussion
- Uncertain about the best solution

## Workflow

Current branch status:
!\`git status\`

Recent commits:
!\`git log --oneline -5\`

**User arguments:**

Ask: $ARGUMENTS

**End of user arguments**

**This is the traditional Pull Request workflow**, but with explicit intent that review and approval are required.

**Process:**

1. **Ensure Branch is Ready**:
   !\`git status\`
   - Commit all changes
   - Push to remote: \`git push origin [branch-name]\`

2. **Create Ask PR**: Create a PR that clearly needs review

   Title: conventional commits format, prefixed with \`[ASK]\`

   Description template:

   \`\`\`markdown
   ## ðŸ¤” Ask - Review and Approval Needed

   **This is an ASK PR**: These changes need team review and discussion.

   <!--
     References: [link to relevant issues]
   -->

   ### What changed

   [Detailed description of changes]

   ### Why

   [Rationale and context]

   ### Questions for reviewers

   - [ ] Question 1
   - [ ] Question 2

   ### Concerns

   - Potential concern 1
   - Potential concern 2

   ### Test Plan

   - [ ] Unit tests
   - [ ] Integration tests
   - [ ] Manual testing steps

   ### Alternatives considered

   - Alternative 1: [why not chosen]
   - Alternative 2: [why not chosen]
   \`\`\`

3. **Request Reviewers**: Assign specific reviewers who should weigh in

4. **Add Labels**:
   - "needs-review"
   - "breaking-change" (if applicable)
   - "security" (if applicable)

5. **Link Issues**: Reference related issues in the description

6. **Monitor Discussion**: Be responsive to reviewer feedback and questions

## Use GitHub MCP Tools

1. Check current branch and ensure it's pushed
2. Create a well-formatted pull request with [ASK] prefix
3. Set reviewers
4. Add appropriate labels
5. Link related issues from commit messages

## Decision Guide

Use **Ask** when:

- âœ… Change is complex or risky
- âœ… Breaking changes involved
- âœ… Need team input on approach
- âœ… Multiple solutions possible
- âœ… Security implications

Use **/show** instead if: confident in approach, just want visibility

Use **/ship** instead if: change is tiny, obvious, and safe


"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for beepboop.md 1`] = `
"---
description: Communicate AI-generated content with transparent attribution
argument-hint: <task-description>
_hint: AI attribution
_category: Utilities
_order: 2
---

# AI-Attributed Communication Command

Execute the user's requested task (e.g., posting PR comments, GitHub issue comments, or other communications through various MCPs), but frame the output with clear AI attribution.

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






## Instructions

**User arguments:**

Beepboop: $ARGUMENTS

**End of user arguments**

**IMPORTANT Communication Format:**

1. **Opening**: Begin with "*Beep boop, I am Claude Code ðŸ¤–, my user has reviewed and approved the following written by me:*"
   - Use italics for this line
   - Clearly establishes AI authorship

2. **Middle**: Perform the requested task (post comment, create review, etc.)
   - Execute whatever communication task the user requested
   - Write the actual content that accomplishes the user's goal

3. **Closing**: End with "*Beep boop, Claude Code ðŸ¤– out!*"
   - Use italics for this line
   - Provides clear closure

## Purpose

This command ensures transparency about AI usage while maintaining that the user has reviewed and approved the content. It prevents offloading review responsibility to other users while being open about AI assistance.

## Examples

- Posting a GitHub PR review comment
- Adding a comment to a GitHub issue
- Responding to feedback with AI-generated explanations
- Any communication where AI attribution is valuable
"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for busycommit.md 1`] = `
"---
description: Create multiple atomic git commits, one logical change at a time
argument-hint: [optional-commit-description]
_hint: Atomic commits
_category: Workflow
_order: 2
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






Create multiple atomic git commits, committing the smallest possible logical unit at a time

**User arguments:**

Busycommit: $ARGUMENTS

**End of user arguments**

## Commit Message Rules

Follows [Conventional Commits](https://www.conventionalcommits.org/) standard.

1. **Format**: \`type(#issue): description\`
   - Use \`#123\` for local repo issues
   - Use \`owner/repo#123\` for cross-repo issues
   - Common types: \`feat\`, \`fix\`, \`docs\`, \`refactor\`, \`test\`, \`chore\`

2. **AI Credits**: **NEVER include AI credits in commit messages**
   - No "Generated with Claude Code"
   - No "Co-Authored-By: Claude" or "Co-Authored-By: Happy"
   - Focus on the actual changes made, not conversation history

3. **Content**: Write clear, concise commit messages describing what changed and why

## Process

1. Run \`git status\` and \`git diff\` to review changes
2. Run \`git log --oneline -5\` to see recent commit style
3. Stage relevant files with \`git add\`
4. Create commit with descriptive message
5. Verify with \`git status\`

## Example

\`\`\`bash
git add <files>
git commit -m "feat(#123): add validation to user input form"
\`\`\`


## Atomic Commit Approach

Each commit should represent ONE logical change. Do NOT bundle multiple unrelated changes into one commit.

- Identify the smallest atomic units of change
- For EACH atomic unit: stage only those files/hunks, commit, verify
- Use \`git add -p\` to stage partial file changes when a file contains multiple logical changes
- Repeat until all changes are committed
- It is OK to create multiple commits without stopping - keep going until \`git status\` shows clean

## Multi-Commit Example

If a single file contains multiple unrelated changes, use \`git add -p\` to stage hunks interactively:

\`\`\`bash
# Stage only the validation-related hunks from the file
git add -p src/user-service.ts
# Select 'y' for validation hunks, 'n' for others
git commit -m "feat(#123): add email format validation"

# Stage the error handling hunks
git add -p src/user-service.ts
git commit -m "fix(#124): handle null user gracefully"

# Stage remaining changes
git add src/user-service.ts
git commit -m "refactor: extract user lookup to helper"
\`\`\`
"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for code-review.md 1`] = `
"---
description: Code review using dynamic category detection and domain-specific analysis
argument-hint: (optional) [branch, PR#, or PR URL] - defaults to current branch
_hint: Review code
_category: Workflow
_order: 35
_requested-tools:
  - Bash(git diff:*)
  - Bash(git status:*)
  - Bash(git log:*)
  - Bash(git rev-parse:*)
  - Bash(git merge-base:*)
  - Bash(git branch:*)
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






# Code Review

Perform a code review using dynamic category detection.

## Phase 0: Setup & Categorization

### Determine What to Review

Parse the argument to determine the review target:

| Input | Action |
|-------|--------|
| No argument | Detect divergence point, confirm scope with user |
| Branch name | Use specified branch as base |
| PR number (e.g., \`123\`) | Fetch PR diff from GitHub |
| PR URL (e.g., \`https://github.com/owner/repo/pull/123\`) | Extract PR number and fetch diff |



**For GitHub PRs:**

1. Try GitHub MCP first: \`mcp__github__pull_request_read\` with \`method: "get_diff"\`
2. Fall back to \`gh\` CLI: \`gh pr diff <number>\`
3. If neither works, report error and stop


**For local branches (no argument or branch name provided):**

1. **Get current branch**: \`git rev-parse --abbrev-ref HEAD\`

2. **Check for uncommitted changes**: \`git status --porcelain\`
   - If output is non-empty, note that uncommitted changes exist

3. **Detect divergence point** (skip if branch name was provided as argument):
   - Get all local branches except current: \`git branch --format='%(refname:short)'\`
   - For each branch, find merge-base: \`git merge-base HEAD <branch>\`
   - Count commits from merge-base to HEAD: \`git rev-list --count <merge-base>..HEAD\`
   - The branch with the **fewest commits back** (closest merge-base) is the likely parent
   - If no other branches exist, fall back to \`main\`, \`master\`, or \`develop\` if they exist as remote tracking branches

4. **Confirm scope with user** using \`AskUserQuestion\`:

   **Question 1 - "Review scope"** (header: "Base branch"):
   - Option A: \`From <detected-branch>\` â€” "Review N commits since diverging from <branch>"
   - Option B: \`Different branch\` â€” "Specify another branch to compare against"
   - Option C: \`Uncommitted only\` â€” "Review only staged/unstaged changes, skip committed work"

   **Question 2 - "Include uncommitted?"** (header: "Uncommitted", only ask if uncommitted changes exist AND user didn't pick option C):
   - Option A: \`Yes\` â€” "Include N staged/unstaged files in review"
   - Option B: \`No\` â€” "Review only committed changes"

5. **Collect changed files** based on user selection:
   - From branch: \`git diff --name-only <base>...HEAD\`
   - Uncommitted unstaged: \`git diff --name-only\`
   - Uncommitted staged: \`git diff --name-only --cached\`
   - Combine and deduplicate the file list

6. **If no changes**: Report "Nothing to review" and stop

### Categorize Files

Check for CLAUDE.md - if it exists, note any project-specific review patterns.

Categorize each changed file into ONE primary category based on these patterns:

| Category | File Patterns |
|----------|---------------|
| Frontend/UI | \`*.tsx\`, \`*.jsx\`, \`components/\`, \`pages/\`, \`views/\`, \`*.vue\` |
| Frontend/Styling | \`*.css\`, \`*.scss\`, \`*.less\`, \`styles/\`, \`*.tailwind*\`, \`*.styled.*\` |
| Backend/API | \`routes/\`, \`api/\`, \`controllers/\`, \`services/\`, \`*.controller.*\`, \`*.service.*\`, \`*.resolver.*\` |
| Backend/Data | \`migrations/\`, \`models/\`, \`prisma/\`, \`schema.*\`, \`*.model.*\`, \`*.entity.*\` |
| Tooling/Config | \`scripts/\`, \`*.config.*\`, \`package.json\`, \`tsconfig.*\`, \`vite.*\`, \`webpack.*\`, \`eslint.*\` |
| CI/CD | \`.github/\`, \`.gitlab-ci.*\`, \`Dockerfile\`, \`docker-compose.*\`, \`*.yml\` in CI paths |
| Tests | \`*.test.*\`, \`*.spec.*\`, \`__tests__/\`, \`__mocks__/\`, \`*.stories.*\` |
| Docs | \`*.md\`, \`docs/\`, \`README*\`, \`CHANGELOG*\` |


Output the categorization:

\`\`\`
## Categorization

Base branch: <branch>
Total files changed: <n>

| Category | Files |
|----------|-------|
| <category> | <count> |
...
\`\`\`

## Phase 1: Branch Brief

From the diff and recent commit messages (\`git log <base>...HEAD --oneline\`), infer:

- **Goal**: What this branch accomplishes (1-3 sentences)
- **Constraints**: Any implied requirements (security, performance, backwards compatibility)
- **Success checklist**: What must work after this change, what must not break

\`\`\`
## Branch Brief

**Goal**: ...
**Constraints**: ...
**Checklist**:
- [ ] ...
\`\`\`

## Phase 2: Category Reviews

For each detected category with changes, run a targeted review. Skip categories with no changes.

### Frontend/UI Review Criteria

- Accessibility: ARIA attributes, keyboard navigation, screen reader support
- Component patterns: Composition, prop drilling, context usage
- State management: Unnecessary re-renders, stale closures
- Performance: memo/useMemo/useCallback usage, lazy loading, bundle impact

### Frontend/Styling Review Criteria

- Responsive design: Breakpoints, mobile-first
- Design system: Token usage, consistent spacing/colors
- CSS specificity: Overly specific selectors, !important usage
- Theme support: Dark mode, CSS variables

### Backend/API Review Criteria

- Input validation: Sanitization, type checking, bounds
- Security: Authentication checks, authorization, injection risks
- Error handling: Proper status codes, meaningful messages, logging
- Performance: N+1 queries, missing indexes, pagination

### Backend/Data Review Criteria

- Migration safety: Reversibility, data preservation
- Data integrity: Constraints, foreign keys, nullability
- Index usage: Queries have appropriate indexes
- Backwards compatibility: Existing data still works

### Tooling/Config Review Criteria

- Breaking changes: Does this affect developer workflow?
- Dependency compatibility: Version conflicts, peer deps
- Build performance: Added build time, bundle size

### CI/CD Review Criteria

- Secrets exposure: Credentials in logs, env vars
- Pipeline efficiency: Caching, parallelization
- Failure handling: Notifications, rollback strategy

### Tests Review Criteria

#### FIRST Principles

| Principle | What to Check |
|-----------|---------------|
| **Fast** | Tests complete quickly, no I/O, no network calls, no sleep()/setTimeout delays |
| **Independent** | No shared mutable state, no execution order dependencies between tests |
| **Repeatable** | No Date.now(), no Math.random() without seeding, no external service dependencies |
| **Self-validating** | Meaningful assertions that verify behavior, no manual verification needed |

#### TDD Anti-patterns

| Anti-pattern | Detection Signals |
|--------------|-------------------|
| **The Liar** | \`expect(true).toBe(true)\`, empty test bodies, tests with no assertions |
| **Excessive Setup** | >20 lines of arrange code, >5 mocks, deep nested object construction |
| **The One** | >5 assertions testing unrelated behaviors in a single test |
| **The Peeping Tom** | Testing private methods, asserting on internal state, tests that break on any refactor |
| **The Slow Poke** | Real database/network calls, file I/O, hard-coded timeouts |

#### Test Structure (AAA Pattern)

- **Arrange**: Clear setup with minimal fixtures
- **Act**: Single action being tested
- **Assert**: Specific, behavior-focused assertions


### Docs Review Criteria

- Technical accuracy: Code examples work, APIs documented correctly
- Completeness: All new features documented
- Clarity: Easy to follow, good examples

**Output format per category:**

\`\`\`
## <Category> Review (<n> files)

### file:line - [blocker|risky|nit] Title
Description of the issue and why it matters.
Suggested fix or question to investigate.

...
\`\`\`

## Phase 3: Cross-Cutting Analysis

After reviewing all categories, check for cross-cutting issues:

- API changed but tests didn't update?
- New feature but no documentation?
- Migration added but no rollback tested?
- Config changed but README not updated?
- Security-sensitive code without corresponding test?

\`\`\`
## Cross-Cutting Issues

- [ ] <issue description>
...
\`\`\`

## Phase 4: Summary

### PR Description (draft)

Provide a ready-to-paste PR description:

\`\`\`
## What changed
- <by category, 1-2 bullets each>

## Why
- <motivation>

## Testing
- <how to verify>

## Notes
- <migration steps, breaking changes, etc.>
\`\`\`

### Review Checklist

\`\`\`
## Before Merge

### Blockers (must fix)
- [ ] ...

### Risky (highlight to reviewers)
- [ ] ...

### Follow-ups (can defer)
- [ ] ...
\`\`\`

---

**User arguments:**

Code-review: $ARGUMENTS

**End of user arguments**
"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for commit.md 1`] = `
"---
description: Create a git commit following project standards
argument-hint: [optional-commit-description]
_hint: Git commit
_category: Workflow
_order: 1
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






Create a git commit following project standards

**User arguments:**

Commit: $ARGUMENTS

**End of user arguments**

## Commit Message Rules

Follows [Conventional Commits](https://www.conventionalcommits.org/) standard.

1. **Format**: \`type(#issue): description\`
   - Use \`#123\` for local repo issues
   - Use \`owner/repo#123\` for cross-repo issues
   - Common types: \`feat\`, \`fix\`, \`docs\`, \`refactor\`, \`test\`, \`chore\`

2. **AI Credits**: **NEVER include AI credits in commit messages**
   - No "Generated with Claude Code"
   - No "Co-Authored-By: Claude" or "Co-Authored-By: Happy"
   - Focus on the actual changes made, not conversation history

3. **Content**: Write clear, concise commit messages describing what changed and why

## Process

1. Run \`git status\` and \`git diff\` to review changes
2. Run \`git log --oneline -5\` to see recent commit style
3. Stage relevant files with \`git add\`
4. Create commit with descriptive message
5. Verify with \`git status\`

## Example

\`\`\`bash
git add <files>
git commit -m "feat(#123): add validation to user input form"
\`\`\`

"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for commitlint-checklist-nodejs.md 1`] = `
"---
description: Audit commit hook automation for Node.js projects
argument-hint: (no arguments - interactive)
_hint: Audit commit hooks (Node.js)
_category: Utilities
_order: 25
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product




# Commit Hook Automation Checklist (Node.js)

Scan the Node.js repository and report what commit automation is in place.

**User arguments:**

Commitlint-checklist: $ARGUMENTS

**End of user arguments**

## Checks to Scan

Scan the repository for these configurations (do not display this list to user):

**ðŸ”§ Infrastructure**
- Husky: \`.husky/\`, \`package.json\` â†’ \`"prepare": "husky"\`
- Pre-commit hook: \`.husky/pre-commit\`
- lint-staged: \`lint-staged.config.*\`, \`.lintstagedrc.*\`, or \`package.json\` lint-staged key

**ðŸ§¹ Code Quality**
- Linting: \`eslint.config.*\`, \`biome.json\`, \`.eslintrc.*\`
- Formatting: \`.prettierrc*\`, \`biome.json\`
- Type checking: \`tsconfig.json\` + lint-staged runs \`tsc --noEmit\`
- Knip: \`knip.json\` or knip in scripts
- jscpd: \`.jscpd.json\` or jscpd in scripts

**ðŸ”’ Security**
- Secret scanning: \`.secretlintrc.json\`, secretlint in dependencies

**ðŸ“ Commits**
- Conventional commits: \`.husky/commit-msg\`, \`commitlint.config.*\`, \`@commitlint/*\` in deps

**ðŸ§ª Testing**
- Pre-commit tests: lint-staged or pre-commit hook runs test command
- Coverage thresholds: vitest/jest config with coverage thresholds

## Output Format

Display ONLY this summary format (no tables, no detailed breakdown):

\`\`\`
âœ… Passing:
  ðŸ”§ Infrastructure: Husky, pre-commit hook, lint-staged
  ðŸ§¹ Code Quality: Linting, formatting, type checking, Knip, jscpd
  ðŸ§ª Testing: Pre-commit tests, coverage thresholds

âŒ Missing (2):
  â€¢ Secret scanning (secretlint)
  â€¢ Conventional commits (commitlint)

ðŸ“Š 10/12 checks passing
\`\`\`

Omit categories with no passing checks from the "Passing" section.

## Follow-up

After the summary, tell the user:

> For working examples of most items above, see [hevolx/agent-instructions](https://github.com/hevolx/agent-instructions) on GitHub.
>
> Would you like help implementing any of the missing checks?
"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for create-adr.md 1`] = `
"---
description: Create a new Architecture Decision Record (ADR)
argument-hint: <title or topic of the architectural decision>
_hint: Create ADR
_category: Utilities
_order: 20
---

# Create ADR: Architecture Decision Record Creator

Create a new ADR to document an architectural decision. ADRs capture the "why" behind technical choices, helping future developers understand constraints and tradeoffs.

> ADRs were introduced by Michael Nygard in 2011. The core structure (Context, Decision, Consequences) remains the standard.

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






**User arguments:**

Create-adr: $ARGUMENTS

**End of user arguments**

(If no input provided, ask user for the architectural decision topic)

## Process

### Step 1: Detect Existing ADR Setup

Check for existing ADR directory and structure:

\`\`\`bash
# Check common ADR directories (in order of preference)
for dir in doc/adr docs/adr decisions doc/architecture/decisions; do
  if [ -d "$dir" ]; then
    echo "Found: $dir"
    ls "$dir"/*.md 2>/dev/null
    break
  fi
done
\`\`\`

**If ADRs exist:** Read the first few ADRs (especially 0001 if present) to understand:

- The template/structure this project uses
- Any project-specific sections or frontmatter
- Naming conventions and style

Adapt the new ADR to match the existing pattern.

**If no ADR directory exists:** Run the initialization flow (Step 1b).

### Step 1b: Initialize ADR Practice (First-Time Setup)

When no existing ADRs are found, help the user set up their ADR practice.

Ask the user (use AskUserQuestion):

**Directory location:**

- doc/adr (Recommended - conventional location)
- docs/adr
- decisions
- doc/architecture/decisions

**Template style:**

- Minimal (Nygard's original: Context, Decision, Consequences)
- Standard (Minimal + Status, Date, References)
- With scope (Standard + applies_to patterns, code examples)

**Create a foundational ADR-0001?**

- Yes - document "We will use ADRs to record architectural decisions"
- No - proceed directly to creating the requested ADR

If creating ADR-0001, generate it with:

- Context: Why the team needs to document decisions
- Decision: Adopt ADRs following [chosen template style]
- Consequences: Better knowledge transfer, slight overhead per decision

### Step 2: Determine ADR Number

Calculate next number from existing ADRs:

1. Extract highest existing number
2. Increment by 1
3. Format as 4-digit zero-padded (e.g., \`0001\`, \`0012\`)

### Step 3: Discovery Questions

Gather context through conversation (use AskUserQuestion for structured choices):

**Context & Problem**

- What forces are at play? (technological, social, project constraints)
- What problem, pattern, or situation prompted this decision?
- What triggered the need to decide now? (bug, confusion, inconsistency, new requirement)
- Are there related PRs, issues, or prior discussions to reference?

**The Decision**

- What are we deciding to do (or not do)?
- What alternatives were considered?
- Why was this approach chosen over alternatives?

**Consequences**

- What becomes easier or more consistent with this decision?
- What becomes harder, more constrained, or riskier?
- What tradeoffs are we explicitly accepting?

**Scope**

- Which parts of the codebase does this apply to?
- Are there exceptions or areas where this doesn't apply?

### Step 4: Generate ADR File

Create \`{adr_directory}/NNNN-title-slug.md\`:

- Convert title to kebab-case slug (lowercase, hyphens, no special chars)
- Use today's date for the \`date\` field
- Default status to \`accepted\` (most ADRs are written after the decision is made)

**ADR Template:**

\`\`\`markdown
---
status: accepted
date: YYYY-MM-DD
applies_to:
  - "**/*.ts"
  - "**/*.tsx"
---

# N. Title

## Context

[Forces at play - technological, social, project constraints.
What problem prompted this? Value-neutral description of the situation.]

## Decision

We will [decision statement in active voice].

[If the decision involves code patterns, include concrete examples:]

**Forbidden pattern:**
\\\`\\\`\\\`typescript
// âŒ BAD - [explanation]
[example of what NOT to do]
\\\`\\\`\\\`

**Required pattern:**
\\\`\\\`\\\`typescript
// âœ… GOOD - [explanation]
[example of what TO do]
\\\`\\\`\\\`

## Consequences

**Positive:**
- [What becomes easier]
- [What becomes more consistent]

**Negative:**
- [What becomes harder]
- [What constraints we accept]

**Neutral:**
- [Other impacts worth noting]

## References

- [Related PRs, issues, or documentation]
\`\`\`

### Step 5: Refine applies_to Scope

Help user define which files this decision applies to using glob patterns:

- All TypeScript files: **/*.ts
- All React component files: **/*.tsx
- Only files in components directory: src/components/**
- Exclude test files (prefix with !): !**/*.test.ts
- Exclude type definition files: !**/*.d.ts
- Specific package only: packages/api/**

If the decision applies broadly, use **/* (all files).

**Note**: \`applies_to\` is recommended for the "With scope" template. Linters and AI assistants use these patterns to determine which files to check against this ADR.

### Step 6: Confirm and Write

Show the complete ADR content and ask user to confirm before writing.

After creation, suggest:

- Review the ADR for completeness
- Commit with \`/commit\`

## Tips for Good ADRs

1. **Focus on the "why"** - The decision itself may be obvious; the reasoning often isn't
2. **Keep it concise** - 1-2 pages maximum; should be readable in 5 minutes
3. **Use active voice** - "We will use X" not "X will be used"
4. **Include concrete examples** - Code examples make abstract decisions tangible
5. **Document tradeoffs honestly** - Every decision has costs; be explicit about them
6. **Link to context** - Reference PRs, issues, or discussions where the decision was made
7. **Be specific about scope** - Use \`applies_to\` patterns to clarify affected code

## Status Values

| Status | When to Use |
|--------|-------------|
| \`proposed\` | Under discussion, not yet agreed |
| \`accepted\` | Agreed upon and should be followed |
| \`deprecated\` | No longer relevant (context changed) |
| \`superseded\` | Replaced by another ADR (link to it) |

To supersede an existing ADR:

1. Create new ADR with the updated decision
2. Update old ADR's status to \`superseded by ADR-NNNN\`

## Integration with Other Commands

- After creating: Commit with \`/commit\`
- If decision needs discussion: Create issue with \`/create-issues\`
"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for create-issues.md 1`] = `
"---
description: Create implementation plan from feature/requirement with PRD-style discovery and TDD acceptance criteria
argument-hint: <feature/requirement description or GitHub issue URL/number>
_hint: Create issues
_category: Planning
_order: 2
---

# Create Issues: PRD-Informed Task Planning for TDD

Create structured implementation plan that bridges product thinking (PRD) with test-driven development.

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






**User arguments:**

Create-issues: $ARGUMENTS

**End of user arguments**

(If no input provided, check conversation context)

## Input Processing

The input can be one of:
1. **GitHub Issue URL** (e.g., \`https://github.com/owner/repo/issues/123\`)
2. **GitHub Issue Number** (e.g., \`#123\` or \`123\`)
3. **Feature Description** (e.g., "Add user authentication")
4. **Empty** - use conversation context

### GitHub Issue Integration

If input looks like a GitHub issue:

**Step 1: Extract Issue Number**
- From URL: extract owner/repo/number
- From number: try to infer repo from git remote
- From branch name: check patterns like \`issue-123\`, \`123-feature\`, \`feature/123\`

**Step 2: Fetch Issue**



Try to fetch the issue using GitHub MCP (mcp__github__issue_read tool).

If GitHub MCP is not configured, show:
\`\`\`
GitHub MCP not configured!
See: https://github.com/modelcontextprotocol/servers/tree/main/src/github
Trying GitHub CLI fallback...
\`\`\`

Then try using \`gh issue view [ISSUE_NUMBER] --json\` as fallback.


**Step 3: Use Issue as Discovery Input**
- Title â†’ Feature name
- Description â†’ Problem statement and context
- Labels â†’ Type/priority hints
- Comments â†’ Additional requirements and discussion
- Linked issues â†’ Dependencies

Extract from GitHub issue:
- Problem statement and context
- Acceptance criteria (if present)
- Technical notes (if present)
- Related issues/dependencies

## Process

## Discovery Phase

Understand the requirement by asking (use AskUserQuestion if needed):

**Problem Statement**
- What problem does this solve?
- Who experiences this problem?
- What's the current pain point?

**Desired Outcome**
- What should happen after this is built?
- How will users interact with it?
- What does success look like?

**Scope & Constraints**
- What's in scope vs. out of scope?
- Any technical constraints?
- Dependencies on other systems/features?

**Context Check**
- Search codebase for related features/modules
- Check for existing test files that might be relevant




## Key Principles

**From PRD World:**
- Start with user problems, not solutions
- Define success criteria upfront
- Understand constraints and scope

**From TDD World:**
- Make acceptance criteria test-ready
- Break work into small, testable pieces
- Each task should map to test(s)



## Integration with Other Commands

- **Before /create-issues**: Use \`/spike\` if you need technical exploration first
- **After /create-issues**: Use \`/red\` to start TDD on first task

"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for cycle.md 1`] = `
"---
description: Execute complete TDD cycle - Red, Green, and Refactor phases in sequence
argument-hint: <feature or requirement description>
_hint: Full TDD cycle
_category: Test-Driven Development
_order: 5
---

**User arguments:**

Cycle: $ARGUMENTS

**End of user arguments**

RED+GREEN+REFACTOR (one cycle) PHASE! Apply the below to the user input above.

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






(If there was no info above, fallback to the context of the conversation)


## TDD Fundamentals

### The TDD Cycle

The foundation of TDD is the Red-Green-Refactor cycle:

1. **Red Phase**: Write ONE failing test that describes desired behavior

   - The test must fail for the RIGHT reason (not syntax/import errors)
   - Only one test at a time - this is critical for TDD discipline
     - Exception: For browser-level tests or expensive setup (e.g., Storybook \`*.stories.tsx\`), group multiple assertions within a single test block to avoid redundant setup - but only when adding assertions to an existing interaction flow. If new user interactions are required, still create a new test. Split files by category if they exceed ~1000 lines.
   - **Adding a single test to a test file is ALWAYS allowed** - no prior test output needed
   - Starting TDD for a new feature is always valid, even if test output shows unrelated work
   - For DOM-based tests, use \`data-testid\` attributes to select elements rather than CSS classes, tag names, or text content
   - Avoid hard-coded timeouts both in form of sleep() or timeout: 5000 etc; use proper async patterns (\`waitFor\`, \`findBy*\`, event-based sync) instead and rely on global test configs for timeout settings

2. **Green Phase**: Write MINIMAL code to make the test pass

   - Implement only what's needed for the current failing test
   - No anticipatory coding or extra features
   - Address the specific failure message

3. **Refactor Phase**: Improve code structure while keeping tests green
   - Only allowed when relevant tests are passing
   - Requires proof that tests have been run and are green
   - Applies to BOTH implementation and test code
   - No refactoring with failing tests - fix them first

### Core Violations

1. **Multiple Test Addition**

   - Adding more than one new test at once
   - Exception: Initial test file setup or extracting shared test utilities

2. **Over-Implementation**

   - Code that exceeds what's needed to pass the current failing test
   - Adding untested features, methods, or error handling
   - Implementing multiple methods when test only requires one

3. **Premature Implementation**
   - Adding implementation before a test exists and fails properly
   - Adding implementation without running the test first
   - Refactoring when tests haven't been run or are failing

### Critical Principle: Incremental Development

Each step in TDD should address ONE specific issue:

- Test fails "not defined" â†’ Create empty stub/class only
- Test fails "not a function" â†’ Add method stub only
- Test fails with assertion â†’ Implement minimal logic only

### Optional Pre-Phase: Spike Phase

In rare cases where the problem space, interface, or expected behavior is unclear, a **Spike Phase** may be used **before the Red Phase**.
This phase is **not part of the regular TDD workflow** and must only be applied under exceptional circumstances.

- The goal of a Spike is **exploration and learning**, not implementation.
- The code written during a Spike is **disposable** and **must not** be merged or reused directly.
- Once sufficient understanding is achieved, all spike code is discarded, and normal TDD resumes starting from the **Red Phase**.
- A Spike is justified only when it is impossible to define a meaningful failing test due to technical uncertainty or unknown system behavior.

### General Information

- Sometimes the test output shows as no tests have been run when a new test is failing due to a missing import or constructor. In such cases, allow the agent to create simple stubs. Ask them if they forgot to create a stub if they are stuck.
- It is never allowed to introduce new logic without evidence of relevant failing tests. However, stubs and simple implementation to make imports and test infrastructure work is fine.
- In the refactor phase, it is perfectly fine to refactor both test and implementation code. That said, completely new functionality is not allowed. Types, clean up, abstractions, and helpers are allowed as long as they do not introduce new behavior.
- Adding types, interfaces, or a constant in order to replace magic values is perfectly fine during refactoring.
- Provide the agent with helpful directions so that they do not get stuck when blocking them.

"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for forever.md 1`] = `
"---
description: Run autonomously until stopped or stuck
argument-hint: [optional: initial task or focus area]
_hint: Autonomous mode
_category: Workflow
_order: 20
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product




**User arguments:**

Forever: $ARGUMENTS

**End of user arguments**

Run autonomously, finding and completing work until interrupted or truly stuck.

## Operating Loop

Execute this cycle continuously:

### 1. Find Work

Check in order until something is found:

1. **Arguments above** - If provided, start there
2. **Conversation context** - Any unfinished discussion
3. **Gaps** - Incomplete implementations or missing tests
4. **Git status** - Uncommitted changes to address
5. **Think** - If nothing else, consider: What would improve this codebase?

### 2. Execute Work

- Make atomic, committable progress
- Leave clear trail via commits

### 3. Continue or Pivot

After completing a unit of work:

- If more related work exists - continue
- If blocked - note blocker, find different work
- If genuinely nothing to do - report status and wait

**Do not stop unless:**

- User interrupts (Escape)
- Genuinely no work can be identified
- A decision requires human judgment (ambiguous requirements, architectural choices)

## Anti-Stuck Tactics

When progress stalls:

| Situation | Action |
|-----------|--------|
| Test failures | Make the failing tests pass |
| Unclear requirements | Make reasonable assumption, document it, proceed |
| Build errors | Fix incrementally, commit fixes |
| Context confusion | Re-read recent commits and task tracker to reorient |
| Repeated failures | Try different approach, or move to different task |

## Work Discovery Heuristics

When thinking about what to do:

- Tests that could be added (coverage gaps)
- Code that could be simplified
- Documentation that's missing or stale
- TODOs or FIXMEs in code
- Dependencies that could be updated
- Performance improvements
- Refactoring opportunities

## Session Continuity

Every few completed tasks:

- Update task tracker with progress notes
- Commit work in progress
- Brief self-summary of what's been done

This ensures work survives context limits.

## Communication Style

- Work silently - don't narrate every step
- Report meaningful completions (commits, closed issues)
- Surface decisions that need human input
- Keep responses concise to preserve context
"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for gap.md 1`] = `
"---
description: Analyze conversation context for unaddressed items and gaps
argument-hint: [optional additional info]
_hint: Find gaps
_category: Workflow
_order: 11
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






Analyze the current conversation context and identify things that have not yet been addressed. Look for:

1. **Incomplete implementations** - Code that was started but not finished
2. **Unused variables/results** - Values that were captured but never used
3. **Missing tests** - Functionality without test coverage

4. **User requests** - Things the user asked for that weren't fully completed
5. **TODO comments** - Any TODOs mentioned in conversation
6. **Error handling gaps** - Missing error cases or edge cases
7. **Documentation gaps** - Undocumented APIs or features
8. **Consistency check** - Look for inconsistent patterns, naming conventions, or structure across the codebase


Present findings as a prioritized list with:

- What the gap is
- Why it matters
- Suggested next action

If there are no gaps, confirm that everything discussed has been addressed.

**User arguments:**

Gap: $ARGUMENTS

**End of user arguments**
"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for green.md 1`] = `
"---
description: Execute TDD Green Phase - write minimal implementation to pass the failing test
argument-hint: <implementation description>
_hint: Make it pass
_category: Test-Driven Development
_order: 3
---

**User arguments:**

Green: $ARGUMENTS

**End of user arguments**

GREEN PHASE! Apply the below to the user input above.

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






(If there was no info above, fallback to the context of the conversation)


## TDD Fundamentals

### The TDD Cycle

The foundation of TDD is the Red-Green-Refactor cycle:

1. **Red Phase**: Write ONE failing test that describes desired behavior

   - The test must fail for the RIGHT reason (not syntax/import errors)
   - Only one test at a time - this is critical for TDD discipline
     - Exception: For browser-level tests or expensive setup (e.g., Storybook \`*.stories.tsx\`), group multiple assertions within a single test block to avoid redundant setup - but only when adding assertions to an existing interaction flow. If new user interactions are required, still create a new test. Split files by category if they exceed ~1000 lines.
   - **Adding a single test to a test file is ALWAYS allowed** - no prior test output needed
   - Starting TDD for a new feature is always valid, even if test output shows unrelated work
   - For DOM-based tests, use \`data-testid\` attributes to select elements rather than CSS classes, tag names, or text content
   - Avoid hard-coded timeouts both in form of sleep() or timeout: 5000 etc; use proper async patterns (\`waitFor\`, \`findBy*\`, event-based sync) instead and rely on global test configs for timeout settings

2. **Green Phase**: Write MINIMAL code to make the test pass

   - Implement only what's needed for the current failing test
   - No anticipatory coding or extra features
   - Address the specific failure message

3. **Refactor Phase**: Improve code structure while keeping tests green
   - Only allowed when relevant tests are passing
   - Requires proof that tests have been run and are green
   - Applies to BOTH implementation and test code
   - No refactoring with failing tests - fix them first

### Core Violations

1. **Multiple Test Addition**

   - Adding more than one new test at once
   - Exception: Initial test file setup or extracting shared test utilities

2. **Over-Implementation**

   - Code that exceeds what's needed to pass the current failing test
   - Adding untested features, methods, or error handling
   - Implementing multiple methods when test only requires one

3. **Premature Implementation**
   - Adding implementation before a test exists and fails properly
   - Adding implementation without running the test first
   - Refactoring when tests haven't been run or are failing

### Critical Principle: Incremental Development

Each step in TDD should address ONE specific issue:

- Test fails "not defined" â†’ Create empty stub/class only
- Test fails "not a function" â†’ Add method stub only
- Test fails with assertion â†’ Implement minimal logic only

### Optional Pre-Phase: Spike Phase

In rare cases where the problem space, interface, or expected behavior is unclear, a **Spike Phase** may be used **before the Red Phase**.
This phase is **not part of the regular TDD workflow** and must only be applied under exceptional circumstances.

- The goal of a Spike is **exploration and learning**, not implementation.
- The code written during a Spike is **disposable** and **must not** be merged or reused directly.
- Once sufficient understanding is achieved, all spike code is discarded, and normal TDD resumes starting from the **Red Phase**.
- A Spike is justified only when it is impossible to define a meaningful failing test due to technical uncertainty or unknown system behavior.

### General Information

- Sometimes the test output shows as no tests have been run when a new test is failing due to a missing import or constructor. In such cases, allow the agent to create simple stubs. Ask them if they forgot to create a stub if they are stuck.
- It is never allowed to introduce new logic without evidence of relevant failing tests. However, stubs and simple implementation to make imports and test infrastructure work is fine.
- In the refactor phase, it is perfectly fine to refactor both test and implementation code. That said, completely new functionality is not allowed. Types, clean up, abstractions, and helpers are allowed as long as they do not introduce new behavior.
- Adding types, interfaces, or a constant in order to replace magic values is perfectly fine during refactoring.
- Provide the agent with helpful directions so that they do not get stuck when blocking them.

"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for issue.md 1`] = `
"---
description: Analyze GitHub issue and create TDD implementation plan
argument-hint: [optional-issue-number]
_hint: Analyze issue
_category: Planning
_order: 1
---

Analyze GitHub issue and create TDD implementation plan.

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






Process:

1. Get Issue Number

**User arguments:**

Issue: $ARGUMENTS

**End of user arguments**

- Check if argument is an issue number
- Otherwise try branch name patterns: issue-123, 123-feature, feature/123, fix/123
- If not found: ask user

2. Fetch Issue



Try to fetch the issue using GitHub MCP (mcp__github__issue_read tool).

If GitHub MCP is not configured, show:
\`\`\`
GitHub MCP not configured!
See: https://github.com/modelcontextprotocol/servers/tree/main/src/github
Trying GitHub CLI fallback...
\`\`\`

Then try using \`gh issue view [ISSUE_NUMBER] --json\` as fallback.


3. Analyze and Plan

Summarize the issue and requirements, then:

## Discovery Phase

Understand the requirement by asking (use AskUserQuestion if needed):

**Problem Statement**
- What problem does this solve?
- Who experiences this problem?
- What's the current pain point?

**Desired Outcome**
- What should happen after this is built?
- How will users interact with it?
- What does success look like?

**Scope & Constraints**
- What's in scope vs. out of scope?
- Any technical constraints?
- Dependencies on other systems/features?

**Context Check**
- Search codebase for related features/modules
- Check for existing test files that might be relevant




## TDD Fundamentals

### The TDD Cycle

The foundation of TDD is the Red-Green-Refactor cycle:

1. **Red Phase**: Write ONE failing test that describes desired behavior

   - The test must fail for the RIGHT reason (not syntax/import errors)
   - Only one test at a time - this is critical for TDD discipline
     - Exception: For browser-level tests or expensive setup (e.g., Storybook \`*.stories.tsx\`), group multiple assertions within a single test block to avoid redundant setup - but only when adding assertions to an existing interaction flow. If new user interactions are required, still create a new test. Split files by category if they exceed ~1000 lines.
   - **Adding a single test to a test file is ALWAYS allowed** - no prior test output needed
   - Starting TDD for a new feature is always valid, even if test output shows unrelated work
   - For DOM-based tests, use \`data-testid\` attributes to select elements rather than CSS classes, tag names, or text content
   - Avoid hard-coded timeouts both in form of sleep() or timeout: 5000 etc; use proper async patterns (\`waitFor\`, \`findBy*\`, event-based sync) instead and rely on global test configs for timeout settings

2. **Green Phase**: Write MINIMAL code to make the test pass

   - Implement only what's needed for the current failing test
   - No anticipatory coding or extra features
   - Address the specific failure message

3. **Refactor Phase**: Improve code structure while keeping tests green
   - Only allowed when relevant tests are passing
   - Requires proof that tests have been run and are green
   - Applies to BOTH implementation and test code
   - No refactoring with failing tests - fix them first

### Core Violations

1. **Multiple Test Addition**

   - Adding more than one new test at once
   - Exception: Initial test file setup or extracting shared test utilities

2. **Over-Implementation**

   - Code that exceeds what's needed to pass the current failing test
   - Adding untested features, methods, or error handling
   - Implementing multiple methods when test only requires one

3. **Premature Implementation**
   - Adding implementation before a test exists and fails properly
   - Adding implementation without running the test first
   - Refactoring when tests haven't been run or are failing

### Critical Principle: Incremental Development

Each step in TDD should address ONE specific issue:

- Test fails "not defined" â†’ Create empty stub/class only
- Test fails "not a function" â†’ Add method stub only
- Test fails with assertion â†’ Implement minimal logic only

### Optional Pre-Phase: Spike Phase

In rare cases where the problem space, interface, or expected behavior is unclear, a **Spike Phase** may be used **before the Red Phase**.
This phase is **not part of the regular TDD workflow** and must only be applied under exceptional circumstances.

- The goal of a Spike is **exploration and learning**, not implementation.
- The code written during a Spike is **disposable** and **must not** be merged or reused directly.
- Once sufficient understanding is achieved, all spike code is discarded, and normal TDD resumes starting from the **Red Phase**.
- A Spike is justified only when it is impossible to define a meaningful failing test due to technical uncertainty or unknown system behavior.

### General Information

- Sometimes the test output shows as no tests have been run when a new test is failing due to a missing import or constructor. In such cases, allow the agent to create simple stubs. Ask them if they forgot to create a stub if they are stuck.
- It is never allowed to introduce new logic without evidence of relevant failing tests. However, stubs and simple implementation to make imports and test infrastructure work is fine.
- In the refactor phase, it is perfectly fine to refactor both test and implementation code. That said, completely new functionality is not allowed. Types, clean up, abstractions, and helpers are allowed as long as they do not introduce new behavior.
- Adding types, interfaces, or a constant in order to replace magic values is perfectly fine during refactoring.
- Provide the agent with helpful directions so that they do not get stuck when blocking them.

"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for kata.md 1`] = `
"---
description: Generate a TDD practice challenge with boilerplate test setup
argument-hint: (no arguments - interactive)
_hint: Practice kata
_category: Utilities
_order: 10
_requested-tools:
  - WebFetch(domain:raw.githubusercontent.com)
  - WebFetch(domain:api.github.com)
---

# Kata: TDD Practice Challenge Generator

Generate a complete TDD practice setup:
- \`CHALLENGE.md\` - Problem description
- Test file with first test placeholder
- Implementation file with empty function

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






**User arguments:**

Kata: $ARGUMENTS

**End of user arguments**

(This command is interactive - arguments are ignored)

## Data Source

Exercises from [cyber-dojo](https://github.com/cyber-dojo/exercises-start-points) (60 exercises, stable 10+ years).

- **List:** \`https://api.github.com/repos/cyber-dojo/exercises-start-points/contents/start-points\`
- **Fetch:** \`https://raw.githubusercontent.com/cyber-dojo/exercises-start-points/master/start-points/{NAME}/readme.txt\`

## Kata Categories

<kata_categories>
<by_difficulty>
  <beginner description="Simple logic, single function">
    Fizz_Buzz, Leap_Years, Prime_Factors, Word_Wrap, Closest_To_Zero,
    Remove_Duplicates, Array_Shuffle, Friday_13th, Five_Weekends,
    Number_Names, Print_Diamond, LCD_Digits, Fisher_Yates_Shuffle
  </beginner>
  <intermediate description="Multiple rules, edge cases">
    Roman_Numerals, Reverse_Roman, Bowling_Game, Tennis, Anagrams,
    ISBN, Balanced_Parentheses, Calc_Stats, Recently_Used_List,
    Phone_Numbers, Combined_Number, Group_Neighbours, Longest_Common_Prefix,
    Yatzy, Yatzy_Cutdown, Harry_Potter, Vending_Machine, Count_Coins,
    100_doors, Haiku_Review, ABC_Problem, Align_Columns, Best_Shuffle,
    Filename_Range, Unsplice, 12_Days_of_Xmas
  </intermediate>
  <advanced description="Complex algorithms, state management">
    Game_of_Life, Mars_Rover, Mine_Sweeper, Mine_Field, Eight_Queens,
    Knights_Tour, Reversi, Poker_Hands, Levenshtein_Distance,
    Magic_Square, Saddle_Points, Tiny_Maze, Gray_Code, Monty_Hall,
    Number_Chains, Wonderland_Number, Zeckendorf_Number,
    Diff_Selector, Diversion, Reordering, Fizz_Buzz_Plus
  </advanced>
</by_difficulty>

<by_type>
  <string_text description="String/Text manipulation">
    Anagrams, Word_Wrap, Align_Columns, Best_Shuffle, Haiku_Review,
    Phone_Numbers, 12_Days_of_Xmas, Longest_Common_Prefix, Unsplice
  </string_text>
  <math_numbers description="Math & Numbers">
    Fizz_Buzz, Fizz_Buzz_Plus, Prime_Factors, Roman_Numerals, Reverse_Roman,
    Zeckendorf_Number, Number_Names, Combined_Number, Closest_To_Zero,
    Count_Coins, ISBN, LCD_Digits, Leap_Years, Five_Weekends, Friday_13th
  </math_numbers>
  <data_structures description="Data Structures & Algorithms">
    Recently_Used_List, Balanced_Parentheses, Calc_Stats, Remove_Duplicates,
    Array_Shuffle, Fisher_Yates_Shuffle, Levenshtein_Distance, Gray_Code,
    Number_Chains, Wonderland_Number, 100_doors, Magic_Square, Saddle_Points,
    Diff_Selector, Diversion, Reordering
  </data_structures>
  <game_logic description="Game Logic & Simulation">
    Game_of_Life, Bowling_Game, Tennis, Yatzy, Yatzy_Cutdown,
    Mine_Sweeper, Mine_Field, Mars_Rover, Reversi, Poker_Hands,
    Harry_Potter, Eight_Queens, Knights_Tour, Tiny_Maze, Monty_Hall,
    Vending_Machine, Print_Diamond, ABC_Problem, Group_Neighbours, Filename_Range
  </game_logic>
</by_type>
</kata_categories>

## Process

Use conversational AskUserQuestion flow. User can skip to suggestions at any point.

<execution_steps>

<step_1>
  <description>Ask about difficulty preference</description>
  <prompt>
    <message>What difficulty level interests you?</message>
    <options>
      <option value="beginner">
        <label>Beginner</label>
        <description>Simple logic, single function (Fizz_Buzz, Leap_Years, Prime_Factors...)</description>
      </option>
      <option value="intermediate">
        <label>Intermediate</label>
        <description>Multiple rules, edge cases (Roman_Numerals, Bowling_Game, Tennis...)</description>
      </option>
      <option value="advanced">
        <label>Advanced</label>
        <description>Complex algorithms, state (Game_of_Life, Mars_Rover, Mine_Sweeper...)</description>
      </option>
      <option value="skip">
        <label>Just show me suggestions!</label>
        <description>Skip questions and see kata recommendations</description>
      </option>
    </options>
  </prompt>
  <if value="skip">Jump to step_suggest</if>
  <set_variable>$DIFFICULTY = selected value</set_variable>
</step_1>

<step_2>
  <description>Ask about challenge type</description>
  <prompt>
    <message>What kind of challenge interests you?</message>
    <options>
      <option value="string_text">
        <label>String/Text</label>
        <description>Text manipulation, parsing, formatting</description>
      </option>
      <option value="math_numbers">
        <label>Math & Numbers</label>
        <description>Calculations, conversions, number theory</description>
      </option>
      <option value="data_structures">
        <label>Data Structures</label>
        <description>Lists, algorithms, transformations</description>
      </option>
      <option value="game_logic">
        <label>Game Logic</label>
        <description>Games, simulations, state machines</description>
      </option>
      <option value="skip">
        <label>Show me suggestions now!</label>
        <description>Skip remaining questions</description>
      </option>
    </options>
  </prompt>
  <if value="skip">Jump to step_suggest</if>
  <set_variable>$TYPE = selected value</set_variable>
</step_2>

<step_suggest>
  <description>Show kata suggestions based on filters</description>
  <logic>
    - Filter kata_categories by $DIFFICULTY (if set)
    - Filter by $TYPE (if set)
    - If no filters, pick 3 varied suggestions
    - Select 3 katas that match criteria
    - For each kata, fetch a preview or use the descriptions below
  </logic>
  <kata_descriptions>
    <!-- Beginner -->
    Fizz_Buzz: Print 1-100 replacing multiples of 3/5 with Fizz/Buzz
    Leap_Years: Determine if a year is a leap year (divisibility rules)
    Prime_Factors: Find prime factors of a number
    Word_Wrap: Wrap text to fit within a column width
    Closest_To_Zero: Find the number closest to zero from a list
    Remove_Duplicates: Remove duplicate elements from a list
    Array_Shuffle: Randomly shuffle array elements
    Friday_13th: Count Friday the 13ths in a given year
    Five_Weekends: Find months with 5 Fridays, Saturdays, and Sundays
    Number_Names: Convert numbers to English words (one, two, three...)
    Print_Diamond: Print a diamond shape of letters
    LCD_Digits: Display numbers as LCD-style digits
    Fisher_Yates_Shuffle: Implement the Fisher-Yates shuffle algorithm

    <!-- Intermediate -->
    Roman_Numerals: Convert Arabic numbers to Roman numerals
    Reverse_Roman: Convert Roman numerals back to Arabic numbers
    Bowling_Game: Score a 10-pin bowling game with spares/strikes
    Tennis: Track tennis game score with deuce/advantage
    Anagrams: Find all anagrams of a word from a dictionary
    ISBN: Validate ISBN-10 check digits
    Balanced_Parentheses: Check if brackets are properly balanced
    Calc_Stats: Calculate min/max/count/average from numbers
    Recently_Used_List: Implement a most-recently-used list
    Phone_Numbers: Convert phone numbers to words using keypad letters
    Combined_Number: Arrange numbers to form the largest combined number
    Group_Neighbours: Group adjacent equal elements in a list
    Longest_Common_Prefix: Find longest common prefix among strings
    Yatzy: Score a Yatzy dice game (full version)
    Yatzy_Cutdown: Simplified Yatzy scoring
    Harry_Potter: Calculate discounts for Harry Potter book sets
    Vending_Machine: Calculate change for vending machine purchases
    Count_Coins: Count ways to make change with given coins
    100_doors: Toggle 100 doors puzzle (open/close pattern)
    Haiku_Review: Validate haiku syllable structure (5-7-5)
    ABC_Problem: Spell words using lettered blocks
    Align_Columns: Align text into formatted columns
    Best_Shuffle: Shuffle string so no character stays in place
    Filename_Range: Generate filename sequences (file001, file002...)
    Unsplice: Reverse a string splice operation
    12_Days_of_Xmas: Generate "12 Days of Christmas" lyrics

    <!-- Advanced -->
    Game_of_Life: Conway's cellular automaton simulation
    Mars_Rover: Navigate a rover on a grid with commands (L/R/M)
    Mine_Sweeper: Generate numbers for a minesweeper grid
    Mine_Field: Place mines randomly on a grid
    Eight_Queens: Place 8 queens on a chessboard without conflicts
    Knights_Tour: Find a path visiting all squares on a chessboard
    Reversi: Implement Reversi/Othello game logic
    Poker_Hands: Compare and rank poker hands
    Levenshtein_Distance: Calculate edit distance between strings
    Magic_Square: Generate magic squares (rows/cols/diags sum equal)
    Saddle_Points: Find saddle points in a matrix
    Tiny_Maze: Solve a small maze pathfinding problem
    Gray_Code: Generate Gray code sequences
    Monty_Hall: Simulate the Monty Hall probability problem
    Number_Chains: Find chains where numbers link by digits
    Wonderland_Number: Find 6-digit number with multiplication property
    Zeckendorf_Number: Represent as sum of non-consecutive Fibonacci numbers
    Diff_Selector: Select differences between data sets
    Diversion: Route around obstacles
    Reordering: Reorder elements based on rules
    Fizz_Buzz_Plus: Extended FizzBuzz with additional rules
  </kata_descriptions>
  <prompt>
    <message>Here are 3 katas that match your criteria:</message>
    <options>
      <option value="kata_1">
        <label>{Kata_Name_1}</label>
        <description>{Description from kata_descriptions}</description>
      </option>
      <option value="kata_2">
        <label>{Kata_Name_2}</label>
        <description>{Description from kata_descriptions}</description>
      </option>
      <option value="kata_3">
        <label>{Kata_Name_3}</label>
        <description>{Description from kata_descriptions}</description>
      </option>
      <option value="more_suggestions">
        <label>Show me 3 different ones</label>
        <description>Same criteria, different suggestions</description>
      </option>
      <option value="more_questions">
        <label>Ask me more questions...</label>
        <description>Refine my preferences further</description>
      </option>
    </options>
  </prompt>
  <if value="more_suggestions">Pick 3 different katas matching same criteria, repeat step_suggest</if>
  <if value="more_questions">
    - If user skipped from step_1: Jump to step_1 (ask difficulty)
    - If user answered step_1 but skipped step_2: Jump to step_2 (ask type)
    - If user answered both: Jump to step_1 to reconsider from start
  </if>
  <set_variable>$SELECTED_KATA = selected kata name</set_variable>
</step_suggest>

<step_fetch>
  <description>Fetch kata content from cyber-dojo</description>
  <action>
    Use WebFetch to get:
    https://raw.githubusercontent.com/cyber-dojo/exercises-start-points/master/start-points/{$SELECTED_KATA}/readme.txt
  </action>
  <store>$KATA_CONTENT = fetched readme content</store>
</step_fetch>

<step_confirm>
  <description>Show full kata details and ask for language</description>
  <display>
    Present the full kata content to the user:

    ## {$SELECTED_KATA}

    **Difficulty:** {$DIFFICULTY_STARS based on category - â­ Beginner, â­â­ Intermediate, â­â­â­ Advanced}

    ### Problem Description
    {$KATA_CONTENT from readme.txt}
  </display>
  <prompt>
    <message>What language will you use for this kata?</message>
    <options>
      <option value="typescript">
        <label>TypeScript + Vitest</label>
        <description>Modern test runner with great DX</description>
      </option>
      <option value="javascript">
        <label>JavaScript + Vitest</label>
        <description>Same great runner, no types</description>
      </option>
      <option value="python">
        <label>Python + pytest</label>
        <description>Simple and powerful testing</description>
      </option>
      <option value="back">
        <label>Show me other katas</label>
        <description>Go back to suggestions</description>
      </option>
    </options>
  </prompt>
  <note>User can also select "Other" to type a custom language/framework</note>
  <if value="back">Jump back to step_suggest</if>
  <set_variable>$LANGUAGE = selected value (or custom input)</set_variable>
</step_confirm>

<step_generate>
  <description>Generate kata files based on language</description>
  <action>Create the following files based on $LANGUAGE:</action>

  <file name="CHALLENGE.md">
# Kata: {$SELECTED_KATA}

## Difficulty
{$DIFFICULTY_STARS based on category}

## Problem
{$KATA_CONTENT from readme.txt}

## TDD Approach

Work through this kata using the red-green-refactor cycle:

1. **Red**: Write a failing test for the simplest case
2. **Green**: Write minimal code to pass
3. **Refactor**: Clean up while keeping tests green
4. **Repeat**: Add the next test case

## Source

[cyber-dojo: {$SELECTED_KATA}](https://cyber-dojo.org)
  </file>

  <file name="kata.ts" condition="$LANGUAGE == typescript">
/**
 * {$SELECTED_KATA}
 * See CHALLENGE.md for requirements
 */
export function solve(input: unknown): unknown {
  throw new Error("Not implemented - start with a failing test!");
}
  </file>

  <file name="kata.test.ts" condition="$LANGUAGE == typescript">
import { describe, it, expect } from "vitest";
import { solve } from "./kata";

describe("{$SELECTED_KATA}", () => {
  it.todo("should handle the simplest case");

  // Add your first real test here using the red-green-refactor cycle
});
  </file>

  <file name="kata.js" condition="$LANGUAGE == javascript">
/**
 * {$SELECTED_KATA}
 * See CHALLENGE.md for requirements
 */
export function solve(input) {
  throw new Error("Not implemented - start with a failing test!");
}
  </file>

  <file name="kata.test.js" condition="$LANGUAGE == javascript">
import { describe, it, expect } from "vitest";
import { solve } from "./kata.js";

describe("{$SELECTED_KATA}", () => {
  it.todo("should handle the simplest case");

  // Add your first real test here using the red-green-refactor cycle
});
  </file>

  <file name="kata.py" condition="$LANGUAGE == python">
"""
{$SELECTED_KATA}
See CHALLENGE.md for requirements
"""

def solve(input):
    raise NotImplementedError("Start with a failing test!")
  </file>

  <file name="test_kata.py" condition="$LANGUAGE == python">
import pytest
from kata import solve

class TestKata:
    def test_placeholder(self):
        """Remove this and add your first real test"""
        pytest.skip("Start with a failing test!")

    # Add your first real test here using the red-green-refactor cycle
  </file>

  <custom_language condition="$LANGUAGE is custom input (user typed via 'Other')">
    Generate appropriate boilerplate based on user's specified language:
    - CHALLENGE.md (always)
    - Implementation file with idiomatic naming and empty function
    - Test file using common test framework for that language

    Examples:
    - "Go" â†’ kata.go + kata_test.go (testing package)
    - "Rust" â†’ src/lib.rs + tests/kata_test.rs (cargo test)
    - "Java" â†’ Kata.java + KataTest.java (JUnit)
    - "C#" â†’ Kata.cs + KataTests.cs (xUnit/NUnit)
    - "Ruby" â†’ kata.rb + kata_spec.rb (RSpec)
    - "Elixir" â†’ kata.ex + kata_test.exs (ExUnit)
    - "Haskell" â†’ Kata.hs + KataSpec.hs (Hspec)
    - "Clojure" â†’ kata.clj + kata_test.clj (clojure.test)

    Use your knowledge of the language's conventions and popular test frameworks.
    Follow the same pattern: empty function that throws/raises "not implemented".
  </custom_language>

  <message>
    Kata setup complete!

    Created files:
    - CHALLENGE.md (problem description)
    {if $LANGUAGE != other}
    - Implementation file with empty solve() function
    - Test file with placeholder test
    {endif}

    Start practicing with \`/red\` to write your first failing test!
  </message>
</step_generate>

</execution_steps>

## Notes

- Kata content is fetched at runtime from cyber-dojo's GitHub repository
- The cyber-dojo project has been maintained for 10+ years with stable URLs
- All exercises are designed for TDD practice
- User can always go back to refine their preferences
- Boilerplate uses \`solve()\` as the main function - rename as needed for clarity
"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for polish.md 1`] = `
"---
description: Review and address issues in existing code - fix problems or justify skipping
argument-hint: [branch, PR#, file, or area to polish]
_hint: Fix or skip issues
_category: Workflow
_order: 36
_requested-tools:
  - Bash(git diff:*)
  - Bash(git status:*)
  - Bash(git log:*)
  - Bash(git rev-parse:*)
  - Bash(git merge-base:*)
  - Bash(git branch:*)
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






# Polish

Take another pass at existing work to address issues. Unlike \`/code-review\` which only identifies problems, \`/polish\` resolves each finding by either:

1. **Fixing** - Implement the improvement
2. **Skipping with justification** - Document why the issue can be deferred or ignored

## Phase 0: Determine Scope

Parse the argument to determine what to polish:

| Input | Action |
|-------|--------|
| No argument | Detect divergence point, review uncommitted + committed changes |
| Branch name | Changes from that branch to HEAD |
| PR number (e.g., \`123\`) | Fetch PR diff from GitHub |
| PR URL (e.g., \`github.com/.../pull/123\`) | Extract PR number and fetch diff |
| File/path | Focus on specific file(s) |



**For GitHub PRs:**

1. Try GitHub MCP first: \`mcp__github__pull_request_read\` with \`method: "get_diff"\`
2. Fall back to \`gh\` CLI: \`gh pr diff <number>\`
3. If neither works, report error and stop


**For local branches:**

1. Get current branch: \`git rev-parse --abbrev-ref HEAD\`
2. Detect divergence point (same logic as \`/code-review\`)
3. Collect changed files from diff and uncommitted changes

## Phase 1: Identify Issues

Categorize files based on these patterns:

| Category | File Patterns |
|----------|---------------|
| Frontend/UI | \`*.tsx\`, \`*.jsx\`, \`components/\`, \`pages/\`, \`views/\`, \`*.vue\` |
| Frontend/Styling | \`*.css\`, \`*.scss\`, \`*.less\`, \`styles/\`, \`*.tailwind*\`, \`*.styled.*\` |
| Backend/API | \`routes/\`, \`api/\`, \`controllers/\`, \`services/\`, \`*.controller.*\`, \`*.service.*\`, \`*.resolver.*\` |
| Backend/Data | \`migrations/\`, \`models/\`, \`prisma/\`, \`schema.*\`, \`*.model.*\`, \`*.entity.*\` |
| Tooling/Config | \`scripts/\`, \`*.config.*\`, \`package.json\`, \`tsconfig.*\`, \`vite.*\`, \`webpack.*\`, \`eslint.*\` |
| CI/CD | \`.github/\`, \`.gitlab-ci.*\`, \`Dockerfile\`, \`docker-compose.*\`, \`*.yml\` in CI paths |
| Tests | \`*.test.*\`, \`*.spec.*\`, \`__tests__/\`, \`__mocks__/\`, \`*.stories.*\` |
| Docs | \`*.md\`, \`docs/\`, \`README*\`, \`CHANGELOG*\` |


For each category, identify issues at these severity levels:

- **blocker** - Must fix before merge
- **risky** - Should fix or have strong justification
- **nit** - Nice to have, easily skippable

## Phase 2: Address Each Issue

For each identified issue, present it and then take action:

### Format

\`\`\`
### [file:line] [severity] Title

**Issue:** Description of the problem

**Action taken:**
- [ ] Fixed: [what was done]
- [ ] Skipped: [justification]
\`\`\`

### Decision Guidelines

**Fix when:**

- Security vulnerability
- Correctness bug
- Missing error handling that could crash
- Breaking API changes without migration
- Tests that don't actually test anything

**Skip with justification when:**

- Stylistic preference with no functional impact
- Optimization for unlikely hot paths
- Refactoring that would expand scope significantly
- Issue exists in code outside the change scope
- Technical debt documented for future sprint

### Fixing Issues

When fixing:

1. Make the minimal change to address the issue
2. Ensure tests still pass (run them if needed)
3. Don't expand scope beyond the identified issue

### Watch for Brittle Tests

When refactoring implementation, watch for **Peeping Tom** tests that:

- Test private methods or internal state directly
- Assert on implementation details rather than behavior
- Break on any refactoring even when behavior is preserved

If tests fail after a pure refactoring (no behavior change), consider whether the tests are testing implementation rather than behavior.


### Skipping Issues

Valid skip justifications:

- "Out of scope - exists in unchanged code"
- "Performance optimization unnecessary - called N times per request"
- "Tracked for future work - see issue #X"
- "Intentional design decision - [reason]"
- "Would require significant refactoring - defer to dedicated PR"

Invalid skip justifications:

- "Too hard to fix"
- "It works fine"
- "No time"

## Phase 3: Cross-Cutting Check

After addressing individual issues:

8. **Consistency check** - Look for inconsistent patterns, naming conventions, or structure across the codebase


Additional cross-cutting checks:

- Did fixes introduce new inconsistencies?
- Are skip justifications consistent with each other?
- Any patterns in what was skipped that suggest a bigger issue?

## Phase 4: Summary

\`\`\`
## Polish Summary

### Fixed
- [list of fixes applied]

### Skipped (with justification)
- [issue]: [justification]

### Tests
- [ ] All tests passing
- [ ] No new warnings introduced

### Remaining Work
- [any follow-up items identified]
\`\`\`

---

**User arguments:**

Polish: $ARGUMENTS

**End of user arguments**
"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for pr.md 1`] = `
"---
description: Creates a pull request using GitHub MCP
argument-hint: [optional-pr-title-and-description]
_hint: Create PR
_category: Workflow
_order: 5
---

# Create Pull Request

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






Create a pull request for the current branch using GitHub MCP tools.

## Workflow

Current branch status:
!\`git status\`

Recent commits:
!\`git log --oneline -5\`

**User arguments:**

PR: $ARGUMENTS

**End of user arguments**

**Process:**

1. **Ensure Branch is Ready**:
   !\`git status\`
   - Commit all changes
   - Push to remote: \`git push origin [branch-name]\`

2. **Create PR**: Create a well-formatted pull request

   Title: conventional commits format, like \`feat(#123): add user authentication\`

   Description template:

   \`\`\`markdown
   <!--
     Are there any relevant issues / PRs / mailing lists discussions?
     Please reference them here.
   -->

   ## References

   - [links to github issues referenced in commit messages]

   ## Summary

   [Brief description of changes]

   ## Test Plan

   - [ ] Tests pass
   - [ ] Manual testing completed
   \`\`\`

3. **Set Base Branch**: Default to main unless specified otherwise

4. **Link Issues**: Reference related issues found in commit messages

## Use GitHub MCP Tools

1. Check current branch and ensure it's pushed
2. Create a well-formatted pull request with proper title and description
3. Set the base branch (default: main)
4. Include relevant issue references if found in commit messages


"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for red.md 1`] = `
"---
description: Execute TDD Red Phase - write ONE failing test
argument-hint: [optional additional info]
_hint: Failing test
_category: Test-Driven Development
_order: 2
---

**User arguments:**

Red: $ARGUMENTS

**End of user arguments**

RED PHASE! Apply the below to the user input above.

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






(If there was no info above, fallback to the context of the conversation)


## TDD Fundamentals

### The TDD Cycle

The foundation of TDD is the Red-Green-Refactor cycle:

1. **Red Phase**: Write ONE failing test that describes desired behavior

   - The test must fail for the RIGHT reason (not syntax/import errors)
   - Only one test at a time - this is critical for TDD discipline
     - Exception: For browser-level tests or expensive setup (e.g., Storybook \`*.stories.tsx\`), group multiple assertions within a single test block to avoid redundant setup - but only when adding assertions to an existing interaction flow. If new user interactions are required, still create a new test. Split files by category if they exceed ~1000 lines.
   - **Adding a single test to a test file is ALWAYS allowed** - no prior test output needed
   - Starting TDD for a new feature is always valid, even if test output shows unrelated work
   - For DOM-based tests, use \`data-testid\` attributes to select elements rather than CSS classes, tag names, or text content
   - Avoid hard-coded timeouts both in form of sleep() or timeout: 5000 etc; use proper async patterns (\`waitFor\`, \`findBy*\`, event-based sync) instead and rely on global test configs for timeout settings

2. **Green Phase**: Write MINIMAL code to make the test pass

   - Implement only what's needed for the current failing test
   - No anticipatory coding or extra features
   - Address the specific failure message

3. **Refactor Phase**: Improve code structure while keeping tests green
   - Only allowed when relevant tests are passing
   - Requires proof that tests have been run and are green
   - Applies to BOTH implementation and test code
   - No refactoring with failing tests - fix them first

### Core Violations

1. **Multiple Test Addition**

   - Adding more than one new test at once
   - Exception: Initial test file setup or extracting shared test utilities

2. **Over-Implementation**

   - Code that exceeds what's needed to pass the current failing test
   - Adding untested features, methods, or error handling
   - Implementing multiple methods when test only requires one

3. **Premature Implementation**
   - Adding implementation before a test exists and fails properly
   - Adding implementation without running the test first
   - Refactoring when tests haven't been run or are failing

### Critical Principle: Incremental Development

Each step in TDD should address ONE specific issue:

- Test fails "not defined" â†’ Create empty stub/class only
- Test fails "not a function" â†’ Add method stub only
- Test fails with assertion â†’ Implement minimal logic only

### Optional Pre-Phase: Spike Phase

In rare cases where the problem space, interface, or expected behavior is unclear, a **Spike Phase** may be used **before the Red Phase**.
This phase is **not part of the regular TDD workflow** and must only be applied under exceptional circumstances.

- The goal of a Spike is **exploration and learning**, not implementation.
- The code written during a Spike is **disposable** and **must not** be merged or reused directly.
- Once sufficient understanding is achieved, all spike code is discarded, and normal TDD resumes starting from the **Red Phase**.
- A Spike is justified only when it is impossible to define a meaningful failing test due to technical uncertainty or unknown system behavior.

### General Information

- Sometimes the test output shows as no tests have been run when a new test is failing due to a missing import or constructor. In such cases, allow the agent to create simple stubs. Ask them if they forgot to create a stub if they are stuck.
- It is never allowed to introduce new logic without evidence of relevant failing tests. However, stubs and simple implementation to make imports and test infrastructure work is fine.
- In the refactor phase, it is perfectly fine to refactor both test and implementation code. That said, completely new functionality is not allowed. Types, clean up, abstractions, and helpers are allowed as long as they do not introduce new behavior.
- Adding types, interfaces, or a constant in order to replace magic values is perfectly fine during refactoring.
- Provide the agent with helpful directions so that they do not get stuck when blocking them.


### Test Structure (AAA Pattern)

Structure each test with clear phases:

- **Arrange**: Set up test data and preconditions (keep minimal)
- **Act**: Execute the single action being tested
- **Assert**: Verify the expected outcome with specific assertions

"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for refactor.md 1`] = `
"---
description: Execute TDD Refactor Phase - improve code structure while keeping tests green
argument-hint: <refactoring description>
_hint: Clean up code
_category: Test-Driven Development
_order: 4
---

**User arguments:**

Refactor: $ARGUMENTS

**End of user arguments**

Apply this document (specifically the Refactor phase) to the user input above.

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






(If there was no info above, fallback to the context of the conversation)


## TDD Fundamentals

### The TDD Cycle

The foundation of TDD is the Red-Green-Refactor cycle:

1. **Red Phase**: Write ONE failing test that describes desired behavior

   - The test must fail for the RIGHT reason (not syntax/import errors)
   - Only one test at a time - this is critical for TDD discipline
     - Exception: For browser-level tests or expensive setup (e.g., Storybook \`*.stories.tsx\`), group multiple assertions within a single test block to avoid redundant setup - but only when adding assertions to an existing interaction flow. If new user interactions are required, still create a new test. Split files by category if they exceed ~1000 lines.
   - **Adding a single test to a test file is ALWAYS allowed** - no prior test output needed
   - Starting TDD for a new feature is always valid, even if test output shows unrelated work
   - For DOM-based tests, use \`data-testid\` attributes to select elements rather than CSS classes, tag names, or text content
   - Avoid hard-coded timeouts both in form of sleep() or timeout: 5000 etc; use proper async patterns (\`waitFor\`, \`findBy*\`, event-based sync) instead and rely on global test configs for timeout settings

2. **Green Phase**: Write MINIMAL code to make the test pass

   - Implement only what's needed for the current failing test
   - No anticipatory coding or extra features
   - Address the specific failure message

3. **Refactor Phase**: Improve code structure while keeping tests green
   - Only allowed when relevant tests are passing
   - Requires proof that tests have been run and are green
   - Applies to BOTH implementation and test code
   - No refactoring with failing tests - fix them first

### Core Violations

1. **Multiple Test Addition**

   - Adding more than one new test at once
   - Exception: Initial test file setup or extracting shared test utilities

2. **Over-Implementation**

   - Code that exceeds what's needed to pass the current failing test
   - Adding untested features, methods, or error handling
   - Implementing multiple methods when test only requires one

3. **Premature Implementation**
   - Adding implementation before a test exists and fails properly
   - Adding implementation without running the test first
   - Refactoring when tests haven't been run or are failing

### Critical Principle: Incremental Development

Each step in TDD should address ONE specific issue:

- Test fails "not defined" â†’ Create empty stub/class only
- Test fails "not a function" â†’ Add method stub only
- Test fails with assertion â†’ Implement minimal logic only

### Optional Pre-Phase: Spike Phase

In rare cases where the problem space, interface, or expected behavior is unclear, a **Spike Phase** may be used **before the Red Phase**.
This phase is **not part of the regular TDD workflow** and must only be applied under exceptional circumstances.

- The goal of a Spike is **exploration and learning**, not implementation.
- The code written during a Spike is **disposable** and **must not** be merged or reused directly.
- Once sufficient understanding is achieved, all spike code is discarded, and normal TDD resumes starting from the **Red Phase**.
- A Spike is justified only when it is impossible to define a meaningful failing test due to technical uncertainty or unknown system behavior.

### General Information

- Sometimes the test output shows as no tests have been run when a new test is failing due to a missing import or constructor. In such cases, allow the agent to create simple stubs. Ask them if they forgot to create a stub if they are stuck.
- It is never allowed to introduce new logic without evidence of relevant failing tests. However, stubs and simple implementation to make imports and test infrastructure work is fine.
- In the refactor phase, it is perfectly fine to refactor both test and implementation code. That said, completely new functionality is not allowed. Types, clean up, abstractions, and helpers are allowed as long as they do not introduce new behavior.
- Adding types, interfaces, or a constant in order to replace magic values is perfectly fine during refactoring.
- Provide the agent with helpful directions so that they do not get stuck when blocking them.


## Code Complexity Signals

Look for these refactoring opportunities:

- [ ] Nesting > 3 levels deep
- [ ] Functions > 20 lines
- [ ] Duplicate code blocks
- [ ] Abstractions with single implementation
- [ ] "Just in case" parameters or config
- [ ] Magic values without names
- [ ] Dead/unused code


### Watch for Brittle Tests

When refactoring implementation, watch for **Peeping Tom** tests that:

- Test private methods or internal state directly
- Assert on implementation details rather than behavior
- Break on any refactoring even when behavior is preserved

If tests fail after a pure refactoring (no behavior change), consider whether the tests are testing implementation rather than behavior.


8. **Consistency check** - Look for inconsistent patterns, naming conventions, or structure across the codebase

"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for research.md 1`] = `
"---
description: Research a problem in parallel via web docs, web search, codebase exploration, and deep ultrathink
argument-hint: <research topic or question>
_hint: Deep research
_category: Utilities
_order: 20
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product




**User arguments:**

Research: $ARGUMENTS

**End of user arguments**

Research the following problem or question thoroughly, like a senior developer would.

## Step 1: Launch Parallel Research Agents

Use the Task tool to spawn these subagents **in parallel** (all in a single message):

1. **Web Documentation Agent** (subagent_type: general-purpose)
   - Search official documentation for the topic
   - Find best practices and recommended patterns
   - Locate relevant GitHub issues or discussions

2. **Web Search Agent** (subagent_type: general-purpose)
   - Perform broad web searches for solutions and discussions
   - Find Stack Overflow answers, blog posts, and tutorials
   - Note common pitfalls and gotchas

3. **Codebase Explorer Agent** (subagent_type: Explore)
   - Search the codebase for related patterns
   - Find existing solutions to similar problems
   - Identify relevant files, functions, or components

## Step 2: Library Documentation (Optional)

If the research involves specific frameworks or libraries:
- Use Context7 MCP tools (mcp__context7__resolve-library-id, then get-library-docs)
- Get up-to-date API references and code examples
- If Context7 is unavailable, note this in findings so user knows library docs were harder to obtain

## Step 3: Deep Analysis

With all gathered context, perform extended reasoning (ultrathink) to:
- Analyze the problem from first principles
- Consider edge cases and trade-offs
- Synthesize insights across all sources
- Identify conflicts between sources

## Step 4: Present Findings

Present a structured summary to the user:

### Problem Statement
Describe the problem and why it matters.

### Key Findings
Summarize the most relevant solutions and approaches.

### Codebase Patterns
Document how the current codebase handles similar cases.

### Recommended Approach
Provide your recommendation based on all research.

### Conflicts
Highlight where sources disagree and provide assessment of which is more reliable.

### Sources
List all source links with brief descriptions. This section is required.

## Research Guidelines

- Prioritize official documentation over blog posts
- Prefer solutions that match existing codebase patterns
- Note major.minor versions for libraries/frameworks (patch versions only if critical)
- Flag conflicting information across sources
- Write concise, actionable content
- Use active voice throughout
- **Do not create output files** - present findings directly in conversation unless user explicitly requests a file
"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for ship.md 1`] = `
"---
description: Ship code directly to main - for small, obvious changes that don't need review
argument-hint: [optional-commit-message]
_hint: Direct to main
_category: Ship / Show / Ask
_order: 1
_selectedByDefault: false
---

# Ship - Direct Merge to Main

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






**Ship/Show/Ask Pattern - SHIP**

Ship is for small, obvious changes that don't need code review. Examples:

- Typo fixes
- Formatting changes
- Documentation updates
- Obvious bug fixes
- Dependency updates with passing tests

## Prerequisites

Before shipping directly to main:

1. All tests must pass
2. Linter must pass
3. Changes must be small and low-risk
4. CI must be green (if configured)

## Workflow

Current branch status:
!\`git status\`

Recent commits:
!\`git log --oneline -5\`

**User arguments:**

Ship: $ARGUMENTS

**End of user arguments**

**Process:**

1. **Verify Change Size**: Check git diff to ensure changes are small and focused
   !\`git diff --stat main\`

2. **Run Tests**: Ensure all tests pass
   !\`npm test\` or !\`yarn test\` or appropriate test command for the project

3. **Run Linter**: Ensure code quality checks pass
   !\`npm run lint\` or !\`yarn lint\` or appropriate lint command (if available)

4. **Safety Check**: Confirm with user that this is truly a ship-worthy change:
   - Is this a small, obvious change?
   - Do all tests pass?
   - Is CI green?

   If ANY of these are "no", suggest using \`/show\` or \`/ask\` instead.

5. **Merge to Main**: If all checks pass and user confirms:

   \`\`\`bash
   git checkout main
   git pull origin main
   git merge --ff-only [feature-branch] || git merge [feature-branch]
   git push origin main
   \`\`\`

6. **Cleanup**: Delete the feature branch

   \`\`\`bash
   git branch -d [feature-branch]
   git push origin --delete [feature-branch]
   \`\`\`

7. **Notify**: Show summary of what was shipped

## Safety Rails

If tests fail, linter fails, or changes are large/complex, STOP and suggest:

- Use \`/show\` for changes that should be seen but don't need approval
- Use \`/ask\` (traditional PR) for complex changes needing discussion


"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for show.md 1`] = `
"---
description: Show code to team with auto-merge - for changes that should be visible but don't need approval
argument-hint: [optional-pr-title-and-description]
_hint: Auto-merge PR
_category: Ship / Show / Ask
_order: 2
_selectedByDefault: false
---

# Show - Visible Merge with Optional Feedback

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






**Ship/Show/Ask Pattern - SHOW**

Show is for changes that teammates should see, but don't require approval. Examples:

- Refactoring with test coverage
- New features with comprehensive tests
- Performance improvements
- Non-breaking API changes

## Prerequisites

Before using show:

1. All tests must pass
2. Changes should have good test coverage
3. Changes should be non-breaking or backward compatible
4. CI must be green

## Workflow

Current branch status:
!\`git status\`

Recent commits:
!\`git log --oneline -5\`

**User arguments:**

Show: $ARGUMENTS

**End of user arguments**

**Process:**

1. **Verify Quality**: Check that changes meet show criteria
   - Run tests: !\`npm test\` or !\`yarn test\` or appropriate test command
   - Check coverage is maintained or improved
   - Verify no breaking changes

2. **Create Show PR**: Create a PR that will auto-merge after a short window
   - Title: conventional commits format, prefixed with \`[SHOW]\`
   - Description: Clear explanation of what changed and why
   - Add label: "show" or "auto-merge"
   - Set auto-merge if GitHub setting allows

3. **Configure Auto-Merge**:
   - If GitHub Actions is configured, set to auto-merge after CI passes
   - If not, provide instructions to merge after 1-2 hours of visibility
   - Add notice that feedback is welcome but not required

4. **PR Description Template**:

   \`\`\`markdown
   ## ðŸš€ Show - Auto-merging after CI

   **This is a SHOW PR**: Changes are ready to merge but shared for visibility.
   Feedback welcome but not required. Will auto-merge when CI passes.

   <!--
     References: [link to relevant issues]
   -->

   ### What changed

   [Brief description]

   ### Why

   [Rationale for change]

   ### Test coverage

   - [ ] All tests pass
   - [ ] Coverage maintained/improved
   - [ ] No breaking changes
   \`\`\`

5. **Monitoring**: Check PR status and auto-merge when ready

## Decision Guide

Use **Show** when:

- âœ… Tests are comprehensive
- âœ… Changes are non-breaking
- âœ… You're confident in the approach
- âœ… Team should be aware of the change

Use **/ship** instead if: change is tiny and obvious (typo, formatting)

Use **/ask** instead if: change needs discussion, breaks APIs, or you're uncertain


"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for simplify.md 1`] = `
"---
description: Reduce code complexity while keeping tests green
argument-hint: [file, function, or area to simplify]
_hint: Reduce complexity
_category: Test-Driven Development
_order: 16
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product




**User arguments:**

Simplify: $ARGUMENTS

**End of user arguments**

(If there was no info above, fallback to the context of the conversation)


Reduce complexity while keeping tests green.

## Core Principles

**YAGNI** - Don't build until actually needed. Delete "just in case" code.

**KISS** - Simplest solution that works. Clever is the enemy of clear.

**Rule of Three** - Don't abstract until 3rd occurrence. "Prefer duplication over wrong abstraction" (Sandi Metz).

## When NOT to Simplify

- Essential domain complexity (regulations, business rules)
- Performance-critical optimized code
- Concurrency/thread-safety requirements
- Security-sensitive explicit checks

## Prerequisites

Tests must be green. If failing, use \`/green\` first.

## Code Complexity Signals

Look for these refactoring opportunities:

- [ ] Nesting > 3 levels deep
- [ ] Functions > 20 lines
- [ ] Duplicate code blocks
- [ ] Abstractions with single implementation
- [ ] "Just in case" parameters or config
- [ ] Magic values without names
- [ ] Dead/unused code


## Techniques

| Pattern | Before | After |
|---------|--------|-------|
| Guard clause | Nested \`if/else\` | Early \`return\` |
| Named condition | Complex boolean | \`const isValid = ...\` |
| Extract constant | \`if (x > 3)\` | \`if (x > MAX_RETRIES)\` |
| Flatten callback | \`.then().then()\` | \`async/await\` |

**Also apply:** Consolidate duplicates, inline unnecessary abstractions, delete dead code.

## Validate

1. Tests still green
2. Code reads more clearly
3. No behavioral changes

**Simplify** removes complexity locally. **Refactor** improves architecture broadly. Use \`/refactor\` if changes require structural reorganization.

### Watch for Brittle Tests

When refactoring implementation, watch for **Peeping Tom** tests that:

- Test private methods or internal state directly
- Assert on implementation details rather than behavior
- Break on any refactoring even when behavior is preserved

If tests fail after a pure refactoring (no behavior change), consider whether the tests are testing implementation rather than behavior.

"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for spike.md 1`] = `
"---
description: Execute TDD Spike Phase - exploratory coding to understand problem space before TDD
argument-hint: <exploration description>
_hint: Explore first
_category: Test-Driven Development
_order: 1
---

**User arguments:**

Spike: $ARGUMENTS

**End of user arguments**

SPIKE PHASE! Apply the below to the user input above.

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






(If there was no info above, fallback to the context of the conversation)


## TDD Fundamentals

### The TDD Cycle

The foundation of TDD is the Red-Green-Refactor cycle:

1. **Red Phase**: Write ONE failing test that describes desired behavior

   - The test must fail for the RIGHT reason (not syntax/import errors)
   - Only one test at a time - this is critical for TDD discipline
     - Exception: For browser-level tests or expensive setup (e.g., Storybook \`*.stories.tsx\`), group multiple assertions within a single test block to avoid redundant setup - but only when adding assertions to an existing interaction flow. If new user interactions are required, still create a new test. Split files by category if they exceed ~1000 lines.
   - **Adding a single test to a test file is ALWAYS allowed** - no prior test output needed
   - Starting TDD for a new feature is always valid, even if test output shows unrelated work
   - For DOM-based tests, use \`data-testid\` attributes to select elements rather than CSS classes, tag names, or text content
   - Avoid hard-coded timeouts both in form of sleep() or timeout: 5000 etc; use proper async patterns (\`waitFor\`, \`findBy*\`, event-based sync) instead and rely on global test configs for timeout settings

2. **Green Phase**: Write MINIMAL code to make the test pass

   - Implement only what's needed for the current failing test
   - No anticipatory coding or extra features
   - Address the specific failure message

3. **Refactor Phase**: Improve code structure while keeping tests green
   - Only allowed when relevant tests are passing
   - Requires proof that tests have been run and are green
   - Applies to BOTH implementation and test code
   - No refactoring with failing tests - fix them first

### Core Violations

1. **Multiple Test Addition**

   - Adding more than one new test at once
   - Exception: Initial test file setup or extracting shared test utilities

2. **Over-Implementation**

   - Code that exceeds what's needed to pass the current failing test
   - Adding untested features, methods, or error handling
   - Implementing multiple methods when test only requires one

3. **Premature Implementation**
   - Adding implementation before a test exists and fails properly
   - Adding implementation without running the test first
   - Refactoring when tests haven't been run or are failing

### Critical Principle: Incremental Development

Each step in TDD should address ONE specific issue:

- Test fails "not defined" â†’ Create empty stub/class only
- Test fails "not a function" â†’ Add method stub only
- Test fails with assertion â†’ Implement minimal logic only

### Optional Pre-Phase: Spike Phase

In rare cases where the problem space, interface, or expected behavior is unclear, a **Spike Phase** may be used **before the Red Phase**.
This phase is **not part of the regular TDD workflow** and must only be applied under exceptional circumstances.

- The goal of a Spike is **exploration and learning**, not implementation.
- The code written during a Spike is **disposable** and **must not** be merged or reused directly.
- Once sufficient understanding is achieved, all spike code is discarded, and normal TDD resumes starting from the **Red Phase**.
- A Spike is justified only when it is impossible to define a meaningful failing test due to technical uncertainty or unknown system behavior.

### General Information

- Sometimes the test output shows as no tests have been run when a new test is failing due to a missing import or constructor. In such cases, allow the agent to create simple stubs. Ask them if they forgot to create a stub if they are stuck.
- It is never allowed to introduce new logic without evidence of relevant failing tests. However, stubs and simple implementation to make imports and test infrastructure work is fine.
- In the refactor phase, it is perfectly fine to refactor both test and implementation code. That said, completely new functionality is not allowed. Types, clean up, abstractions, and helpers are allowed as long as they do not introduce new behavior.
- Adding types, interfaces, or a constant in order to replace magic values is perfectly fine during refactoring.
- Provide the agent with helpful directions so that they do not get stuck when blocking them.

"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for summarize.md 1`] = `
"---
description: Summarize conversation progress and next steps
argument-hint: [optional additional info]
_hint: Summarize chat
_category: Workflow
_order: 10
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






Create a concise summary of the current conversation suitable for transferring context to a new conversation.

**User arguments:**

Summarize: $ARGUMENTS

**End of user arguments**

## Summary Structure

Provide a summary with these sections:

### What We Did
- Key accomplishments and changes made
- Important decisions or discoveries
- Files created, modified, or analyzed

### What We're Doing Next
- Immediate next steps
- Pending tasks or work in progress
- Goals or objectives to continue

### Blockers & User Input Needed
- Any issues requiring user intervention
- Decisions that need to be made
- Missing information or clarifications needed

## Output Format

Keep the summary concise and actionable - suitable for pasting into a new conversation to quickly restore context without needing the full conversation history.



"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for tdd.md 1`] = `
"---
description: Remind agent about TDD approach and continue conversation
argument-hint: [optional-response-to-last-message]
_hint: TDD reminder
_category: Test-Driven Development
_order: 1
---

# TDD Reminder

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






## TDD Fundamentals

### The TDD Cycle

The foundation of TDD is the Red-Green-Refactor cycle:

1. **Red Phase**: Write ONE failing test that describes desired behavior

   - The test must fail for the RIGHT reason (not syntax/import errors)
   - Only one test at a time - this is critical for TDD discipline
     - Exception: For browser-level tests or expensive setup (e.g., Storybook \`*.stories.tsx\`), group multiple assertions within a single test block to avoid redundant setup - but only when adding assertions to an existing interaction flow. If new user interactions are required, still create a new test. Split files by category if they exceed ~1000 lines.
   - **Adding a single test to a test file is ALWAYS allowed** - no prior test output needed
   - Starting TDD for a new feature is always valid, even if test output shows unrelated work
   - For DOM-based tests, use \`data-testid\` attributes to select elements rather than CSS classes, tag names, or text content
   - Avoid hard-coded timeouts both in form of sleep() or timeout: 5000 etc; use proper async patterns (\`waitFor\`, \`findBy*\`, event-based sync) instead and rely on global test configs for timeout settings

2. **Green Phase**: Write MINIMAL code to make the test pass

   - Implement only what's needed for the current failing test
   - No anticipatory coding or extra features
   - Address the specific failure message

3. **Refactor Phase**: Improve code structure while keeping tests green
   - Only allowed when relevant tests are passing
   - Requires proof that tests have been run and are green
   - Applies to BOTH implementation and test code
   - No refactoring with failing tests - fix them first

### Core Violations

1. **Multiple Test Addition**

   - Adding more than one new test at once
   - Exception: Initial test file setup or extracting shared test utilities

2. **Over-Implementation**

   - Code that exceeds what's needed to pass the current failing test
   - Adding untested features, methods, or error handling
   - Implementing multiple methods when test only requires one

3. **Premature Implementation**
   - Adding implementation before a test exists and fails properly
   - Adding implementation without running the test first
   - Refactoring when tests haven't been run or are failing

### Critical Principle: Incremental Development

Each step in TDD should address ONE specific issue:

- Test fails "not defined" â†’ Create empty stub/class only
- Test fails "not a function" â†’ Add method stub only
- Test fails with assertion â†’ Implement minimal logic only

### Optional Pre-Phase: Spike Phase

In rare cases where the problem space, interface, or expected behavior is unclear, a **Spike Phase** may be used **before the Red Phase**.
This phase is **not part of the regular TDD workflow** and must only be applied under exceptional circumstances.

- The goal of a Spike is **exploration and learning**, not implementation.
- The code written during a Spike is **disposable** and **must not** be merged or reused directly.
- Once sufficient understanding is achieved, all spike code is discarded, and normal TDD resumes starting from the **Red Phase**.
- A Spike is justified only when it is impossible to define a meaningful failing test due to technical uncertainty or unknown system behavior.

### General Information

- Sometimes the test output shows as no tests have been run when a new test is failing due to a missing import or constructor. In such cases, allow the agent to create simple stubs. Ask them if they forgot to create a stub if they are stuck.
- It is never allowed to introduce new logic without evidence of relevant failing tests. However, stubs and simple implementation to make imports and test infrastructure work is fine.
- In the refactor phase, it is perfectly fine to refactor both test and implementation code. That said, completely new functionality is not allowed. Types, clean up, abstractions, and helpers are allowed as long as they do not introduce new behavior.
- Adding types, interfaces, or a constant in order to replace magic values is perfectly fine during refactoring.
- Provide the agent with helpful directions so that they do not get stuck when blocking them.


## Continue Conversation

**User arguments:**

TDD: $ARGUMENTS

**End of user arguments**

Please continue with the user input above, applying TDD approach.
"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for tdd-review.md 1`] = `
"---
description: Review test suite quality against FIRST principles and TDD anti-patterns
argument-hint: [optional test file or directory path]
_hint: Review tests
_category: Test-Driven Development
_order: 45
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






(If there was no info above, fallback to the context of the conversation)


# Test Quality Review

Analyze test files against FIRST principles and TDD best practices.

## Phase 1: Scope

| Input | Action |
|-------|--------|
| No argument | Find all test files in project |
| File path | Analyze specific test file |
| Directory | Analyze tests in directory |

Detect test files using common patterns: \`*.test.*\`, \`*.spec.*\`, \`*.stories.*\`, \`__tests__/**\`

Also check for framework-specific patterns based on the project's languages and tools (e.g., \`*_test.go\`, \`*_test.py\`, \`Test*.java\`, \`*.feature\` for BDD).

## Phase 2: Analysis

For each test file, check against these criteria:

### Quality Criteria

#### FIRST Principles

| Principle | What to Check |
|-----------|---------------|
| **Fast** | Tests complete quickly, no I/O, no network calls, no sleep()/setTimeout delays |
| **Independent** | No shared mutable state, no execution order dependencies between tests |
| **Repeatable** | No Date.now(), no Math.random() without seeding, no external service dependencies |
| **Self-validating** | Meaningful assertions that verify behavior, no manual verification needed |

#### TDD Anti-patterns

| Anti-pattern | Detection Signals |
|--------------|-------------------|
| **The Liar** | \`expect(true).toBe(true)\`, empty test bodies, tests with no assertions |
| **Excessive Setup** | >20 lines of arrange code, >5 mocks, deep nested object construction |
| **The One** | >5 assertions testing unrelated behaviors in a single test |
| **The Peeping Tom** | Testing private methods, asserting on internal state, tests that break on any refactor |
| **The Slow Poke** | Real database/network calls, file I/O, hard-coded timeouts |

#### Test Structure (AAA Pattern)

- **Arrange**: Clear setup with minimal fixtures
- **Act**: Single action being tested
- **Assert**: Specific, behavior-focused assertions


## Phase 3: Report

Output a structured report:

\`\`\`
## Test Quality Report

### Summary
- Files analyzed: N
- Tests found: N
- Issues found: N (X blockers, Y warnings)

### By File

#### path/to/file.test.ts

| Line | Issue | Severity | Description |
|------|-------|----------|-------------|
| 15 | The Liar | blocker | Test has no assertions |
| 42 | Slow Poke | warning | Uses setTimeout(500) |

### Recommendations
- [ ] Fix blockers before merge
- [ ] Consider refactoring tests with excessive setup
\`\`\`

### Severity Levels

- **blocker**: Must fix - test provides false confidence (The Liar, no assertions)
- **warning**: Should fix - test quality issue (Slow Poke, Excessive Setup)
- **info**: Consider - style or structure suggestion (AAA pattern)

---

**User arguments:**

TDD-review: $ARGUMENTS

**End of user arguments**
"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for upgrade-deps.md 1`] = `
"---
description: Check for dependency upgrades and assess safety before updating
argument-hint: (no arguments - interactive)
_hint: Upgrade packages
_category: Utilities
_order: 50
---

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






# Upgrade Dependencies

Analyze available dependency upgrades, assess codebase safety signals, and guide the user through upgrade decisions.

## Phase 1: Detect Environment

Scan for package manager and project setup:

| Check | How |
|-------|-----|
| Package manager | Look for \`pnpm-lock.yaml\`, \`yarn.lock\`, \`package-lock.json\`, \`bun.lockb\` |
| Lockfile present | Exists and tracked in git |
| Test script | \`package.json\` has \`test\` script |
| Test files | \`*.test.*\`, \`*.spec.*\`, \`__tests__/\` exist |
| CI config | \`.github/workflows/\`, \`.gitlab-ci.yml\`, etc. |
| Type checking | \`tsconfig.json\` or \`jsconfig.json\` present |

Output:

\`\`\`
## Environment

Package manager: pnpm
Lockfile: âœ“ pnpm-lock.yaml (tracked)
Test script: âœ“ "vitest run"
Test files: âœ“ 16 test files found
CI config: âœ“ .github/workflows/release.yml
Types: âœ“ tsconfig.json
\`\`\`

## Phase 2: List Available Upgrades

Run the appropriate outdated command:

| Manager | Command |
|---------|---------|
| pnpm | \`pnpm outdated --format json\` |
| npm | \`npm outdated --json\` |
| yarn | \`yarn outdated --json\` |
| bun | \`bun outdated\` |

Group results by semver level:

\`\`\`
## Available Upgrades

### Patch (low risk)
| Package | Current | Latest | Age |
|---------|---------|--------|-----|
| lodash | 4.17.20 | 4.17.21 | 6 months |

### Minor (medium risk)
| Package | Current | Latest | Age |
|---------|---------|--------|-----|
| vitest | 1.5.0 | 1.6.0 | 3 weeks |

### Major (review changelog)
| Package | Current | Latest | Age |
|---------|---------|--------|-----|
| typescript | 4.9.5 | 5.4.2 | 8 months |
\`\`\`

Note: "Age" is time since the latest version was published. Packages < 14 days old may pose supply chain risk.

## Phase 3: Safety Assessment

Present observable signals (not guarantees):

\`\`\`
## Safety Signals

âœ“ Test files present (16 files)
âœ“ Test script configured
âœ“ CI pipeline exists
âœ“ Lockfile tracked in git
âœ“ TypeScript for type safety
âš  No pre-commit hooks detected
âœ— Coverage threshold not configured

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âš  DISCLAIMER: This is a surface-level check based
on file presence, not actual test quality or
coverage. You know your codebase best.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
\`\`\`

## Phase 4: Ask User

Use \`AskUserQuestion\` to determine next steps:

**Question: "How would you like to proceed?"** (header: "Upgrade")

Options:
1. **Patches only** - "Upgrade patch versions (X.Y.Z â†’ X.Y.Z+1), lowest risk"
2. **Patches + minors** - "Include minor versions (X.Y â†’ X.Y+1), some API additions"
3. **Interactive selection** - "Choose specific packages to upgrade"
4. **Doctor mode** - "Test each upgrade individually, keep only working ones (slower but safest)"
5. **Just show outdated** - "Don't upgrade, just list what's available"

## Phase 5: Execute Upgrade

Based on user selection:

### Patches Only
\`\`\`bash
# pnpm
pnpm update --no-save  # updates lockfile only for patches within range

# or with ncu
npx npm-check-updates --target patch -u && pnpm install
\`\`\`

### Patches + Minors
\`\`\`bash
npx npm-check-updates --target minor -u && pnpm install
\`\`\`

### Interactive Selection
\`\`\`bash
npx npm-check-updates -i
\`\`\`

### Doctor Mode
\`\`\`bash
npx npm-check-updates --doctor -u
\`\`\`

This will:
1. Verify tests pass before starting
2. Try upgrading all dependencies
3. If tests fail, test each dependency individually
4. Keep only upgrades that pass tests

### After Any Upgrade

1. Run tests: \`pnpm test\`
2. Run type check if available: \`pnpm typecheck\`
3. Run build if available: \`pnpm build\`
4. Report results to user

## Phase 6: Summary

\`\`\`
## Upgrade Summary

### Upgraded
- lodash: 4.17.20 â†’ 4.17.21 (patch)
- vitest: 1.5.0 â†’ 1.6.0 (minor)

### Skipped
- typescript: 4.9.5 â†’ 5.4.2 (major - user opted out)

### Verification
- [x] Tests passing
- [x] Types checking
- [x] Build successful

### Next Steps
- Review changelog for skipped major upgrades
- Consider running full test suite in CI before merging
\`\`\`

---

**User arguments:**

Upgrade: $ARGUMENTS

**End of user arguments**
"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for worktree-add.md 1`] = `
"---
description: Add a new git worktree from branch name or issue URL, copy settings, install deps, and open in current IDE
argument-hint: <branch-name-or-issue-url> [optional-base-branch]
_hint: Add worktree
_category: Worktree Management
_order: 1
---

# Git Worktree Setup

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






**User arguments:**

Worktree-add: $ARGUMENTS

**End of user arguments**

<current_state>
Current branch: \`git branch --show-current\`
Current worktrees: \`git worktree list\`
Remote branches: \`git branch -r\`
Uncommitted changes: \`git status --short\`
</current_state>

<execution_steps>
<step_0>
  <description>Ask user for setup mode</description>
  <prompt>
    <message>How would you like to set up the worktree?</message>
    <options>
      <option value="quick">
        <label>Quick</label>
        <description>Just create the worktree (skip deps, settings, IDE)</description>
      </option>
      <option value="full">
        <label>Full setup</label>
        <description>Install dependencies, copy settings, open in IDE</description>
      </option>
    </options>
  </prompt>
  <set_variable>$SETUP_MODE = user selection ("quick" or "full")</set_variable>
  <purpose>Allow quick worktree creation when user just needs the branch</purpose>
</step_0>

<step_0b>
  <description>Detect git hosting provider and available tools (only needed if argument is an issue URL)</description>
  <condition>Only run this step if first argument looks like a git hosting URL</condition>


<detect_provider>
  <check_remote_url>git remote get-url origin</check_remote_url>
  <identify_host>
    - github.com â†’ GitHub
    - gitlab.com â†’ GitLab
    - bitbucket.org â†’ Bitbucket
    - Other â†’ Ask user
  </identify_host>
</detect_provider>
<check_available_tools>
  <list_mcp_servers>Check which git-hosting MCP servers are available (github, gitlab, etc.)</list_mcp_servers>
  <check_cli>Check if gh/glab CLI is available as fallback</check_cli>
</check_available_tools>
<select_tool>
  <if_single_mcp>If only one relevant MCP available, confirm with user</if_single_mcp>
  <if_multiple>Let user choose which tool to use</if_multiple>
  <if_told_earlier>If user specified tool earlier in conversation, use that without asking again</if_told_earlier>
  <store_as>$GIT_HOST_TOOL (e.g., "github_mcp", "gitlab_mcp", "gh_cli")</store_as>
</select_tool>

  <purpose>Detect git hosting provider and select appropriate tool for issue lookup</purpose>
</step_0b>

  <step_1>
    <description>Detect current IDE environment</description>
    <condition>Only if $SETUP_MODE is "full"</condition>
    <detection_methods>
      <method_1>
        <tool>mcp__ide__getDiagnostics</tool>
        <vs_code_insiders>Check for paths containing "Code - Insiders"</vs_code_insiders>
        <vs_code>Check for paths containing "Code/" or "Code.app"</vs_code>
        <cursor>Check for paths containing "Cursor"</cursor>
        <zed>Check for paths containing "Zed"</zed>
      </method_1>
      <method_2>
        <fallback_detection>Use which command to find available IDEs</fallback_detection>
        <commands>which code-insiders code zed cursor</commands>
        <priority_order>code-insiders > cursor > zed > code</priority_order>
      </method_2>
    </detection_methods>
    <set_variables>
      <ide_command>Detected command (code-insiders|code|zed|cursor)</ide_command>
      <ide_name>Human-readable name</ide_name>
      <supports_tasks>true for VS Code variants, false for others</supports_tasks>
    </set_variables>
    <purpose>Automatically detect which IDE to use for opening the new worktree</purpose>
  </step_1>

  <step_2>
    <description>Determine default branch and parse arguments</description>
    <find_default_branch>
      <command>git symbolic-ref refs/remotes/origin/HEAD | sed 's@^refs/remotes/origin/@@'</command>
      <fallback>git remote show origin | grep 'HEAD branch' | cut -d: -f2 | tr -d ' '</fallback>
      <store_as>$DEFAULT_BRANCH (typically "main" or "master")</store_as>
    </find_default_branch>
    <input>The user-provided arguments</input>
    <expected_format>branch-name-or-issue-url [optional-base-branch]</expected_format>
    <example>fix/issue-123-main-content-area-visually-clipped main</example>
    <example_issue_url>https://github.com/owner/project/issues/123 main</example_issue_url>
    <base_branch>Use provided base branch, or $DEFAULT_BRANCH if not specified</base_branch>
  </step_2>

  <step_2_5>
    <description>Handle issue URLs from git hosting provider</description>
    <condition>If first argument matches issue URL pattern (detected in step_0)</condition>
    <url_detection>
      <github>Check if argument contains "github.com" and "/issues/"</github>
      <gitlab>Check if argument contains "gitlab.com" and "/-/issues/"</gitlab>
      <bitbucket>Check if argument contains "bitbucket.org" and "/issues/"</bitbucket>
    </url_detection>
    <url_parsing>
      <github_pattern>https://github.com/{owner}/{repo}/issues/{issue_number}</github_pattern>
      <gitlab_pattern>https://gitlab.com/{owner}/{repo}/-/issues/{issue_number}</gitlab_pattern>
      <bitbucket_pattern>https://bitbucket.org/{owner}/{repo}/issues/{issue_number}</bitbucket_pattern>
      <extract>owner, repo, issue_number from URL</extract>
    </url_parsing>
    <fetch_issue_details>
      <tool>Use $GIT_HOST_TOOL from step_0</tool>
      <method>get issue details</method>
      <parameters>owner, repo, issue_number</parameters>
    </fetch_issue_details>
    <generate_branch_name>
      <determine_type>Analyze issue title/labels to determine type (feat/fix/refactor/chore)</determine_type>
      <format>{type}/issue-{issue_number}-{kebab-case-title}</format>
      <kebab_case>Convert title to lowercase, replace spaces/special chars with hyphens</kebab_case>
      <sanitization>
        <rule>Always use lowercase for branch names</rule>
        <rule>Replace # with - (hash symbol not allowed in git branch names)</rule>
        <rule>Remove or replace other special characters: @, $, %, ^, &, *, (, ), [, ], {, }, \\, |, ;, :, ", ', <, >, ?, /, ~, \`</rule>
        <rule>Replace multiple consecutive hyphens with single hyphen</rule>
        <rule>Trim leading/trailing hyphens</rule>
      </sanitization>
      <truncate>Limit total branch name to reasonable length (~60 chars)</truncate>
    </generate_branch_name>
    <user_confirmation>
      <display>Show generated branch name and ask for confirmation</display>
      <options>"Yes, proceed" or "No, exit" or "Edit branch name"</options>
      <if_no>Exit command gracefully</if_no>
      <if_edit>Allow user to modify the branch name</if_edit>
      <if_yes>Continue with generated/modified branch name</if_yes>
    </user_confirmation>
    <examples>
      <input>https://github.com/owner/project/issues/456</input>
      <title>"Fix duplicate items in list view"</title>
      <generated>fix/issue-456-duplicate-items-in-list-view</generated>
    </examples>
  </step_2_5>

  <step_3>
    <description>Handle uncommitted changes if any exist</description>
    <condition>If git status --short output is not empty (has uncommitted changes)</condition>
    <prompt>
      <message>You have uncommitted changes. Move them to the new branch?</message>
      <options>
        <option value="yes">
          <label>Yes</label>
          <description>Stash changes and apply them in the new worktree</description>
        </option>
        <option value="no">
          <label>No</label>
          <description>Leave changes in current branch</description>
        </option>
      </options>
    </prompt>
    <if_yes>
      <command>git add -A && git stash push -m "Worktree switch: Moving changes to \${branch_name}"</command>
      <set_variable>$STASH_CREATED = true</set_variable>
    </if_yes>
    <if_no>
      <set_variable>$STASH_CREATED = false</set_variable>
    </if_no>
    <purpose>Let user decide whether to move work in progress to new branch</purpose>
  </step_3>

  <step_4>
    <description>Determine worktree parent directory</description>
    <check_if_in_worktree>git rev-parse --is-inside-work-tree && git worktree list --porcelain | grep "$(git rev-parse --show-toplevel)"</check_if_in_worktree>
    <set_parent_path>
      <if_main_worktree>Set parent_path=".."</if_main_worktree>
      <if_secondary_worktree>Set parent_path="../.." (need to go up two levels)</if_secondary_worktree>
    </set_parent_path>
    <purpose>Correctly determine where to create new worktree regardless of current location</purpose>
    <note>This handles both main worktree and secondary worktree scenarios</note>
  </step_4>

  <step_5>
    <description>Fetch latest changes from remote</description>
    <command>git fetch origin</command>
    <purpose>Ensure we have the latest remote branches and default branch state</purpose>
    <note>This ensures new worktrees are created from the most recent default branch</note>
  </step_5>

  <step_6>
    <description>Check if branch exists on remote</description>
    <command>git branch -r | grep "origin/\${branch_name}"</command>
    <decision>
      <if_exists>Branch exists on remote - will checkout existing branch</if_exists>
      <if_not_exists>Branch does not exist - will create new branch from base</if_not_exists>
    </decision>
  </step_6>

  <step_7>
    <description>Create the worktree</description>
    <option_a_new_branch>
      <condition>Remote branch does NOT exist</condition>
      <command>git worktree add \${parent_path}/\${branch_name} -b \${branch_name} --no-track \${base_branch}</command>
      <example>git worktree add ../fix/issue-123 -b fix/issue-123 --no-track origin/main</example>
      <note>--no-track prevents git from setting upstream to base_branch (which would make git push target main!)</note>
    </option_a_new_branch>
    <option_b_existing_branch>
      <condition>Remote branch EXISTS</condition>
      <command>git worktree add \${parent_path}/\${branch_name} \${branch_name}</command>
      <example>git worktree add ../fix/issue-123-main-content-area-visually-clipped fix/issue-123-main-content-area-visually-clipped</example>
    </option_b_existing_branch>
  </step_7>

  <step_7b>
    <description>Set up remote tracking for new branch</description>
    <condition>Only if new branch was created (option_a from step_7)</condition>
    <working_directory>\${parent_path}/\${branch_name}</working_directory>
    <command>cd \${parent_path}/\${branch_name} && git push -u origin \${branch_name}</command>
    <purpose>Establish remote tracking so git status shows ahead/behind and git push/pull work without specifying remote</purpose>
    <note>This creates the remote branch and sets upstream tracking in one step</note>
  </step_7b>

  <step_7c>
    <description>Quick mode completion</description>
    <condition>Only if $SETUP_MODE is "quick"</condition>
    <message>Worktree created at: \${parent_path}/\${branch_name}</message>
    <suggested_next_steps>
      <intro>You can now:</intro>
      <suggestion priority="1">Open in VS Code: \`code \${parent_path}/\${branch_name}\`</suggestion>
      <suggestion priority="2">Open in Cursor: \`cursor \${parent_path}/\${branch_name}\`</suggestion>
      <suggestion priority="3">Navigate to it: \`cd \${parent_path}/\${branch_name}\`</suggestion>
      <suggestion priority="4">Install dependencies: \`cd \${parent_path}/\${branch_name} && pnpm install\`</suggestion>
    </suggested_next_steps>
    <action>STOP here - do not continue to remaining steps</action>
  </step_7c>

  <step_8>
    <description>Copy Claude settings to new worktree</description>
    <condition>Only if $SETUP_MODE is "full"</condition>
    <source>.claude/settings.local.json</source>
    <destination>\${parent_path}/\${branch_name}/.claude/settings.local.json</destination>
    <command>cp -r .claude/settings.local.json \${parent_path}/\${branch_name}/.claude/settings.local.json</command>
    <purpose>Preserve all permission settings and configurations</purpose>
  </step_8>

  <step_9>
    <description>Copy .env.local files to new worktree</description>
    <condition>Only if $SETUP_MODE is "full"</condition>
    <search_command>find . -name ".env.local" -type f</search_command>
    <copy_logic>For each .env.local file found, copy to corresponding location in new worktree</copy_logic>
    <common_locations>
      - app/.env.local
      - packages/*/.env.local
      - (any other .env.local files found)
    </common_locations>
    <copy_command>find . -name ".env.local" -type f -exec sh -c 'mkdir -p "$(dirname "\${parent_path}/\${branch_name}/$1")" && cp "$1" "\${parent_path}/\${branch_name}/$1"' _ {} \\;</copy_command>
    <purpose>Preserve local environment configurations for development</purpose>
    <note>Only copies files that exist; ignores missing ones</note>
  </step_9>

  <step_10>
    <description>Create IDE-specific configuration (conditional)</description>
    <condition>Only if $SETUP_MODE is "full" AND supports_tasks is true (VS Code variants)</condition>
    <vs_code_tasks>
      <create_directory>mkdir -p \${parent_path}/\${branch_name}/.vscode</create_directory>
      <create_file_command>cat > \${parent_path}/\${branch_name}/.vscode/tasks.json << 'EOF'
{
  "version": "2.0.0",
  "tasks": [
    {
      "label": "Auto start Claude",
      "type": "shell",
      "command": "claude",
      "runOptions": {
        "runOn": "folderOpen"
      },
      "presentation": {
        "echo": false,
        "reveal": "always",
        "focus": true,
        "panel": "new"
      }
    }
  ]
}
EOF</create_file_command>
    </vs_code_tasks>
    <purpose>Create auto-start Claude task for VS Code variants on folder open</purpose>
    <note>Only creates for VS Code variants (code, code-insiders, cursor)</note>
    <skip_message>Skipping IDE-specific config for non-VS Code IDEs</skip_message>
  </step_10>

  <step_11>
    <description>Install dependencies in new worktree</description>
    <condition>Only if $SETUP_MODE is "full"</condition>
    <working_directory>\${parent_path}/\${branch_name}</working_directory>
    <command>cd \${parent_path}/\${branch_name} && pnpm install</command>
    <purpose>Ensure all node_modules are installed for the new worktree</purpose>
  </step_11>

  <step_12>
    <description>Apply stashed changes to new worktree (if stash was created)</description>
    <condition>Only if $SETUP_MODE is "full" AND $STASH_CREATED is true</condition>
    <working_directory>\${parent_path}/\${branch_name}</working_directory>
    <command>cd \${parent_path}/\${branch_name} && git stash pop</command>
    <purpose>Restore uncommitted work-in-progress to the new worktree branch</purpose>
    <note>This moves your uncommitted changes to the new branch where you'll continue working</note>
  </step_12>

  <step_13>
    <description>Open detected IDE in new worktree</description>
    <condition>Only if $SETUP_MODE is "full"</condition>
    <command>\${ide_command} \${parent_path}/\${branch_name}</command>
    <ide_specific_behavior>
      <vs_code_variants>Opens folder in VS Code/Insiders/Cursor with tasks.json auto-starting Claude</vs_code_variants>
      <zed>Opens folder in Zed editor</zed>
      <other>Uses detected IDE command to open folder</other>
    </ide_specific_behavior>
    <purpose>Launch development environment for the new worktree using detected IDE</purpose>
    <confirmation_message>Opening worktree in \${ide_name}</confirmation_message>
  </step_13>
</execution_steps>

<important_notes>

- Offers Quick or Full setup mode - Quick just creates the worktree, Full does everything
- Automatically detects and uses your current IDE (VS Code, VS Code Insiders, Cursor, Zed, etc.) in Full mode
- Creates VS Code-specific tasks.json only for VS Code variants (auto-starts Claude on folder open)
- Branch names with slashes (feat/, fix/, etc.) are fully supported
- The worktree directory path will match the full branch name including slashes
- Settings are copied to maintain the same permissions across worktrees
- Environment files (.env.local) are copied to preserve local configurations
- Each worktree has its own node_modules installation
- Uncommitted changes are automatically stashed and moved to the new worktree
- Your work-in-progress seamlessly transfers to the new branch
- IDE detection fallback: checks available editors and uses priority order
- New branches are automatically pushed with \`-u\` to set up remote tracking

Limitations:
- Assumes remote is named "origin" (most common convention)
- Supports macOS and Linux only (no Windows support)
- Requires MCP server or CLI for git hosting provider when using issue URLs (GitHub, GitLab, etc.)
- Dependency install command is pnpm (modify for npm/yarn if needed)
</important_notes>
"
`;

exports[`dynamic generation snapshots > without beads flag > should match snapshot for worktree-cleanup.md 1`] = `
"---
description: Clean up merged worktrees by verifying PR/issue status, consolidating settings, and removing stale worktrees
argument-hint: (no arguments)
_hint: Cleanup worktree
_category: Worktree Management
_order: 2
---

# Worktree Cleanup

## General Guidelines

### Output Style
- **Never explicitly mention TDD** in code, comments, commits, PRs, or issues
- Write natural, descriptive code without meta-commentary about the development process
- The code should speak for itself - TDD is the process, not the product






Clean up merged worktrees by finding the oldest merged branch, consolidating settings, and removing stale worktrees.

**User arguments:**

Worktree-cleanup: $ARGUMENTS

**End of user arguments**

<current_state>
Current branch: \`git branch --show-current\`
Current worktrees: \`git worktree list\`
</current_state>

<execution_steps>
<step_0>
  <description>Detect git hosting provider and available tools</description>


<detect_provider>
  <check_remote_url>git remote get-url origin</check_remote_url>
  <identify_host>
    - github.com â†’ GitHub
    - gitlab.com â†’ GitLab
    - bitbucket.org â†’ Bitbucket
    - Other â†’ Ask user
  </identify_host>
</detect_provider>
<check_available_tools>
  <list_mcp_servers>Check which git-hosting MCP servers are available (github, gitlab, etc.)</list_mcp_servers>
  <check_cli>Check if gh/glab CLI is available as fallback</check_cli>
</check_available_tools>
<select_tool>
  <if_single_mcp>If only one relevant MCP available, confirm with user</if_single_mcp>
  <if_multiple>Let user choose which tool to use</if_multiple>
  <if_told_earlier>If user specified tool earlier in conversation, use that without asking again</if_told_earlier>
  <store_as>$GIT_HOST_TOOL (e.g., "github_mcp", "gitlab_mcp", "gh_cli")</store_as>
</select_tool>

  <purpose>Detect git hosting provider and select appropriate tool for PR verification</purpose>
</step_0>

  <step_1>
    <description>Determine default branch and verify we're on it</description>
    <find_default_branch>
      <command>git symbolic-ref refs/remotes/origin/HEAD | sed 's@^refs/remotes/origin/@@'</command>
      <fallback>git remote show origin | grep 'HEAD branch' | cut -d: -f2 | tr -d ' '</fallback>
      <store_as>$DEFAULT_BRANCH (typically "main" or "master")</store_as>
    </find_default_branch>
    <check_current_branch>git branch --show-current</check_current_branch>
    <verify>Current branch must equal $DEFAULT_BRANCH</verify>
    <error_if_not_default>Exit with error: "This command must be run from the default branch ($DEFAULT_BRANCH)"</error_if_not_default>
    <purpose>Ensure we're consolidating to the default branch worktree</purpose>
  </step_1>

  <step_2>
    <description>Get list of all worktrees</description>
    <command>git worktree list --porcelain</command>
    <parse_output>Extract worktree paths and branch names</parse_output>
    <exclude_default>Filter out the default branch worktree from cleanup candidates</exclude_default>
    <purpose>Identify all worktrees that could potentially be cleaned up</purpose>
  </step_2>

  <step_3>
    <description>Find oldest worktree by directory age</description>
    <get_worktree_ages>
      <detect_platform>uname -s (returns "Darwin" for macOS, "Linux" for Linux)</detect_platform>
      <command_macos>git worktree list | grep -v "\\[$DEFAULT_BRANCH\\]" | awk '{print $1}' > /tmp/worktrees-$$.txt && while IFS= read -r path; do echo "$(/usr/bin/stat -f '%Sm' -t '%Y-%m-%d %H:%M' "$path" 2>/dev/null)|$path"; done < /tmp/worktrees-$$.txt | sort; rm -f /tmp/worktrees-$$.txt</command_macos>
      <command_linux>git worktree list | grep -v "\\[$DEFAULT_BRANCH\\]" | awk '{print $1}' > /tmp/worktrees-$$.txt && while IFS= read -r path; do echo "$(stat -c '%y' "$path" 2>/dev/null | cut -d. -f1)|$path"; done < /tmp/worktrees-$$.txt | sort; rm -f /tmp/worktrees-$$.txt</command_linux>
      <purpose>List all worktrees sorted by directory modification time (oldest first)</purpose>
      <critical_notes>
        - Replace $DEFAULT_BRANCH with value from step_1 (e.g., "main" or "master")
        - grep "\\[branch\\]" matches branch name in brackets, not paths containing the word
        - Temp file approach avoids subshell parsing issues with piped while-loops
        - /usr/bin/stat on macOS avoids homebrew stat conflicts
      </critical_notes>
      <expected_output_format>YYYY-MM-DD HH:MM|/full/path/to/worktree (oldest first)</expected_output_format>
    </get_worktree_ages>
    <filter_recent>
      <exclude_new>For worktrees created within the last 24 hours, let user know that this worktree might not be worth cleaning</exclude_new>
      <get_current_time>date +"%Y-%m-%d %H:%M"</get_current_time>
    </filter_recent>
    <select_oldest>
      <extract_branch_name>Parse branch name from oldest worktree path</extract_branch_name>
      <important_note>DO NOT use "git branch --merged" to check merge status - it's unreliable</important_note>
      <proceed_to_pr_check>Move directly to step 4 to verify PR merge status instead</proceed_to_pr_check>
    </select_oldest>
    <purpose>Identify oldest worktree candidate - actual merge verification happens via PR/MR in next step</purpose>
  </step_3>

  <step_4>
    <description>Verify PR/MR merge status (primary merge verification)</description>
    <determine_repo>
      <check_remote>git remote get-url origin</check_remote>
      <parse_repo>Extract owner/repo from remote URL</parse_repo>
    </determine_repo>
    <search_pr>
      <tool>Use $GIT_HOST_TOOL from step_0 (e.g., mcp__github__search_pull_requests, mcp__gitlab__*, or gh CLI)</tool>
      <query>Find PRs/MRs where head={branch_name} and base=$DEFAULT_BRANCH</query>
      <purpose>Find PR/MR for this branch targeting default branch</purpose>
      <important>This is the PRIMARY way to verify if a branch was merged - NOT git commands</important>
    </search_pr>
    <verify_pr_merged>
      <if_pr_found>
        <get_pr_details>Use $GIT_HOST_TOOL to get full PR/MR info</get_pr_details>
        <confirm_merged>Verify PR/MR state is "closed"/"merged" AND merged_at is not null AND base is "$DEFAULT_BRANCH"</confirm_merged>
        <extract_issue_number>Look for issue references in PR title/body (e.g., #123, owner/repo#123)</extract_issue_number>
        <if_merged>Proceed with cleanup - this branch was definitively merged to default branch</if_merged>
        <if_not_merged>
          <skip_worktree>This worktree is NOT merged - continue to next oldest worktree</skip_worktree>
          <repeat_from_step_3>Go back and find the next oldest worktree to check</repeat_from_step_3>
        </if_not_merged>
      </if_pr_found>
      <if_no_pr>
        <skip_worktree>No PR found - this branch was likely never submitted for review</skip_worktree>
        <continue_to_next>Continue checking next oldest worktree</continue_to_next>
      </if_no_pr>
    </verify_pr_merged>
    <purpose>Use PR/MR status as the authoritative source for merge verification instead of unreliable git commands</purpose>
  </step_4>

  <step_4_5>
    <description>Check and close related issue</description>
    <if_issue_found>
      <get_issue_details>
        <tool>Use $GIT_HOST_TOOL to read issue details</tool>
        <extract_repo>From issue reference (main-repo vs cross-repo)</extract_repo>
      </get_issue_details>
      <check_issue_state>
        <if_open>
          <ask_close>Ask user: "Related issue #{number} is still open. Should I close it? (y/N)"</ask_close>
          <if_yes_close>
            <add_closing_comment>
              <tool>Use $GIT_HOST_TOOL to add comment</tool>
              <body_template>Closing this issue as branch {branch_name} was merged to {default_branch} on {merge_date} via PR/MR #{pr_number}.</body_template>
              <get_merge_date>Extract merge date from PR/MR details</get_merge_date>
              <get_pr_number>Use PR/MR number from search results</get_pr_number>
            </add_closing_comment>
            <close_issue>
              <tool>Use $GIT_HOST_TOOL to close issue</tool>
              <state>closed</state>
              <state_reason>completed</state_reason>
            </close_issue>
          </if_yes_close>
        </if_open>
        <if_closed>Inform user issue is already closed</if_closed>
      </check_issue_state>
    </if_issue_found>
    <if_no_issue>Continue without issue management</if_no_issue>
    <purpose>Ensure proper issue lifecycle management</purpose>
  </step_4_5>

  <step_5>
    <description>Check if worktree is locked</description>
    <check_command>git worktree list --porcelain | grep -A5 "worktree {path}" | grep "locked"</check_command>
    <if_locked>
      <unlock_command>git worktree unlock {path}</unlock_command>
      <notify_user>Inform user that worktree was unlocked</notify_user>
    </if_locked>
    <purpose>Unlock worktree if it was locked for tracking purposes</purpose>
  </step_5>

  <step_6>
    <description>Analyze Claude settings differences</description>
    <read_main_settings>.claude/settings.local.json</read_main_settings>
    <read_worktree_settings>{worktree_path}/.claude/settings.local.json</read_worktree_settings>
    <compare_allow_lists>
      <extract_main_allows>Extract "allow" array from main settings</extract_main_allows>
      <extract_worktree_allows>Extract "allow" array from worktree settings</extract_worktree_allows>
      <find_differences>Identify entries in worktree that are not in main</find_differences>
    </compare_allow_lists>
    <filter_suggestions>
      <include_filesystem>Read permissions for filesystem paths</include_filesystem>
      <exclude_intrusive>Exclude bash commands, write permissions, etc.</exclude_intrusive>
      <focus_user_specific>Include only user-specific, non-disruptive entries</focus_user_specific>
    </filter_suggestions>
    <purpose>Identify useful settings to consolidate before cleanup</purpose>
  </step_6>

  <step_7>
    <description>Suggest settings consolidation</description>
    <if_differences_found>
      <display_suggestions>Show filtered differences to user</display_suggestions>
      <ask_confirmation>Ask user which entries to add to main settings</ask_confirmation>
      <apply_changes>Update main .claude/settings.local.json with selected entries</apply_changes>
    </if_differences_found>
    <if_no_differences>Inform user no settings need consolidation</if_no_differences>
    <purpose>Preserve useful development settings before removing worktree</purpose>
  </step_7>

  <step_8>
    <description>Final cleanup confirmation</description>
    <summary>
      <display_worktree>Show worktree path and branch name</display_worktree>
      <show_pr_status>Show merged PR details if found</show_pr_status>
      <show_issue_status>Show related issue status if found</show_issue_status>
      <show_last_activity>Display directory creation/modification date</show_last_activity>
    </summary>
    <safety_checks>
      <check_uncommitted>git status --porcelain in worktree directory</check_uncommitted>
      <warn_if_dirty>Alert user if uncommitted changes exist</warn_if_dirty>
    </safety_checks>
    <ask_deletion>Ask user confirmation: "Delete this worktree? (y/N)"</ask_deletion>
    <purpose>Final safety check before irreversible deletion</purpose>
  </step_8>

  <step_9>
    <description>Delete worktree</description>
    <if_confirmed>
      <remove_worktree>git worktree remove {path} --force</remove_worktree>
      <cleanup_branch>git branch -d {branch_name}</cleanup_branch>
      <success_message>Inform user worktree was successfully removed</success_message>
      <next_steps>Suggest running command again to find next candidate</next_steps>
    </if_confirmed>
    <if_declined>Exit gracefully with no changes</if_declined>
    <purpose>Perform the actual cleanup and guide user for next iteration</purpose>
  </step_9>
</execution_steps>

<important_notes>

- Uses PR/MR merge status as the ONLY reliable way to verify if a branch was merged
- DOES NOT use "git branch --merged" command as it's unreliable for merge verification
- Only processes branches with PRs/MRs that were definitively merged to default branch
- Skips worktrees without merged PRs/MRs and continues to next oldest candidate
- Checks and optionally closes related issues
- Prioritizes oldest worktrees by directory age first for systematic cleanup
- Warns about very recent worktrees (created within 24 hours) to avoid cleaning active work
- Preserves useful development settings before deletion
- Requires explicit confirmation before any destructive actions
- Handles locked worktrees automatically
- Processes one worktree at a time to maintain control
- Must be run from default branch for safety

Limitations:
- Assumes remote is named "origin" (most common convention)
- Supports macOS and Linux only (no Windows support)
- Requires MCP server or CLI for git hosting provider (GitHub, GitLab, etc.)
</important_notes>
"
`;
